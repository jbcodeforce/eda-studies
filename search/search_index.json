{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Event-Driven Solutions in Hybrid Cloud","text":"A virtual book about event driven solutions Site updated 09/2024 <p>This page content was reviewed 03/2024</p> <p>September:</p> <ul> <li>review dc structure </li> <li>Add content on schema management and schema registry</li> </ul> <p>July:</p> <ul> <li>Update Kafka content, mirror makers, and add governance</li> </ul> <p>March:</p> <ul> <li>update to introduction, concepts</li> <li>Add kafka streams</li> </ul> <p>November:</p> <ul> <li>Update to event storming</li> <li>Add event storming example for vaccine delivery</li> <li>Added summary of IBM MQ</li> </ul> <p>September:</p> <ul> <li>microservice challenges in patterns chapter</li> <li>flow architecture summary</li> <li>Dead letter queue pattern</li> </ul> <p>August:</p> <ul> <li>Reactive systems</li> <li>EDA content for event backbone criteria</li> </ul> <p>July: </p> <ul> <li>Add Choreography and orchestration introduction in design pattern</li> <li>Add Scatter-gather in design pattern</li> <li>Add Kafka connect content</li> <li>Add event sourcing </li> <li>Tune Saga pattern content</li> </ul>"},{"location":"#why-this-site","title":"Why this site?","text":"<p>This virtual book,  isn't just another static resource on Event-Driven Architecture (EDA). It's your dynamic companion on the journey to mastering this innovative approach.</p> <p>While the value of cloud adoption and microservices is no longer in question, the \"how\" of implementing them often remains a challenge. Here, you'll find a comprehensive, continuously evolving source of knowledge tailored to bridge that gap.</p> <p>Imagine a living textbook on EDA, constantly updated with the latest insights and practical guidance. That's the experience this blog aims to deliver. As I delve deeper into this exciting realm, so too will the content on this site, ensuring you stay ahead of the curve.</p> <p>Why choose this book?</p> <ul> <li>Dynamic learning: Unlike static books, this web content allows for continuous updates, ensuring access to the latest knowledge and best practices.</li> <li>Practical focus: Filled with actionable insights and real-world examples, the content goes beyond theory to help you apply EDA concepts effectively.</li> <li>Evolving expertise: As I learn and grow, so does this website, providing you with access to a constantly expanding knowledge base. Join me on this journey to unlock the full potential of Event-Driven Architecture!</li> </ul>"},{"location":"#target-audience-architects-and-leaders-building-modern-applications","title":"Target audience: Architects and Leaders Building Modern Applications","text":"<p>Event-Driven Architecture (EDA) empowers you to create modern, responsive, and scalable applications. Adopting EDA enables new solution based on real-time analytics, complex event processing, distributed and loosely coupled micro-services, this book is specifically aimed at individuals who play a crucial role to deploy event-driven solution, including:</p> <ul> <li>Enterprise Architects: Who oversee the overall technical vision and strategy for an organization. They will find valuable guidance on how EDA aligns with and supports long-term IT goals.</li> <li>Solution Architects: Who translate business needs into technical solutions. This content will equip them with the knowledge and expertise to design and implement robust EDA-based solutions.</li> <li>Developer Leaders: Who guide and mentor development teams. They will gain insights into fostering a culture of collaboration and best practices for building effective event-driven systems, by using well established practices of Domain-driven design and event storming.</li> </ul> <p>If you are passionate about building cutting-edge applications and want to explore the potential of EDA, I wish this book is your resource.</p>"},{"location":"#what-kind-of-problems-are-we-trying-to-solve","title":"What kind of problems are we trying to solve?","text":"<p>In today's software landscape, the rise of cloud computing and distributed systems has brought new challenges and opportunities. The term Event-driven architecture is heavily influenced by the capabilities of software vendors, and sometime is becoming a marketing play. But in reality traditional architectures often struggle with:</p> <ul> <li>Scalability and Performance: As applications grow, maintaining responsiveness and handling increasing load may become very difficult.</li> <li>Microservice Complexity: While microservices offer advantages, managing the complexity of distributed systems and well designed loose coupled components can be challenging.</li> <li>Synchronous Communication Bottlenecks: Point-to-point, synchronous communication between services can create tight coupling, hindering agility and making changes cumbersome.</li> <li>Limited Real-Time Processing: Traditional architectures often lag in processing data as it's generated, leading to delays in insights and actions.</li> <li>Data Consistency Concerns: Achieving consistency across distributed systems can be complex, especially when dealing with eventual consistency models.</li> </ul> <p>Event-Driven Architecture (EDA) is as a powerful solution to these challenges. It offers a paradigm shift by focusing on events as the primary driver of communication and data flow. This blog delves into the core principles of EDA, guiding you in harnessing its potential to build scalable, responsive, and resilient applications \u2013 regardless of specific software vendors or products.</p> <p>While this site does include some implementations using specific open-source products, its primary aim is to remain product-agnostic and to outline the essential principles of EDA.</p>"},{"location":"#why-eda-is-important-in-2020s","title":"Why EDA is important in 2020s","text":"<p>The past two decades have witnessed a radical transformation in how businesses operate. The rise of software as a key strategic differentiator has disrupted entire industries, from retail and media to transportation and finance. This shift, often referred to as \"industry becoming software\", goes beyond simply using software for traditional functions like accounting or CRM. It's about leveraging the power of software to create entirely new business models, engage customers in innovative ways, and gain a competitive edge.</p> <p>In this new landscape, four key capabilities have emerged as critical drivers of success:</p> <ol> <li>Cloud: This shift from on-premises data centers to cloud computing unlocks unprecedented scalability, elasticity, and cost-efficiency. It enables continuous optimization of the compute power usage, changing the pricing model from capex to pay-as-you-go. Cloud elastice resources are readily available, allowing businesses to adapt to changing demands much faster.</li> <li>AI / machine learning: These technologies are transforming how businesses make decisions and automate complex tasks. By automating data analysis and insights generation, AI/ML empowers businesses to act when data are created with greater intelligence and efficiency.</li> <li>Mobile: The rise of mobile apps has fundamentally altered user experiences and interactions with businesses. Users expect seamless mobile access to services and notifications that keep them informed and engaged.</li> <li>Data, especially data as event: Data has become the lifeblood of modern businesses, guiding user experiences, driving decisions, and fueling innovation. However, static data stored in traditional warehouses is no longer enough. Businesses require real-time access to data as events to react instantly to changes and opportunities.</li> </ol> <p>This evolution necessitates a shift in data architecture. Traditional, centralized databases struggle to meet the demands of real-time data access and processing. As a result, data architectures are evolving towards distributed, decentralized models based on data in motion and data lakes.</p> <p>Event-Driven Architecture (EDA) emerges as a critical enabler in this evolving data landscape. By treating data as a stream of events, EDA empowers businesses to:</p> <ul> <li>Gain real-time visibility into data: Capture and process data immediately as it's generated, eliminating the latency associated with traditional approaches. React to changes instantly: Trigger actions and processes based on specific events, enabling real-time decision-making and automated workflows.</li> <li>Improve agility and scalability: Decouple components and enable independent scaling, ensuring the architecture can adapt to changing data volumes and business needs.</li> </ul> <p>Studies reveal a strong correlation between the age of data and the quality of business decisions based on it. Fresh, real-time data holds significantly greater value compared to stale information. The value of the data dismisses over time. This is especially crucial for AI models, where data freshness directly impacts performance and accuracy. EDA plays a critical role in supporting data pipelines for AI/ML, facilitating feature engineering and efficient data storage for machine learning models.</p> <p>While Service-Oriented Architecture (SOA) laid the groundwork for decomposing applications into reusable business services in the early 2000s, EDA emerged as a natural evolution in the mid-2000s. It addressed the limitations of SOA in terms of scalability and inter-dependencies, particularly when dealing with a large number of data producers and consumers. The following diagram, from 2004, illustrates this evolution.</p> <p></p> <p>The mid-2010s witnessed a surge in asynchronous communication and the adoption of Event-Driven Architecture (EDA), particularly among internet startups, to address scalability.  Asynchronous communication, one axe of the adoption, decouples event producers from consumers, enabling independent scaling and improved overall system agility. As EDA leverages data as events, allowing businesses to gain immediate visibility into changes and act on them swiftly. This facilitates faster decision-making and automated workflows.</p> <p>However, it's important to acknowledge that decoupling also introduces complexities in development efforts.</p> <p>The rise of cloud computing has further empowered businesses. Cloud platforms offer:</p> <ul> <li>Horizontal Scalability: Organizations can scale their software capabilities on-demand to meet fluctuating workloads, ensuring optimal performance during peak periods: consumers are added, horizontally, to process events.</li> <li>Rapid Development and Deployment: Cloud-native practices promote agile methodologies and continuous deployment, allowing for frequent feature releases (even multiple times per week) to large user bases.</li> <li>Focus on Innovation: By removing the burden of infrastructure management (server maintenance, OS updates, security patches), IT teams can dedicate more resources to developing innovative features that deliver value to end users.</li> </ul> <p>In essence, Event-Driven Architecture presents a powerful architectural approach for the modern business landscape, enabling organizations to leverage real-time data effectively and gain a competitive advantage in the age of \"industry becoming software.\"</p>"},{"location":"#the-power-of-event-driven-architecture","title":"The Power of Event-Driven Architecture","text":"<p>Event-Driven Architecture (EDA) offers a compelling paradigm for building modern, scalable, and responsive applications. Here's how EDA empowers you:</p> <p>Superior Scalability and Performance: EDA thrives on asynchronous processing. Components react to messages independently, eliminating bottlenecks and enabling seamless scaling to handle heavy workloads efficiently. This translates to consistently high performance and responsiveness for your users.</p> <p>Loose Coupling for Agile Development:  EDA fosters loosely coupled systems. Components communicate through events, minimizing dependencies and simplifying maintenance, updates, and individual component scaling. This promotes modularity, extensibility, and a development approach that's developer-friendly and agile.</p> <p>Real-Time Capabilities:  EDA excels at building real-time and reactive systems. Applications react to events instantaneously, enabling features like real-time analytics, instant notifications, and dynamic user experiences. This ensures your applications stay responsive and users remain engaged.</p> <p>Microservices and Distributed Systems: EDA aligns perfectly with the tenets of microservices and distributed systems. It provides a communication framework where services interact via events. This fosters service autonomy, allowing independent deployment and evolution of individual components. It's ideal for organizations that value a modular and agile software development approach.</p> <p>Event Sourcing and Integration: EDA plays a pivotal role in both event sourcing and event-driven integration. Event sourcing uses events to reconstruct application state, while event-driven integration facilitates data and action exchange across diverse systems. This simplifies the development of complex, interconnected ecosystems.</p> <p>Enhanced Flexibility and Adaptability: EDA promotes decoupled communication, making it easier to introduce new features or third-party services with minimal disruption to existing systems. This inherent flexibility allows your systems to readily adapt and evolve as your business needs change.</p> <p>Business Alignment: EDA aligns naturally with business and domain-driven design approaches. Business events and domain concepts are directly captured and represented within the system's architecture. This facilitates a clear understanding and better alignment between the technical and business domains.</p> <p>&gt;&gt; Next: - EDA concepts</p>"},{"location":"ibm-assets/","title":"IBM EDA assets","text":"<p>From 2018 to 2022, I led the reference architecture work for IBM, with the implementations of different assets to demonstrate the value of Event-driven architecture, at the generic and IBM product specific levels.</p> <p>Here is a list of those assets.</p> <p>Info</p> <p>Updated 1/7/2022</p> <ul> <li>The main EDA content website</li> <li>Main EDA reference implementation</li> <li>Vaccine reference implementation</li> <li>Kafka Connect with hand-on lab</li> <li>Mirror maker 2</li> <li>Consumer group</li> </ul>"},{"location":"ibm-assets/#labs","title":"Labs","text":"<ul> <li>Event Streams on Cloud lab</li> <li>Security and access control with IBM Cloud lab</li> <li>Real time inventory management with kafka streams and kafka connect:</li> </ul>"},{"location":"ibm-assets/#demos","title":"demos","text":"<ul> <li>Real-time invetory with flink, elasticsearch, kafka, cloud object storage</li> </ul> App Type Build Registry Git workflow Local OCP Store sale event producer simulator Quarkus 2.7.1 ok quay.io/ibmcase/eda-store-simulator ko ok gitops ok store-inventory Quarkus 2.7.1 test ko quay.io/ibmcase/store-aggregator ok ok item-inventory Quarkus 2.7.1 test ko quay.io/ibmcase/item-aggregator ok ok <ul> <li>GitOps eda-rt-inventory-gitops includes the deployment of the 3 apps + kafka connectors and docker-compose to run the solution local.</li> <li> <p>Demo scripts in: refarch-eda/scenarios/realtime-inventory</p> </li> <li> <p>Order demos based on developer experience article:</p> <ul> <li>Order demo gitops  The readme is up to date</li> <li>Order producer</li> <li>Order consumer</li> </ul> </li> <li> <p>Angular app with nodejs BFF to present the container shipment demonstration</p> </li> <li>A fleet simulator microservice</li> <li>Real time analytics as streaming application demonstrates real time event processing applied to inputs from the ships and containers used in the K Container Shipment Use Case</li> <li>Event sourcing and CQRS pattern illustrated</li> <li>Event Streams Samples for IBM messaging github</li> </ul>"},{"location":"toworkon/","title":"Stuff that I need to deeply study","text":""},{"location":"toworkon/#kafka-related","title":"Kafka related","text":"<ul> <li>Apicurio schema registry and this one</li> <li>Kafka streams in action</li> <li>kafka cruise control</li> </ul>"},{"location":"aws/eventbridge/","title":"EDA with Amazon EventBridge","text":"<p>See all my personal EventBridge technology summary.</p> <p>As a serverless service, Amazon EventBridge may be used as a technology to support the event backbone capability of an EDA, leveraging the following capabilities:</p> <ul> <li>EventBus construct, to channel messages to consumers.</li> <li>EventBus pushes messages to subscribers.</li> <li>Routing rules logic to target specific consumer given message attributes, or to filter out messages.</li> <li>Add metadata to the message sent, about the source and the event type and AWS specific data like ARN, region...</li> <li>Support transforming the event before going to the target using import transformer.</li> <li>Support archiving events, and keep events for a retention period. One archive per EventBus. Events in archive can be replayed by defining a replay object, using an existing archive, and the event bus source of the archive. Replays are not immediate and can take minutes. </li> <li>EventBridge Pipes can be used to keep message order from source to one destination, perform transformation, enrichment and filtering. The sources can be dynamoDB stream, Kinesis stream, MQ broker, MSK topic, Kafka topic, SQS queue. It processes messages in batch. </li> <li>For error handling it supports Dead Letter Queue, with metadata about the error.</li> </ul>"},{"location":"aws/eventbridge/#fit-for-purpose","title":"Fit for purpose","text":"<ul> <li>Events in the context of EventBridge, may not be events in the context of DDD. It is a message to route, it could mark a command as well as an event as immutable fact.</li> <li>With route and filtering logic not all messages could go to the consumer. </li> <li>Adding a consumer, means modifying routing rules and/or filtering logic.</li> <li>One consumer receive one message.</li> <li>The EventBridge broker is responsible of the message delivery to the consumer, with timeout and retries mechanism. Events are retried for up to 24 hours or 185 times by default. </li> <li>Resource constraints may impact the application SLA. </li> <li>Very well integrated with other AWS services, and easy to use around Lambda function.</li> <li>EDA solution on top of EventBridge will combine more messaging systems like SNS, SQS, Kinesis data streams... </li> <li>Delivery latency increases with more than 3 destinations and high transaction per second throughput.</li> </ul>"},{"location":"aws/eventbridge/#external-tools-to-consider","title":"External tools to consider","text":"<ul> <li>Event Catalog is an Open Source project that helps you document your events, services and domains.</li> <li>EventBridge Atlas an open source project to document, discover and share EventBridge schemas </li> </ul>"},{"location":"aws/eventbridge/#classical-project-activities","title":"Classical project activities","text":"<p>We need to consider the following project activities:</p> <ul> <li>Build an inventory of Event Sources</li> <li>Review non-functional requirements</li> <li>Define what event buses are and schema definition of the message</li> <li>Assess where to deploy event buses</li> <li>Address Event Schema and schema evolution and how to share them between buses.</li> <li>For each event bus, define the schema definition using OpenAPI, JSONSchema</li> <li>Which consumers are interested in which events to define routing rules.</li> <li>What are the requirements around exactly one delivery, message ordering and time base reasoning.</li> <li>The adoption of CloudEvent as a payload cross messaging system to keep an end to end view of the data flow.</li> </ul>"},{"location":"concepts/backbone/","title":"Message Backbone","text":"Chapter updated 09/2024 <p>This page content reviewed 09/2024 Created 2020 - Updated 08/ 2024</p> <p>As introduced before the messaging as a service enterprise may want to deploy support the following components:</p> <p></p> <p>Most of those components, withn this diagram, are described in separate chapters. </p> <p>Message Backbone serves as the core middleware that facilitates asynchronous communication and storage of events. It possesses high availability and replayability capabilities, ensuring the reliable handling and persistence of events.</p>"},{"location":"concepts/backbone/#selecting-event-bus-technologies","title":"Selecting event bus technologies","text":"<p>As introduced in the event backbone capabilities section, there are different messaging capabilities to support. There is no product on the market that supports all those requirements. Enterprise deployment for an event-driven architecture needs to address all those capabilities at different level, and at different time. It is important to any EDA adoption to start small, and add on top of existing foundations but always assess the best fit for purpose.</p>"},{"location":"concepts/backbone/#message-backbone-with-queues","title":"Message Backbone with queues","text":"<p>Consider queue system when we need to:</p> <ul> <li>Support point-to-point delivery (asynchronous implementation of the Command pattern): Asynchronous request/reply communication: the semantic of the communication is for one component to ask a second component to do something on its data. The state of the caller depends of the answer from the called component.</li> <li>Deliver exactly-once semantic: not loosing message, and no duplication. </li> <li>Participate into two-phase commit transaction (certain Queueing system support XA transaction).</li> <li>Keep strict message ordering. This is optional, but FIFO queues are needed in many business applications.</li> <li>Scale, be resilient and always available.</li> </ul> <p>Messages in queue are kept until consumer(s) got them. Consumer has the responsibility to remove the message from the queue, which means, in case of interruption, the record may be processed more than one time. So if wdevlopers need to be able to replay messages, and consider timestamp as an important element of the message processing, then streaming is a better fit.</p> <p>Queueing system are non-idempotents, but some are.</p> <p>Products: IBM MQ, open source RabbitMQ and Active MQ. Some Cloud vendor managed services like AWS SQS, Azure Storage Queues.</p> <p></p> <p>IBM MQ Broker Cluster topologies</p>"},{"location":"concepts/backbone/#pubsub-with-topics","title":"Pub/sub with topics","text":"<p>The leading event middleware in the market is Apache Kafka. Kafka's append-only log format, sequential I/O access, and zero-copy mechanisms enable high throughput with low latency. Its partition-based data distribution allows for horizontal scaling to hundreds of thousands of partitions.</p> <p>Older queue systems are now supporting publish/subscribe (pub/sub) patterns, most of which are based on Java Messaging Service (JMS) APIs and protocols.</p> <p>A typical Kafka cluster has multiple brokers, with their own persistence on disk and Zookeeper Ensemble to manage the cluster states (topology up to mid 2024:)</p> <p></p> <p>Since mid 2024 and the general availability of the Kraft protocol and the controller nodes, Zookeeper is not mandatory and not recommended.</p> <p></p> <p>Topics are key elements that support message persistence and access semantics. Most systems operate with topic subscriptions, where some publish messages only to active subscriptions. In contrast, Kafka allows consumers to connect at any time and replay events from the beginning.</p>"},{"location":"concepts/backbone/#criteria-to-select-a-technology","title":"Criteria to select a technology","text":"<p>A Message as a Service (MaaS) platform must support both queues and topics. For topics, a specific Kafka packaging needs to be chosen. There are numerous packaging options available in the market, and cloud providers offer managed services that eliminate the need for infrastructure management and version upgrades.</p> <p>There are a set of criteria to consider for selecting a Kafka packaging:</p> <ul> <li>Type of asynchronous expected interaction, queue or topic</li> <li>Data volume</li> <li>Easy to move data to Tiered storage</li> <li>Expected Latency</li> <li>Storage needs and retention time</li> <li>User interface to get visibility to topics, consumer groups and consumer lags</li> <li>Support to declarative configuration</li> <li>Monitoring with proprietary and shared platforms</li> <li>Access control: enforce strong access controls and authentication mechanisms</li> <li>Encryption for data in transit and at rest</li> <li>Support to a single glass for queue and topic management</li> <li>Legal and compliance requirements for data handling, with fine grained access control</li> <li>Cost of running the infrastructure and services.</li> <li>Support to address complex disaster recovery strategies</li> </ul>"},{"location":"concepts/backbone/#centralized-cluster-vs-multiple-clusters","title":"Centralized Cluster vs multiple clusters","text":"<p>Adopting a centralized Kafka cluster in a production environment can offer numerous benefits, but it also necessitates careful considerations of various factors. It is essential to evaluate segregation strategies to maximize performance, optimize costs, and facilitate greater adoption of Kafka. The strategic dimensions to consider are:</p> <ul> <li>Geo-localization requirements</li> <li>Operational decoupling</li> <li>Tenant isolation</li> <li>Application optimization</li> <li>Service availability, and data criticality</li> <li>Cost and operating attention to each workload.</li> </ul>"},{"location":"concepts/backbone/#factor-for-centralized","title":"Factor for centralized","text":"<p>Here are some key factors to consider:</p> <ul> <li>Number of applications and small number of topic per application.</li> <li>Application designed for real-time messaging and distributed data processing may better fit for leveraging common cluster. While ETL data pipeline can afford more downtime than a real-time messaging infrastructure for frontline applications.</li> <li>Data volume and data criticality</li> <li>Data isolation by applications and domains: a centralized Kafka cluster facilitates better data governance, data sharing, and data consistency across the organization. It enables a common data streaming strategy and reduces the risk of data silos. But domain specific data boundaries leads to adopt decentralized cluster.</li> <li>Retention time</li> <li>Network latency for producers and consumers connecting to the centralized cluster.</li> <li>Streaming processing of consumer-process-produce with exactly once semantic can only be done in the same cluster.</li> <li>Scalability needs in the future: data volume growth</li> <li>Maximun number of broker per clusters- Cluster with 100 brokers is alaready complex to manage. A small cluster needs at least 3 to 6 brokers and 3 to 5 Zookeeper node. Those components should be spread across multiple availability zones for redundancy.</li> <li>Level of automation to manage the cluster: a centralized approach simplifies management, and maintenance. It reduces the complexity of configuration, upgrades, and troubleshooting compared to managing multiple independent clusters</li> <li>Monitoring of the cluster health with a way to drill down to the metrics for root cause analysis</li> <li>Current expertise and resources to manage a centralized system effectively.</li> <li>Legal and compliance requirements for data handling.</li> <li>Cost with bigger hardware for each node, number of nodes, versus smaller cluster. Consider idle time. Centralized cluster should cost less thant multiple decentralized clusters. By consolidating resources and reducing redundant infrastructure and DevOps efforts, organizations can achieve significant cost savings. But if the data transfer to the centralized Kafka cluster is high then the benefits will be low.</li> <li>Evaluate how well a centralized cluster can integrate with existing systems and data sources.</li> <li>Plan for how the centralized system will handle failures and ensure data durability. Bigger failover is more complex and may be more costly.</li> <li>Replication Strategies: Assess the replication needs to prevent data loss in case of cluster issues.</li> <li>Determine if a centralized cluster promotes better collaboration among teams.</li> <li>Consider how operational responsibilities will be divided among teams.</li> <li>Kafka was not designed for multi-tenancy. Ther is no concept like namespaces for enforcing quotas and access control. Confluent has added a lot on top of the open-source platform to support.</li> </ul> <p>Sharing a single Kafka cluster across multiple teams and different use cases requires precise application and cluster configuration, a rigorous governance process, standard naming conventions, and best practices for preventing abuse of the shared resources.</p>"},{"location":"concepts/backbone/#adopting-multiple-clusters","title":"Adopting multiple clusters","text":"<p>The larger a cluster gets, the longer it can take to upgrade and expand the cluster due to rolling restarts, data replication, and rebalancing. Using separate Kafka clusters allows faster upgrades and more control over the time and the sequence of rolling out a change.</p> <ul> <li>There is nothing in Kafka for a bad tenant to monopolize the cluster resources, having smaller cluster for each functional area or domain, is better for isolation.</li> </ul>"},{"location":"concepts/backbone/#practices","title":"Practices","text":"<ul> <li>Adopt a topic naming convention with prefixes becomes part of the security policy to be controlled with access control list.</li> <li>Limit admin access to one user per domains, clarify the rule of operations.</li> <li>There may be need for hundreds of granular ACLs for some applications that are less trusted, and coarse-grained ACLs for others.</li> <li>Add a way to manage metadata about cluster, topics, schema, producer and consumer apps.</li> </ul>"},{"location":"concepts/backbone/#streaming-capabilities","title":"Streaming capabilities","text":"<p>Consider streaming system, like Kafka, AWS Kinesis Data Stream, as pub/sub and persistence system for:</p> <ul> <li>Publish events as immutable facts of what happened in an application.</li> <li>Get continuous visibility of the data Streams.</li> <li>Keep data once consumed, for future consumers, and for replay-ability.</li> <li>Scale horizontally the message consumption.</li> <li>When need to apply event sourcing, and time based reasoning. </li> </ul>"},{"location":"concepts/backbone/#event-router-needs","title":"Event router needs","text":"<p>Some vendors are proposing event routing capability to top of message broker. This is marginal and addresses a very limited problem of routing to target subscribers. Consider event router when:</p> <ul> <li>There is very basic logic to apply to the message to route to a target consumer</li> <li>Being able to apply filter and transformation on message in the message broker</li> <li>Many to many routing</li> <li>Point to point with enrichment</li> </ul>"},{"location":"concepts/data-pipeline/","title":"Data pipeline","text":"<p>The extended architecture extends the basic EDA reference architecture with concepts showing how data science, artificial intelligence and machine learning can be incorporated into an event-driven solution. The following diagram illustrates the event sources on the left injecting events to topics where green components are consuming from. Those components apply filtering, compute aggregates and stateful operation with time window based rules. Some of those components can include training scoring model, to do for example anomaly detection. The model is built with data scientist workbench tool, like Watson Studio.</p> <p></p> <p>The starting point for data scientists to be able to derive machine learning models or analyze data for trends and behaviors is the existence of the data in a form that they can be consumed. For real-time intelligent solutions, data scientists typically inspect event histories and decision or action records from a system. Then, they reduce this data to some simplified model that scores new event data as it arrives.</p>"},{"location":"concepts/data-pipeline/#getting-the-data-for-the-data-scientist","title":"Getting the data for the data scientist:","text":"<p>With near real-time event streams, the challenge is in handling unbounded data or a continuous flow of events. To make this consumable for the data scientist you need to capture the relevant data and store it so that it can be pulled into the analysis and model-building process as required.</p> <p>Following our event-driven reference architecture the event stream would be a Kafka topic on the event backbone.  From here there are two possibilities for making that event data available and consumable to the data scientist:</p> <ul> <li>The event stream or event log can be accessed directly through Kafka and pulled into the analysis process</li> <li>The event stream can be pre-processed by the streaming analytics system and stored for future use in the analysis process. You have a choice of store type to use. Within public IBM cloud object storage Cloud Object Store can be used as a cost-effective historical store.</li> </ul> <p>Both approaches are valid, pre-processing through streaming analytics provides opportunity for greater manipulation of the data, or storing data over time windows for complex event processing. However, the more interesting distinction is where you use a predictive (ML model) to score arriving events or stream data in real time. In this case you may use streaming analytics to extract and save the event data for analysis, model building, and model training and also for scoring (executing) a derived model in line in the real time against arriving event data.</p> <ul> <li>The event and decision or action data is made available in cloud object storage for model building through streaming analytics.</li> <li>Models may be developed by tuning and parameter fitting, standard form fitting, classification techniques, and text analytics methods.</li> <li>Increasingly artificial intelligence (AI) and machine learning (ML) frameworks are used to discover and train useful predictive models as an alternative to parameterize existing model types manually.</li> <li>These techniques lead to process and data flows where the predictive model is trained offline using event histories from the event and the decision or action store possibly augmented with some supervisory outcome labelling, as illustrated by the paths from the <code>Event Backbone</code> and <code>Stream Processing</code> store into <code>Learn/Analyze</code>.</li> <li>A model trained in this way includes some \u201cscoring\u201d API that can be invoked with fresh event data to generate a model-based prediction for future behavior and event properties of that specific context.</li> <li>The scoring function is then easily reincorporated into the streaming analytics processing to generate predictions and insights.</li> </ul> <p>These combined techniques can lead to the creation of near real-time intelligent applications:</p> <ol> <li>Event-driven architecture</li> <li>Identification of predictive insights using event storming methodology</li> <li>Developing models for these insights using machine learning</li> <li>Near real-time scoring of the insight models using a streaming analytics processing framework</li> </ol> <p>These are scalable easily extensible, and adaptable applications responding in near real time to new situations. There are easily extended to build out and evolve from an initial minimal viable product (MVP) because of the loose coupling in the event-driven architecture, , and streams process domains.</p>"},{"location":"concepts/data-pipeline/#data-scientist-workbench","title":"Data scientist workbench","text":"<p>To complete the extended architecture for integration with analytics and machine learning, consider the toolset and frameworks that the data scientist can use to derive the models.  Watson Studio  provides tools for data scientists, application developers, and subject matter experts to collaboratively and easily work with data to build and train models at scale.</p> <p>For more information see Getting started with Watson Studio.</p>"},{"location":"concepts/data-pipeline/#modern-data-lake","title":"Modern Data Lake","text":"<p>One of the standard architecture to build data lake is the lambda architecture with data injection, stream processing, batch processing to data store and then queries as part of the service layer. It is designed to handle massive quantities of data by taking advantage of both batch and stream processing methods. Lambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record. The batch layer pre-computes results using a distributed processing system that can handle very large quantities of data. Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data.</p> <p>The following figure is an enhancement of the lambda architecture with the adoption of Kafka as event backbone for data pipeline and source of truth and streaming processing to support real time analytics and streaming queries.</p> <p></p> <p>On the left the different data sources, injected using different protocols like MQTT, HTTP, or Kafka Connect... The business applications are supported by different microservices that are exposed by APIs and event-driven. The APIs is managed by API management product. Business events are produced as facts about the business entities, and persisted in the append log of kafka topic. Transactional data can be injected from MQ queues to Kafka topic, via MQ connectors. </p> <p>The data platform offers a set of capabilities to expose data for consumers like Data Science workbench (Watson Studio) via virtualization and data connections. The data are cataloged and governed to ensure integrity and visibility. The storage can be block based, document oriented or table oriented.</p> <p>Batch queries and map reduce can address huge data raw, while streaming queries can support real time aggregates and analytics.</p>"},{"location":"concepts/eda/","title":"Event-driven architecture","text":"Chapter updated 07/2024 <p>This page content reviewed 07/2024 Created 2018</p>"},{"location":"concepts/eda/#components-of-the-architecture","title":"Components of the architecture","text":"<p>To provide a comprehensive understanding of event-driven architecture (EDA) and its high-level building blocks, it is crucial to consider the following components without any technology bias. The diagram below illustrates these components:</p> Event-driven architecture high level view <p>The main goals of this architecture are to support scaling, decoupling, and acting on data as early as created, while enabling data pipelines for future batch processing and AI/ML development.</p> <ul> <li> <p>Event sources encompass any applications from which we need to obtain data. While most of these sources may not be inherently event-oriented, we can retrieve events from them by pulling/querying their APIs or data storage (databases) or by connecting change data capture agents to receive continuous event feeds.</p> </li> <li> <p>Event Backbone serves as the core middleware that facilitates asynchronous communication and storage of events. It possesses high availability and replayability capabilities, ensuring the reliable handling and persistence of events.</p> </li> <li> <p>New event-driven applications: These applications, whether microservices or functions, are designed with events as a fundamental part of their architecture from day one. They act as both producers and consumers of events, exposing APIs for utilization by web apps, mobile apps, or B2B apps. As a result, they support both synchronous and asynchronous protocols.</p> </li> <li> <p>Events are stored within the event backbone for extended retention periods and can also be directed to target sinks such as data lakes for further processing and analysis.</p> </li> <li> <p>Sinks represent longer term storage or downstream backend processing destinations for event data. Integrating with sinks can be challenging, as the sink software may not inherently support idempotency or exactly-once delivery. Normally consistency boundaries are before reaching the sinks.  </p> </li> <li> <p>Event Stream Processing: The final component of the architecture involves acting on the events through a consume-process-publish semantic. Event processing encompasses activities such as data processing, real-time analytics, stateful aggregate computation, data transformation, data pipelines, and Complex Event Processing.</p> </li> </ul> <p>It is important to note that cross-cutting capabilities such as security, devops and governance must be applied across all these components. Infrastructure as Code practices should be leveraged with technologies that support this architecture. Governance efforts need to address critical aspects such as data lineage, schema management, and APIs management.</p> <p>By considering these high-level building blocks and their interactions, organizations can create scalable, decoupled, and proactive architectures that enable early data actions, support future batch processing, and facilitate AI/ML development.</p>"},{"location":"concepts/eda/#zooming-into-the-capabilities-of-the-event-backbone","title":"Zooming into the capabilities of the Event Backbone","text":"<p>The Event Backbone is not tied to a specific technology, as different requirements necessitate the use of different tools. In the realm of asynchronous communication, applications produce messages, consume them, or do both in the form of consume-process-produce processing.</p> Event backbone component view <p>For the producer, two behaviors can be illustrated: </p> <ol> <li>When applications need to request another service to perform a task on their behalf, it follows the classical command pattern. In this scenario, there is a single interested consumer, and the exchange between the two applications employs request/reply messages. The communication requires exactly-once delivery, message ordering, and no data loss. Queues are the ideal technology for supporting this type of communication.</li> <li>When applications need to broadcast changes in their main business entity's state, they produce events as immutable facts. The appropriate technology approach is the pub/sub model on a topic. Two technologies should be considered: the older topic-based approach like JMS, or the streaming approach supported by products like Apache Kafka.</li> </ol> <p>For consumers, they can either subscribe to a queue or topic and pull messages or receive messages as part of their subscription in a push model. With pulling, the consumer needs to filter out messages that it is not interested in. With pushing, the event backbone can apply filtering and routing rules. It's important to note that in a queueing system, messages are deleted once read, while in classical topic implementations, messages are retained until all known subscribers have received the message. In Kafka, messages disappear after a retention time or upon reaching a log size threshold. With streaming from a topic, one consumer can read many messages from different topics, and one message can be read by many consumers arriving at different times.</p> <p>The consume-process-produce pattern is prevalent in technologies like Kafka Streams. This pattern involves a three-step transactional process that supports exactly-once delivery. Other technologies like Apache Flink or Apache Spark Streaming may also support similar implementations. The key aspect of this pattern is the creation of new facts through processing, which are then published to another topic. This approach is valuable for building data pipelines, performing stateful aggregation using time windows, enabling real-time analytics, and implementing complex event processing.</p> <p>When it comes to streaming and topics, messages can be ordered, and the topic itself may include timestamps. This allows for the implementation of complex event processing logic that can examine event sequencing, detect missing events, and handle out-of-order events.</p> <p>Topics or queues should provide persistence for resilience purposes, allowing for the replaying of historical messages or restarting from a specific message. Persistence is implemented differently depending on the technology used. For example, Kafka employs offset management, partitions, and files on the event broker's disks.</p> <p>The event backbone should be scalable and support clustering of brokers to handle increased loads.</p> <p>There is an inherent peculiarity in the terminology used, as we refer to it as the \"event backbone,\" while in the realm of queueing middleware, the concept of \"event\" does not exist, only messages do. A more appropriate name for this component would be \"messaging middleware\" or \"messaging system.\" This distinction can be confusing, but if we examine the underlying APIs (such as JMS, Kafka, Amazon Kinesis, etc.), we find that the fundamental data structure is a record or message. In modern times, it is accepted that the term \"message\" is used when records are persisted until consumed, with message consumers typically being directly related to the producer, who cares about message delivery and processing. On the other hand, events are persisted as a replayable stream, and event consumers are not tightly coupled to the producer, being able to consume events at any point in time.</p> <p>In most topic implementations, the concept of partition or shard exists to enable parallel scaling of the number of consumers.</p> <p>Finally, two essential services are necessary in this middleware to support the minimal governance requirements of most EDA implementations:</p> <ul> <li>Schema registry: This service maintains control over the contract between producers and consumers by defining the data schema used for message exchange. The message includes metadata about the schema version, and it may also contain the URL of the schema registry. This allows consumers to dynamically retrieve the schema definition at runtime, enabling the replaying of both old and new messages from the same topic or queue.</li> <li>AsyncAPI definition support: The emerging AsyncAPI standard provides a specification for asynchronous communication and binding. To manage AsyncAPI definitions effectively, an API manager solution should be utilized, which includes code generation for consumers based on the defined bindings.</li> </ul> <p>By incorporating these services into the event backbone, organizations can ensure the necessary governance and standardization within their EDA implementations.</p>"},{"location":"concepts/eda/#event-sources","title":"Event sources","text":"<p>As introduced before, in this category, we include applications that were not originally designed to produce events. These applications often use queueing systems and databases. To capture data updates and gain visibility into them, tools like Change Data Capture (CDC) are employed to get records from SQL based database. CDC enables the injection of updated records as messages into queues or topics.</p> Event sources component view <p>For Document oriented database, some change stream softwares are used. Finally for legacy, transactional queueing system, some product, like IBM MQ offers message replication to streaming queue, which can be injected to pub/sub middleware via queue connector.</p> <p>When dealing with event sources, several design considerations need to be taken into account. These considerations include:</p> <ul> <li> <p>Avoiding Message Duplication: It is essential to implement mechanisms that prevent the duplication of messages. Duplicate messages can lead to inconsistent data processing and undesired side effects. Techniques like message deduplication or leveraging unique identifiers can help ensure that only unique and distinct events are processed.</p> </li> <li> <p>Message Reliability: Ensuring that messages are not lost during the event capture process is crucial. Reliable message delivery mechanisms, such as guaranteeing message persistence and employing acknowledgement mechanisms, should be implemented. This ensures that every captured event is successfully propagated and processed downstream.</p> </li> <li> <p>Throughput Optimization: Managing the throughput of event capture is vital for maintaining system performance and stability. Designing efficient mechanisms to handle high volumes of events, such as employing proper batching techniques, parallel processing, or throttling, can help optimize the overall throughput and prevent overloading downstream messaging systems.</p> </li> </ul>"},{"location":"concepts/eda/#event-driven-microservices","title":"Event-driven microservices","text":"<p>Event-driven microservices are small, independent applications that expose a set of operations on a single entity and communicate with each other through web APIs, typically utilizing the RESTful protocol. They are developed by agile, a two-pizza size teams,  following DevOps practices and leveraging cloud-native development approaches.</p> <p>When designing and implementing microservices, we employ domain-driven design (DDD) principles, focusing on the domain and sub-domain, identifying events, aggregates, commands, and bounded contexts. These DDD elements serve as the foundation for building robust microservices.</p> <p>In the context of microservice implementation, it is beneficial to align with the principles outlined in the reactive manifesto. By adhering to these principles, microservices exhibit the following essential characteristics:</p> <ul> <li>Responsive: delivers a consistent quality of service to end users or systems, react quickly and consistently to events happening in the system.</li> <li>Elastic: The system stays responsive under varying workload, it can scale up and down the resource utilization depending of the load to the system.</li> <li>Resilient: stays responsive in the face of failure, this is a key characteristics. Achieving resilience often involves implementing distributed systems and employing techniques such as replication, containment, isolation, and delegation.</li> <li>Message driven: the underlying behavior is to have an asynchronous, message-driven communication. This approach enables loose coupling between application components through the exchange of asynchronous messages. It helps to minimize or isolate the negative effects of resource contention, coherency delays and inter-service communication network latency. It is the base to support the other reactive characteristics. It also helps for isolation and support location transparency.</li> </ul> <p>The relationships between those characteristics are very important to also illustrate the need for exchanging messages asynchronously:</p> <p></p> <p>For more information on reactive systems, see this chapter</p>"},{"location":"concepts/eda/#event-sinks","title":"Event sinks","text":"<p>Event sinks serve as targets for downstream processing, and their characteristics can vary depending on the asynchronous middleware employed.</p> <p>Writing to a sink application presents challenges, particularly when ensuring exactly once delivery is crucial. Using idempotence identifier may be a solution if the sink backend supports it. The implementation needs to consider reliability of the sink backend and what do do in case of communication failure. Implementing appropriate retry logic can increase the chances of successful event delivery, byt dead letter queue needs to be in place too. The last challenge to overcome is when to consider the message from the event backbone to be really processed. There is no XA transaction between the pub/sub middleware and the backend, so we can not use transactional engine. The read commit to the messaging needs to be done after the write operation on the sink backend. But this means duplicate may occur.</p> <p>The following diagram illustrates some of those challenges:</p> <p></p> <p>End to end monitoring may help assessing if some messages may be lost, or if event consumers to sink are behind.</p> <p>Also schema management may be useful, and depending on the sink backend it may be necessary to do some data transformation within the message consumer logic. Finally end to end governance needs to be considered.</p>"},{"location":"concepts/eda/#event-streaming-processing","title":"Event streaming processing","text":"<p>Event streaming processing is a key value proposition of Event-Driven Architecture, particularly in addressing the requirements of modern data pipelines and enabling timely actions on data, especially in the context of AI/ML.</p> <p>In event streaming, events are persisted over a long time period, ordered, timestamped, and immutable. Leveraging these characteristics, we can implement elements of traditional Transform operations performed in batch ETL jobs, but in real-time and consumable by any interested parties. This shift transforms black box ETL into transparent processing within a continuous data pipeline.</p> <p>In the classical data preparation process for ML model development, data scientists typically discover raw data and perform feature engineering by applying transformations, often using Python notebooks. Feature stores are added into the pipeline, to better help model training and feature reuse. Once confident in the model scoring, a data engineer then operationalizes the data pipeline to feed data into the model, ensuring that the ML model is well-prepared for deployment. The model is integrated as a service, and any data issues can impact the model results. Traditionally, the implementation of the data pipeline is done using ETL products. However, in most modern pipelines, integration with streaming data is becoming necessary.</p> <p>The following diagram illustrates this concept of modern data pipeline for preparing events/ features for ML models and a feature store.  </p> <p></p> <p>This diagram showcases an example using an autonomous car system, where each car sends telemetry data to a topic. The raw data is consumed, transformed, and mapped according to the processing defined by the data engineer. This processing may involve computing aggregates and producing, what we used to call in the Complex Event Processing (CEP), synthetic events. The resulting synthetic event is published to another topic and may be relevant for other consumers. From an EDA perspective, this ability to share events for multiple use cases is a significant value proposition. One of the consumers in this scenario is a sink adapter, which writes the events to a feature store. The feature store, which also includes data from a warehouse or data lake, is then utilized by a predictive service to prepare data for the ML model. For instance, the car dispatcher service may leverage the predictive service to obtain estimated travel times. </p> <p>In many EDA deployments, the logic of consuming, processing, and publishing events is commonplace, offering tremendous flexibility in data processing. Data becomes a real-time asset, shareable as needed, and empowers organizations to unlock the full potential of their data resources.</p>"},{"location":"concepts/eda/#scalability","title":"Scalability","text":"<p>Event-Driven architectures are highly scalable by design.  By leveraging an event-driven backbone, these architectures enable the addition or removal of consumers based on the volume of messages awaiting consumption from a topic. This scalability is particularly beneficial in architectures where in-stream transformation is required, such as data pipeline workflows. Messages can be consumed and transformed rapidly, enabling near real-time decision-making processes that require millisecond-level responsiveness.</p> <p>When using a system like Apache Kafka, the data within a given topic is partitioned by the broker. This partitioning enables parallel processing of the data for consumption. Consumers are typically assigned to specific partitions, allowing for efficient and concurrent processing of messages.</p> <p>In addition to scaling up, EDA also offers the flexibility to scale down, even down to zero. The ability to autonomously scale up and down in response to workload demands is referred to as \"elasticity\". This elasticity promotes energy and cost efficiency by dynamically allocating resources only as needed, ensuring optimal utilization and minimizing waste.</p> <p>By leveraging the inherent scalability and elasticity of Event-Driven Architectures, organizations can effectively handle varying workloads, accommodate increased data volumes, and respond to changing demands in a highly efficient and cost-effective manner.</p>"},{"location":"concepts/eda/#resiliency","title":"Resiliency","text":"<p>The reduced inter-dependency between applications in an Event-Driven architecture contributes to increased resiliency. When services experience failures, they have the ability to autonomously restart and recover events, even replaying them if necessary. This self-healing capability reduces the reliance on immediate availability of specific services, ensuring the overall functionality of the system remains intact. More details about consumer offset management and data projection rebuilding after recovery can be found in the Kafka Consumer article.</p> <p>The decoupling of services also means, the services do not need to know who are the consumer, and for consumer who produce the events. There are a number of advantages to this characteristics. For example, even if a service goes down temporarily, events will still be produced or consumed once it has recovered, ensuring guaranteed delivery. This decoupling reduces the impact of service failures on the overall system and enhances fault tolerance.</p> <p>To illustrate this concept, let's consider a shipping company that operates a fleet of container ships equipped with smart IoT devices. These devices collect data about container health, such as temperature and GPS position. At the vessel level, edge computing with a local event backbone enables simple aggregations and correlations before periodically sending the data to a central onshore monitoring platform. If the vessel's network goes offline, preventing the refrigerator containers from sending data, the data can still be collected and transmitted once the network connectivity is restored. This demonstrates resilience between data centers, ensuring data continuity and availability.</p> <p>The following diagram illustrating those concepts with the underlying technologies involved.</p> <p></p> <p>Applications on the right side run in a data center or cloud provider region and receive aggregated data from the Kafka cluster operating on the vessel. Data replication for the topic is achieved using Mirror Maker 2. A second level of real-time analytics can compute aggregates across multiple vessels even when the connection is lost, as the mirroring mechanism retrieves the records upon reconnection. On the vessel level, multiple brokers ensure high availability, while cross-broker data replication ensures data resilience. Real-time analytic components can scale horizontally, even when computing global aggregates, utilizing the Kafka Streams capability of KTables and stores.</p>"},{"location":"concepts/eda/#kappa-architecture","title":"Kappa architecture","text":"<p>The Kappa Architecture is a software architecture designed for processing streaming data.</p> <p>The primary premise behind the Kappa Architecture is that it enables both real-time and batch processing, especially for analytics, using a single technology stack. </p> <p></p> <p>The Kappa Architecture achieves this by leveraging a unified, append-only log that serves as the source of truth for all data. This log can be processed in real-time, as events are generated, as well as in batch mode, allowing for flexible and efficient data processing to meet various analytical requirements.</p> <ul> <li>Streaming processing is the practice of taking action on a series of data at the time the data is created. It can be done with different technologies like Kafka Streams, Apache Sparks streaming, Apache Flink, Redis streaming, or Hazelcast.</li> <li>The serving layer is where OLAP queries and searches are done, most of the time with indexing and other advanced capabilities are needed to offer excellent response time, high throughput and low latency. </li> </ul> <p>It is a simpler alternative to the\u00a0Lambda Architecture \u2013 as all data is treated as if it were a stream. Both architectures entail the storage of historical data to enable large-scale analytics.</p> <p>&gt;&gt;&gt; Next : From SOA to EDA</p>"},{"location":"concepts/fit_for_purpose/","title":"Fit for purpose","text":""},{"location":"concepts/fit_for_purpose/#fit-for-purpose","title":"Fit for Purpose","text":"<p>In this note, we'll outline some of the key criteria to consider and assess during the establishment and ongoing governance of an event-driven architecture (EDA).</p> <p>While not an exhaustive list, these criteria provide a solid foundation for analysis and evaluation:</p> <ol> <li>Business Alignment: Ensure the EDA aligns with the organization's strategic objectives and can effectively support the required business capabilities and use cases.</li> <li>Architectural Patterns: Evaluate the appropriate architectural patterns and design principles to be applied, such as publish-subscribe, event sourcing, or CQRS, based on the specific requirements.</li> <li>Technology Selection: Assess the suitability of various event-driven technologies, messaging platforms, and stream processing tools to meet the performance, scalability, and integration needs.</li> <li>Data Governance: Establish robust data governance practices to manage data models, data lineage, and the flow of events across the EDA ecosystem.</li> <li>Operational Resilience: Evaluate the reliability, fault-tolerance, and disaster recovery capabilities of the EDA to ensure high availability and minimize disruptions.</li> <li>Developer Experience: Consider the ease of use, developer productivity, and overall experience in building and maintaining event-driven applications.</li> </ol>"},{"location":"concepts/legacy-itg/","title":"Legacy Integration","text":"<p>When developing new digital business applications, developers often need to integrate legacy applications and databases into the event-driven architecture. There are two main approaches to directly connect these legacy systems to the event-driven system:</p> <ol> <li> <p>Legacy applications connected with IBM MQ. Developers can directly connect the IBM MQ messaging system to the Kafka event backbone. Refer to the IBM Event Streams getting started with MQ article for guidance on this integration.. The key benefit of this approach is to leverage the transactional capabilities of IBM MQ. This allows writing to both the database and the message queue within the same transaction, ensuring data consistency and integrity:</p> <p></p> </li> <li> <p>Where databases support the capture of changes to data, developers can publish changes as events to Kafka integrating them into the event-driven infrastructure. This can be achieved by leveraging the transactional outbox pattern:</p> <p>The application prepares the events representing the data changes and writes them to a dedicated 'outbox' table, as part of the same transaction that updates the main application tables.</p> <p>A change data capture (CDC) agent then reads the events from the outbox table and publishes them to the Kafka event backbone.</p> <p></p> <p>This approach ensures that the events are consistent with the underlying data changes, as the event publication is part of the same atomic database transaction.</p> </li> </ol> <p>One of the challenges of basic CDC products, is the replication per table pattern, which leads to rebuild the transaction integrity using kafka stream by joining data from multiple topics. The TCC (Transactionally consistent consumer) technology allows Kafka replication to have semantics similar to a relational database. This dramatically increases the types of business logic that can be implemented. Developer can recreate the order of operations in source transactions across multiple Kafka topics and partitions and consume Kafka records that are free of duplicates by including the Kafka transactionally consistent consumer library in your Java applications. </p> <p>TCC allows:</p> <ul> <li>Elimination of any duplicates, even in abnormal termination scenarios</li> <li>Reconstruction of exact transaction order, despite those transactions having been performance optimized and applied out of order to Kafka</li> <li>Reconstruction of exact operation order within a transaction, despite said operations having been applied to different topics and/or partitions.  This is not offered by default Kafka's \"exactly once\" functionality</li> <li>Ability for hundreds+ producers to participate in a single transaction.  Kafka's implementation has one producer create all messages for a transaction, despite those messages going to different topics.</li> <li>Provides a unique bookmark, so that downstream applications can check-point and resume exactly where they last left off if they fail.</li> </ul> <p>We recommend listening to this presentation from Shawn Roberston - IBM, on A Solution for Leveraging Kafka to Provide End-to-End ACID Transactions</p> <p>The second, very important, feature is on the producer side, with the Kafka custom operation processor (or KCOP) infrastructure. KCOP helps you to control over the Kafka producer records that are written to Kafka topics in response to insert, update, and delete operations that occur on source database tables. It allows a user to programmatically dictate the exact key an byte values of the message written to Kafka. Therefore any individual row transformation message encoding format is achievable. Out of the box it includes Avro, CSV, JSON message encoding formats. It is possible to perform column level RSA encryption on certain values before writing. It also permits enriching of the message with additional annotation if needed. Developers have the complete choice over how data is represented. Eg. Can write data in Kafka Compaction compliant format with deletes being represented as Kafka tombstones or can write the content of the message being deleted.</p> <p>It also supports Kafka Header fields for efficient downstream routing without the need for immediate de-serialization. The KCOP allows a user to determine how many messages are written to Kafka in response to a source operation, the content of the messages, and their destination.</p> <ul> <li>Allows for filtering based on column values.</li> <li>Allows for writing the entire row with sensitive data to highly restricted topics and a subset of the columns to wider shared topics.</li> <li>Allows for writing the same message in two different formats to two different topics.  Useful in environments where some consuming applications want JSON, others prefer Avro, both can be produced in parallel if desired.</li> <li>Allows for sending special flags to a monitoring topic.  Eg. when a transaction exceeds $500, in addition to the regular message, a special topic is written to notifying of the large value transaction</li> </ul> <p>The two diagrams above, illustrate a common architecture for data pipeline, using event backbone, where the data is transformed into different data model, that can be consumed by components that act on those data, and move the data document into data lake for big data processing.</p> <p>Finally it is important to note that the deployment of the event streams, CDC can be co-located in the mainframe to reduce operation and runtime cost. It also reduces complexity. In the following diagram, event stream brokers are deployed on OpenShift on Linux on Z and the CDC servers on Linux too.</p> <p></p> <p>This architecture pattern try to reduce the MIPs utilization on the mainframe to the minimum by still ensuring data pipeline, with transactional integrity. </p> <ul> <li>Quality of Service \u2013 autoscaling / balancing between Linux nodes, Resilience.</li> <li>Latency  - memory speed (Network -&gt;  HyperSocket, with memory speed and bandwidth)</li> <li>Reduce MIPS  (avoid Authentication-TLS overhead on z/OS as no network traffic is encrypted)</li> <li>Avoid network spend / management / maintenance between servers</li> <li>Improved QoS for the Kafka service \u2013 inherits Z platform  (Event Streams is the only Kafka variant currently supported on Linix on Z) </li> <li>Reduced complexity / management cost</li> <li>Reduced latency / network infrastructure (apply \u2013 Kafka hop is now  in memory) \u2013 avoids need for encryption </li> </ul> <p>The CDC server uses Transaction Capture Consumer to keep transaction integrity while publishing to kafka topic. CICS Business events are mechanism for declaratively emitting event from CICS routines.</p>"},{"location":"concepts/soa-eda/","title":"From SOA to EDA","text":"Chapter updated 07/2024 <p>This page content reviewed 07/2024 Created 09/2023</p> <p>I will take a imaginary business case of car ride on autonomous vehicles. </p> <p>For this discussion, the simplest requirements include, booking a ride to go from one location to another location, manage a fleet of autonomous cars, get payment from the customer once the trip is done, get estimation for the ride duration and price to present a proposition to the end-users.   </p> <p>The modern synchronous based microservice component view may look like in the following figure:</p> <p></p> <p>We will detail the solution design and implementation in this note.</p> <p>In the early 2000s, when designing this kind of solution, we were using a service oriented architecture, with services being coarse grained and API definition based on service operations. So may be, the search for a trip, booking the trip, get the payment were supported by a unique Ride Service. Monitoring of the rides may be in a separate services, or payment. May be the persistence is within a single SQL database, and project leaders were spending time to design a domain data model, with a lot of relationships, even many-to-many relationships, to support interesting queries. The data model was domain oriented and query oriented. Service operations supported the classical Create, Update, Read and Delete (CRUD) operations, with all the query APIs as part of the same service interface definition.</p> <p></p> <p>Those big data schemas, the coarse grained service interfaces, the synchronous calls bring the coupling between components and generate frictions for change over time. Developers were reluctant to change the interface definitions. Enterprise Service Buses were used, to expose those interfaces so it was easier to reuse, do some data mappings, interface mappings, and implement the gateway patterns. SOA was synonyms of ESB.</p> <p>In the 2010s, some of those business applications were considered as business process management solutions. Even when the process did not involved human tasks, the process was doing SOA services orchestration and BPEL was the technology of choice. Later BPMN engines took the lead, but process applications were fat, monolithic, including User Interfaces (server page serving), data model definition, service integrations, and flow definitions. The process execution was stateful.</p> <p>To have some fun, this is how BPM experts would have modeled, at the high level, the riding application using BPMN:</p> <p></p> <p>We will not dig into the details of this process flow, but what is interesting still, is the sequencing of actions over time which led to identify the commands to perform within the flow, which helped to design the service interfaces. The approach had vertu as it engages business users in modeling the business process. Defining terms, data elements, and business routing rules. The technology enforced creating monolithic applications.  </p> <p>In previous diagram, the failover and compensation flows are not highlighted. But this is where all the architecture decisions have to be made, to select the best implementation choice, to identify when the process execution reached a non-idempotent service, to design the operations for compensation... </p> <p>Which leads to my next argument: there are a lot of people who are currently claiming that EDA will be the silver bullet to address service decoupling, scaling, resilience... From the previous example, I have heard architects claiming the following challenges that only could be addressed with EDA:</p> <ul> <li>Order service is responsible to talk to multiple services, and orchestrates service calls. </li> <li>Orchestration logic should be outside of the microservice. I want to immediatly react on this one, as service orchestration is done to follow a business process. As seen in the process flow above, there is a business logic to route the execution flow among those steps: it is part of the context of the order service to implement the process about an order. We are in the domain-driven design bounded context. The implementation of this orchestration flow can be in code, or in business process engine, in state machine, but at least owned by one team.</li> <li>Strong coupling between the components. The order service needs to understand the semantic of the other services. Which is partially true, as what it really needs to understand, is the interface characteristic of the services. Which includes data model, protocol, SLA, interaction type, communication type, ... The data model is part of the interface contract and is the main argument for coupling. Any change to the API of the downstream services impact the order / orchestrator service. There are way to go over that, by using contract testing, so each change to the contracts can be seen during CI/CD pipeline executions. Now it is true that when a service is used by a big number of other services then API versioning becomes a challenge. On the other side of the argument, on most of simple business application the number of services stands to stay low and interface characteristics do not change that often. Data model coupling still exists in messaging based solutions. Schema registry and the metadata exchanged within the message helps to address those coupling, but it means now, consumers need to be aware of the producer. This is an inversion of control. </li> <li>Choreography of APIs is hard to do. I touched on this point before, but one thing important is to differentiate choreography from orchestration. I have seen arguments for EDA by illustrating how difficult to implement compensation flow with synchronous processing. I am not sure about that, as it was done in the SOA world before. The problem is not the way we interact with service, but by the lack of transaction support in RESTful API as it was possible to do with SOAP WS-Transaction protocol. Asynchronous messaging, event bus,... do not help that much on compensation flow. </li> </ul> Choreography vs Orchestration <p>This different models are used in the context of the Saga pattern, which helps to support a long running transaction between distributed systems that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions:</p> <ul> <li>Orchestration is when one controller is responsible to drive each participant on what to do and when. </li> <li>Choreography applies each service produces and listens to other service\u2019s events and decides if an action should be taken or not.</li> </ul> <p>In the autonomous car ride example, choreography may be used, as it seems that some services are maintaining states of the overall ride transaction: the order, the car dispatching, the route...</p> <ul> <li>Another argument is related to availability: if one of the service is not responding quickly, then all the components in the calling chain are impacted. And in case of outages, if one component fails, error will propagate back to caller chain. There are patterns to handle such issues, like circuit breaker, throttling, or bulkhead. Now this is true, asynchronous processing helps to support failure and slower services.</li> </ul> <p>&gt;&gt;&gt; Next : Message backbone</p>"},{"location":"methodology/getting-started/","title":"Event-driven architecure getting started","text":""},{"location":"methodology/getting-started/#project-approach","title":"Project approach","text":"<p>A common question I got in enterprise architecture workshop and event-driven architecture establishments is how to start. So here are a simple list of things that can be done for a given project:</p> <ol> <li>Start by Event Storming workshop with the line of business subject-matter expert to model the process from an  event point of view</li> <li>Define the domain, sub-domains, commands, aggregates, events and bounded contexts using Domain-driven design.</li> <li>For a given bounded context, assess the life cycle of the business entity and defines events for state change.</li> <li>Externalize the event schema definition in Avro and start adopting automatic construction of POJO or Python classes from schema and then pipeline to push to repository. Any integration needs to leverage schema.</li> <li>In Java thin to use reactive messaging and implement consumer code that manage read-offset commitment</li> </ol>"},{"location":"methodology/getting-started/#demonstration-code","title":"Demonstration code","text":""},{"location":"methodology/getting-started/#pubsub-with-kafka-quarkus-reactive-messaging-and-avro","title":"Pub/sub with Kafka, Quarkus reactive messaging and Avro","text":"<p>EDA quickstart - Java Avro First Demo: The demonstration illustrates the following concepts:</p> <ul> <li>ava applications with Quarkus framework and Microprofile 3.0 reactive messaging to produce and consumer order event messages</li> <li>Avro schema definition, own by the order manager component producer of events</li> <li>App uses health, metrics and OpenAPI extensions</li> <li>Use Apicurio schema registry and the maven plugin to upload new definition to the connected registry.</li> </ul>"},{"location":"methodology/getting-started/#python-producer-consumer-with-avro-and-kafka","title":"Python producer / consumer with Avro and Kafka","text":""},{"location":"methodology/ddd/","title":"Domain Driven Design Summary","text":"<p>This article summarizes the domain driven design methodology to implement and event-driven solution and describes the high level project steps from the Event storming session to microservices design specifications.</p> <p>Domain-driven design was deeply described in Eric Evans's \"Domain Driven Design: Tackling Complexity in the Heart of Software\" book in 2004.</p>"},{"location":"methodology/ddd/#overview","title":"Overview","text":"<p>Domain Driven Design is about understanding the business domain in which the solution has to be developed. The goals for the design may be summarized as:</p> <ul> <li>Improve communication between subject matter experts and developer, make the code readable by human and for long term maintainability. </li> <li>Support highly modular cloud native microservices.</li> <li>Adopt event-coupled microservices - facilitating independent modification and evolution of each microservice separately.</li> <li>Allow applying event-driven patterns such as event sourcing, CQRS and SAGA to address some of the challenges of distributed system implementation: data consitency, transaction cross domains, and complex queries between aggregates managed by different services.</li> <li>Domain Context is important, and used to define the business term definitions within it, to get a unique definition of the main business concepts.</li> <li>For microservice implementation, we need to find the boundary of responsability within the domain, to be able to apply a clear separation of concern pattern. </li> </ul> <p>DDD suits better long term projects, where the domain of knowledge is important and complex, and may be less appropriate for small projects.</p>"},{"location":"methodology/ddd/#getting-started","title":"Getting started","text":"<p>The event-storming methodology is used to model the to-be scenario with an end-to-end business process discovery. Discovering events using event storming simplifies the process discovery and sequencing of tasks. BPMN modeling may also being used to model a process but it has  the tendency to lock the thinking in a flow, while event storming focuses on what happened as facts, and so it easier to get events sequencing. </p> <p>Also events coming from no-where could still be discovered and have their value. Event storming workshop helps to identify domain driven design elements such as:</p> <ul> <li>Domains - sub domains</li> <li>Event Sequence flow.</li> <li>Events \u2013 business term definition: the start of ubiquitous language.</li> <li>Critical events.</li> <li>Business entities, aggregates, value objects</li> <li>Commands</li> <li>Actors - Users \u2013 based on Empathy maps and stakeholders list.</li> <li>Event linkages.</li> <li>Business Policies.</li> <li>Event prediction and probability flows.</li> </ul> <p></p> <p>From there, we complement the first analysis by extending it with the domain driven design elements.</p>"},{"location":"methodology/ddd/#domain-driven-design-steps","title":"Domain driven design steps","text":""},{"location":"methodology/ddd/#step-1-assess-domains-and-sub-domains","title":"Step 1: Assess domains and sub-domains","text":"<p>Domain is what an organization does, and it includes the \"how to\", it performs its operations. Domain may be composed of sub-domains. A \"Core\" domain is a part of the business domain that is of primary importance to the success of the organization, the organization needs to excel at it, to make business impact and difference. The application to be built is within a domain.</p> <p>Domain model is a conceptual object model representing part of the domain to be used in the solution. It includes behavior and data.</p> <p>During the event storming analysis, we define the domains and groups a set of sub-domains together. </p> <p>Core domains represent what will differentiate the company business value. Domains are decomposed in sub-domains. Supporting domains may be integrated and are not core to the main business, but support business processes. Payment for example are classically delegated to SaaS provider or Financial providers.</p> <p>In the following figure, the container shipping domain is what the application we have to develop belongs to,  and is composed of sub-domains like order, shipping, inventory, .... Other domains like weather, CRM, invoice,  are supporting the shipping domain but are not the focus of the design. Here is an example of such domain and subdomains:</p> <p></p> <p>We have three core sub-domains and the rest are supports. Shipping over seas company needs to excel at managing container inventory, managing the shipping, the itineraries, and vessels.</p>"},{"location":"methodology/ddd/#step-2-define-the-application-context","title":"Step 2: Define the application context","text":"<p>At the high level, when doing the analysis, we should have some insight decisions of the top level application to develop, with some ideas of the other systems to interact with. A classical \"system context diagram\" is a efficient tool to represent the application this high level context. The external systems to integrate with, are strongly correlated to the domains discovered from previous step. </p> <p>Each interface to those system needs to be documented using the interface characteristics approach presented by Kim Clark from IBM.</p>"},{"location":"methodology/ddd/#interface-characteristics","title":"Interface Characteristics","text":"<p>The full set of interface characteristics to consider for each system to integrate with is summarized below:</p> <ul> <li> <p>FUNCTIONAL DEFINITION</p> <ul> <li>Principal data objects</li> <li>Operation/function</li> <li>Read or change</li> <li>Request/response objects</li> </ul> </li> <li> <p>TECHNICAL INTERFACE</p> <ul> <li>Transport</li> <li>Protocol</li> <li>Data format</li> </ul> </li> <li> <p>INTERACTION TYPE</p> <ul> <li>Request-response or fire-forget</li> <li>Thread-blocking or asynchronous</li> <li>Batch or individual</li> <li>Message size</li> </ul> </li> <li> <p>PERFORMANCE</p> <ul> <li>Response times</li> <li>Throughput</li> <li>Volumes</li> <li>Concurrency</li> </ul> </li> <li> <p>INTEGRITY</p> <ul> <li>Validation</li> <li>Transactionality</li> <li>Statefulness</li> <li>Event sequence</li> <li>Idempotence</li> </ul> </li> <li> <p>SECURITY</p> <ul> <li>Identity/authentication</li> <li>Authorization</li> <li>Data ownership</li> <li>Privacy</li> </ul> </li> <li> <p>RELIABILITY</p> <ul> <li>Availability</li> <li>Delivery assurance</li> </ul> </li> <li> <p>ERROR HANDLING</p> <ul> <li>Error management capabilities</li> <li>Known exception conditions</li> <li>Unexpected error presentation</li> </ul> </li> </ul> <p>So with a system context diagram and interface characteristics to external system we have a good undestanding of the application context and interaction with external domains.</p>"},{"location":"methodology/ddd/#step-3-define-the-ubiquitous-language","title":"Step 3: Define the ubiquitous language","text":"<p>Eric Evans wrote: \"To communicate effectively, the code must be based on the same language used to write the requirements, the same language the developers speak with each other and with domain experts\"</p> <p>This is where the work from the event storming and the relation with the business experts should help. Domain experts use their jargon while technical team members have their own language tuned for discussing the domain in terms of design. The terminology of day-to-day discussions is disconnected from the terminology embedded in the code, so the ubiquitous language helps to allign knowledge with design elements, code and tests. (Think about technology and Java Eenterprise Edition design patterns jargon versus ShippingOrder, order provisioning, fullfillment... )</p> <p>The vocabulary of that ubiquitous language includes the class names and prominent operation names. The language includes terms to discuss rules that have been made explicit in the model. Be sure to commit the team to exercising that language relentlessly in all communication within the business and in the code. Use the same language in diagrams, writing, and especially speech.</p> <p>Play with the model as we talk about the system. Describe scenarios out loud using the elements and interactions of the model, combining concepts in ways allowed by the model. Find easier ways to say what we need to say, and then take those new ideas back down to the diagrams and code.</p>"},{"location":"methodology/ddd/#entities-and-value-objects","title":"Entities and Value Objects","text":"<p>Entities are part of the ubiquitous language, and represent business concepts that can be uniquely identified by some attributes.  They have a life cycle that is important to model, specially in the context of an event-driven solution.</p> <p>Value Objects represent things in the domain but without identity, and they are frequently transient, created for an operation and then discarded.</p> <p>Some time, in a certain context, a value object could become an entity. As an example, an Order will be a value object in the context of a Fullfilment domain, while a core entity in order management domain.</p> <p>Below is an example of entities (Customer and Shipping Order) and value objects (delivery history and delivery specification):</p> <p></p>"},{"location":"methodology/ddd/#aggregate-boundaries","title":"Aggregate boundaries","text":"<p>An aggregate is a cluster of associated objects that we treat as a unit for the purpose of data changes.  It is a collection of values and entities which are bound together by a root entity (the aggregate root). An entity is most likely an aggregate and every things related to it, define its boundaries. External clients consider the aggregate as a all. It guaranteees the consistency of changes made within the aggregate.</p>"},{"location":"methodology/ddd/#bounded-contexts","title":"Bounded Contexts","text":"<p>Bounded Context explicitly defines the boundaries of our model. A language in one bounded context can model  the business domain for the solving of a particular problem. </p> <p>This concept is critical in large software projects. </p> <p>A Bounded Context sets the limits around what a specific team works on and helps them to define their own vocabulary within  that particular context. When we define a bounded context, we define who uses it, how they use it, where it applies within  a larger application context, and what it consists of, in terms of things like OpenAPI documentation and code repositories.</p> <p>Within a business context every use of a given domain terms, phrases, or sentences, the Ubiquitous Language, inside the boundary has a specific contextual meaning. So order context is a bounded context and groups order, ordered product type, pickup and shipping addresses, delivery specifications, delivery history....</p>"},{"location":"methodology/ddd/#context-maps","title":"Context maps","text":"<p>Bounded contexts are not independent, a solution uses multiple bounded contexts, which are interacting with each others. The touchpoint between business contexts is a contract.</p> <p>To define relationships and integrations between bounded contexts we can consider the nature of the collaboration between teams, which can  be grouped as:</p> <ul> <li>Cooperation: Uses well established communication and control. Change to the APIs are immediately communicated, the integration is both ways and teams solve together integration issues.  If more formal cooperation is needed, the shared kernel pattern can be used and technic like contract testing is used. Sometime the shared kernel can be deployed a mediation flow, canonical model in enterprise service bus. But the scope has to be small anyway.</li> <li>Customer-supplier: the supplier context provides a service to its customers. Upstream or the downstream team can dictate the integration contract. With conformist supplier defines the contract based on its model and domain, and customer conforms to it.  When the customer needs to translate the upstream bounded context model into its own model, it uses anticorruption layer.  Anticorruption layer helps to isolate from messy model, model with high velocity of changes, or when the consumer model is core to its operations. When power is on the consumer side, then the supplier uses open-host service by decoupling its interface from the implementation, and design it for the consumer. This is the published language.</li> <li>Separate ways: no collaboration at all. Which leads to duplicate functionality in multiple bounded contexts.  Avoid this solution when integrating sub-domains.</li> </ul> <p>The context map illustrates the integration between bounded contexts. It is the first high level design of the system components and the models they implement.</p> <p>The diagram below represents a simple view of e-commerce domain with the sub domains and bounded contexts</p> <p></p>"},{"location":"methodology/ddd/#business-operation-api","title":"Business operation API","text":"<p>As part of the commands discovered during the event storming, some of them are related to business operations that could be managed and have business objective in term of cost control or new revenu stream. </p> <p>If needed, some more specific requirement about the APIs can be formalized by capturing all key aspects of API adoption with a lean approach.</p> <p>The characteristics to consider for a development point of view and DDD are:</p> <ul> <li>API product description</li> <li>SLA and expected performance </li> <li>Exposure to developer communities</li> <li>Expected short, medium and long term API management and versioning strategy</li> </ul> <p>For an implementation point of view, when mapping API to RESTful resource the data and verbs need to be defined, When APIs are asynchronous, the description of the delivery channel becomes important.</p>"},{"location":"methodology/ddd/#repositories","title":"Repositories","text":"<p>Repository represents the infrastructure service to persist the root aggregate during its full life cycle. Client applications request objects from the repository using query methods that select objects based on criteria specified  by the client, typically the value of certain attributes. Application logic never accesses storage implementation directly, only via the repository.</p>"},{"location":"methodology/ddd/#event-linked-microservices-design-structure","title":"Event linked microservices design - structure","text":"<p>A complete event-driven microservice specification (the target of this design step) includes specifications of the following elements:</p> <ul> <li> <p>Event Topics</p> <ul> <li>Used to configure the Event Backbone</li> <li>Mapped to the life cycle of the root entity</li> <li>Topics can be chained to address different consumer semantic</li> <li>Single partition for keeping order and support exactly once delivery</li> </ul> </li> <li> <p>Event types within each event topic</p> </li> <li> <p>Microservices:</p> <ul> <li>They may be finer grained than aggregates or mapped to aggregate boundaries.</li> <li>They may separate query and command; possibly multiple queries.</li> <li>They could define demonstration control and serve main User Interface.</li> <li>Reference the related Entities and value objects within each microservice.</li> <li>Define APIs  ( Synchronous or asynchronous) using standards like openAPI. Could be done bottom up from the code, as most of TDD implementation will lead to.</li> <li>Topics and events Subscribed to.</li> <li>Events published / emitted.</li> </ul> </li> <li> <p>List of end to end interactions:</p> <ul> <li>List of logic segments per microservice</li> </ul> </li> <li> <p>Recovery processing, scaling:</p> <ul> <li>We expect this to be highly patterned and template driven not requiring example-specific design.</li> </ul> </li> </ul>"},{"location":"methodology/ddd/#step-4-define-modules","title":"Step 4: Define modules","text":"<ul> <li> <p>Each aggregate will be implemented as some composition of:</p> <ol> <li>a command microservice managing state changes to the entities in this aggregate</li> <li>possibly one or more separate (CQRS) query services providing internal or external API query capabilities</li> <li>additional simulation, predictive analytics or User Interface microservices</li> </ol> </li> <li> <p>The command microservice will be built around a collection of active entities for the aggregate, keyed by some primary key.</p> </li> <li>The separation of each aggregate into specific component microservices as outlined above, will be a complete list of microservices for the build / sprint.</li> <li>Identify the data collections, and collection organization (keying structure) in each command and query microservice for this build.</li> </ul>"},{"location":"methodology/ddd/#step-5-limit-the-context-and-scope-for-this-particular-build-sprint","title":"Step 5: Limit the context and scope for this particular build / sprint","text":"<p>We assume that we are developing a particular build for a sprint within some agile development approach, deferring additional functions and complexity to later sprints:</p> <ul> <li>Working from the initial list of aggregates, select which aggregates will be included in this build</li> <li> <p>For each aggregate the possible choices are:</p> <ol> <li>to completely skip and workaround the aggregate in this build.</li> <li>to include a full lifecycle implementation of the aggregate</li> <li>to provide a simplified lifecycle implementation - typically a table of entities is initialized at start up, and state changes to existing entities are tracked</li> </ol> </li> <li> <p>Determine whether there are simulation services or predictive analytics service to be included in the build</p> </li> <li>Identify the external query APIs and command APIs which this build should support</li> <li>Create entity lifecycle diagrams for entites having a full lifecycle implementation in this build / sprint.</li> </ul>"},{"location":"methodology/ddd/#step-6-generate-microservice-interaction-diagrams-for-the-build","title":"Step 6: Generate microservice interaction diagrams for the build","text":"<ul> <li>The diagram will show API calls initiating state change. They should map the commands discovered during the event storming sessions.</li> <li>It shows for each interaction whether this is a synchronous API calls or an asynchronous event interaction via the event backbone.</li> <li>The diagram labels each specific event interaction between microservices trigerring a state change.</li> <li>Typically queries are synchronous API calls since the caller cannot usefully proceed until a result is returned.</li> <li> <p>From these, we can extract:</p> <ol> <li>a complete list of event types on each event backbone topic, with information passed on each event type.</li> <li>the complete list of \u201clogic segments\u201d for each microservice processing action in response to an API call or initiating event.</li> </ol> </li> <li> <p>When, at the next level of detail, the individual fields in each event are specified and typed, the CloudEvents standard may be used as a starting point.</p> </li> </ul>"},{"location":"methodology/ddd/#step-7-test-drivent-development-meets-ddd","title":"Step 7: Test Drivent Development meets DDD","text":"<p>Test Driven Development is a well established practice to develop software efficiently. Since 2003 and extreme programming practice, it helps when doing refactoring, improve code quality and focus on simple design. As Dan North stated in 2018, it could have been bettwe name 'Example Guided Design' as starting by the test improve the design, and writing tests that map example and end user stories will make it more clear and mapped to the requirements. With DDD, TDD needs to support the ubiquituous language and have tests intent clearly stated.  We can easily adopt the following practices:</p> <ul> <li>Isolate tests per bounded context</li> <li>Adopt collective ownership of code and tests</li> <li>Pair programming between tester and developer</li> <li>Enforce contunuous feedbacks</li> <li>Keep tests in source control with the code and for integration tests in a dedicated folder or even repository. Have their executions as part of CI/CD pipeline.</li> <li>Write test with te Gherkin syntax:</li> </ul> <pre><code>Feature: title of the scenario or test method\nGiven [initial context]\nWhen [event or trigger]\nthen [expected output]\n</code></pre> <p>Here is an example of such structure in Junit 5 test:</p> <pre><code>public void given_order_created_should_emit_event() throws OrderCreationException {\n    assume_there_is_no_event_emitted();\n    OrderEventPayload new_order = ShippingOrderTestDataFactory.given_a_new_order();\n    // when\n    service.createOrder(new_order);\n    // then\n    assertTrue(order_created_event_generated());\n    OrderCommandEvent createdOrderEvent = (OrderCommandEvent)commandEventProducer.getEventEmitted();\n    assertTrue(new_order.getOrderID().equals(((OrderEventPayload)createdOrderEvent.getPayload()).getOrderID()));\n}\n</code></pre>"},{"location":"methodology/ddd/#step-8-address-failover-and-other-nfrs","title":"Step 8: Address failover and other NFRs","text":"<p>Non-Functional Requirements impact architecture decision, technology choices and coding. So address those NFRs as early as possible during requirements and analysis and then design phases. </p> <ul> <li>If a microservice fails it will need to recover its internal state by reloading data from one or more topics, from the latest committed read.</li> <li>In general, command and query microservices will have a standard pattern for doing this.</li> <li>Any custom event filtering and service specific logic should be specified.</li> </ul>"},{"location":"methodology/ddd/#concepts-and-rationale-underlying-the-design-approach","title":"Concepts and rationale underlying the design approach","text":"<p>What is the difference between event information stored in the event backbone and state data stored in the microservices?</p> <p>The event information stored persistently in the event backbone is organized by topic and, within each topic, entirely by event time-of-occurrence. While the state information in a microservice is a list (collection) of all currently active entities of the owning aggregate (e.g. all orders, all car rides etc) and the current state of each such entity. The entity records are keyed by primary key, like an OrderID. While implementing microservice using event sourcing, CQRS, the persisted entity records are complementary to the historically organized information in the event backbone.</p> <p>When is it acceptable to be using synchronous interactions between services instead of asynchronous event interacts through the event backbone?</p> <p>For non-state-changing queries, for which the response is always instantaneously available a synchronous query call may be acceptable and will provide a simpler more understandable interface. Any processing which can be though of as being triggered by some state change in another aggregate should be modelled with an asynchronous event, because as the solution evolves other new microservices may also need to be aware of such event. We do not want to have to go back and change logic in existing service where this event originated to have that microservice actively report the event to all potential consumers.</p> <p>How do we save microservices from having to maintain data collections with complex secondary indexing for which eventual consistency will be hard to implement?</p> <ul> <li>Each command  microservice should do all its state changing updates using the primary key lookup only for its entities.</li> <li>Each asynchronous event interaction between microservices should carry primary entityIds ( orderID, VoyageID, shipID) for any entities associated with the interaction.</li> <li>Each query which might require speciaoized secondary indexing to respond to queries can be implemented in a separate CQRS query service which subscribes to events  to do all internal updating and receives events from the event backbone in a ( Consistent) eventually correct order.</li> <li>This allows for recovery of any failed service by rebuilding it in \"eventually correct\" order.</li> </ul>"},{"location":"methodology/ddd/#more-reading-and-sources","title":"More reading and sources","text":"<ul> <li>Eric Evans' book</li> <li>Scott Millet and Nick Tune's book: \"Patterns, Principles, and Practices of Domain-Driven Design\"</li> <li>Kyle Brown's article: Apply Domain-Driven Design to microservices architecture</li> </ul>"},{"location":"methodology/ddd/eda_assessment/","title":"EDA adoption assessment question","text":"<p>This section lists a set of questions architects can use to assess the EDA deployment and adoption.</p>"},{"location":"methodology/ddd/eda_assessment/#definitions-used-in-the-questions","title":"Definitions used in the questions","text":"Term Definition Event represents a significant change in state or an occurrence that has happened within a system. It is a domain-specific occurrence that reflects a change in the state of the domain model Event backbone core middleware that facilitates asynchronous communication and storage of events. It possesses high availability and replayability capabilities, ensuring the reliable handling and persistence of events. Event schema structured definition that describes the format and structure of an event within an event-driven system. Avro, Json, XSD are common way to define schemas Queue Queues are addressable locations to deliver messages to and store them reliably until they need to be consumed. Topics represent end-points to publish and consume records Multi-tenant a single instance of software serves multiple customers or tenants Domain we use the domain driven design definition :  specific area of knowledge, activity, or interest that is the focus of a particular application Bounded context boundaries within which a particular domain model applies, ensuring clarity and focus."},{"location":"methodology/ddd/eda_assessment/#business","title":"Business","text":"Question Response Assessment What specific business goals are you aiming to achieve with event-driven architecture? How will this architecture align with your overall digital transformation strategy? Is the EDA is a tactical strategy to address some application requirements around resilience and scaling ? or is it more strategic initiative? Do you use event in few application, isolated, mostly supplementary to other development activities? Are you taking any business decision, automatically, on the events created in close real-time? Do you have a strategic use and governance of EDA, promoted cross-organization by IT leadership? Do you have some governance authority for API, Events, business services, business process and data definitions? A center of excellence, or competence. Do you want to move to managed services and scale to zero infrastructure? Do you adopt a multi cloud provider strategy?"},{"location":"methodology/ddd/eda_assessment/#current-architecture","title":"Current architecture","text":"Question Response Assessment Do you use an event-bus currently? Do you use a shared event cluster between domains? Do you use queues in point to point and topics for pub/sub or both? Which technology for queueing and messaging? How many event backbone clusters? How many node per cluster in average? Do you use baremetal nodes or VM based or pure container on k8s? Do you deploy cluster in different regions / Datacenters? Do you consider datalake or lakehouse to be a target sink for events? How many applications are using events / messaging today? Does some of those topics / queues exposed as part of a B2B interaction? Do you classify your business applications into different business impact classification ? if so are all the critical applications using asynchronous communication today? If classifications are in place can you share uptime requirements? What are the DR needs today? Are you using process orchestration product today? Do you have remote applications that needs access data with lower latency? how many of them? Do you have exposed topic or queue to the public internet? How many application are exposing business services today? REST or SOAP? Do you use schema registry? How many of them? Do you use Avro schema, protobuf or JSON or a combination of them? None is possible too. Do you use AsyncAPI document to document topics and schema? Do you generate technical events from file upload to bucket notification and content management? Do you consider logs to be a source of events? Do you use container and orchestration like Kubernetes in your modern application development practices? How long do you need to keep events in event backbone? For audit reason do you need to keep message for longer term ? Do you need to replay event of the past? Do you use some dedicated connector framework to integrate systems to your event backbone? Do you use topic replication between event clusters today? Do you use stateful processing on the events to compute aggregate within time windows? Are you using an API management platfor; today? Does it support AsyncAPI management too?"},{"location":"methodology/ddd/eda_assessment/#future-architecture","title":"Future architecture","text":"Question Response Assessment How many applications will use events / messaging next year? How many applications need to be changed to generate events or consume events? Can you give us the scalability requirements in term of transaction per second on average and at peak? Do you need transactional applications with strong consistency be integrated in your EDA? Do you need to integrate data coming from mainframe? Do you plan to use streaming for consuming event, processing them and generate new events? Do you have a need to manage topic from a single user interface? Do you want to apply event-sourcing as an implementation pattern?"},{"location":"methodology/ddd/eda_assessment/#event-sources","title":"Event sources","text":"Question Response Assessment What are you connecting in term of native applications ? Do you use change data capture today to get event from database? What are data replications ? Where are existing data sources ? Is there any SaaS integration for data source? Do we have requirements for private networking?"},{"location":"methodology/ddd/eda_assessment/#event-sinks","title":"Event Sinks","text":"Question Response Assessment Do you plan to use Lakehouse technology to persist event? Do you plan to use database to persist data from event? Where are you connecting to?"},{"location":"methodology/ddd/eda_assessment/#devops","title":"DevOps","text":"Question Response Assessment How many developers implement asynchronous event generation for business applications? Are schema management part of your DevOps practices? Do you automate event backbone cluster creation with Infrastructure as Code? Do you automate topic and queue creation? Do you have the necessary in-house skills, or will you need to hire or train staff? Do you use Domain-driven design and event storming to model the application logic with events? Do you have automatic process to sccale up the messaging middleware? Do you manage consumer lag behind for the event processing?"},{"location":"methodology/ddd/eda_assessment/#governance","title":"Governance","text":"Question Response Assessment Does the organization has standard practices? Are the API and Event designs and development coordinated? Do you have a process to identify message consumers automatically? Do you track data lineage today from producer to different consumers and to sinks?"},{"location":"methodology/ddd/eda_assessment/#security","title":"Security","text":"Question Response Assessment What security measures are needed to protect event data? Do you need to uniquely control access to topic or queue for each application?"},{"location":"methodology/event-storming/","title":"Event Storming","text":"<p>Often running into solution from simple problem statement leads to wrong technology choices. While fit for purpose practices are very important, understanding the problem from a business domain point of view, understanding the business process to support is the most important work to be done by a system architect and software developers.</p> <p>Domain-driven design has growth in the adoption while implementing service oriented architecture, but are becoming more valuable in the microservice, and EDA design. Before getting to the element of DDD, event storming is a methodology to model the business process from an event point of view, facts that happened, which is far simpler than modeling with Business Process Management Notation. The design team may start as soon as possible with the domain subject matter expert to understand the flow of events, and then extends by adding DDD constructs like aggregates, commands, domain, sub-domains, bounded contexts.</p> <p>In this article we are presenting the end to end set of activities to run a successful event-driven solution using cloud native microservices.</p> <p>The target audience are solution architects, designers (Design Thinking), lead developers.</p>"},{"location":"methodology/event-storming/#event-storming-introduction","title":"Event Storming introduction","text":"<p>Event storming is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. A domain event is something meaningful to the experts that happened in the domain. The workshop focuses on communication between product owner, domain experts and developers.</p> <p>The event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\". This approach is recognized in the Domain Driven Design (DDD) community as a technique for rapid capture of a solution design and improve team understanding of the domain. Domain represents some area of the business that has the analysis focus. This article outlines the method and describes refinements and extensions that are useful in designs for an event-driven architecture. This extended approach adds an insight storming step to identify and capture value adding predictive insights about possible future events. The predictive insights are generated by using data science analysis, data models, artificial intelligence (AI), or machine learning (ML).</p> <p>This article describes in general terms all the steps to run an event storming workshop.</p>"},{"location":"methodology/event-storming/#conducting-the-event-and-insight-storming-workshop","title":"Conducting the event and insight storming workshop","text":"<p>Before conducting an event storming workshop, complete a Design Thinking Workshop in which Personas and Empathy Maps are developed and business pains and goals are defined. The event storming workshop adds more specific design on the events occurring at each step of the process, natural contexts for microservices and predictive insights to guide operation of the system. With this approach, a team that includes business owners and stakeholders can define a Minimal Viable Prototype (MVP) design for the solution.  The resulting design is organized as a collection of loosely coupled microservices linked through an event-driven architecture and one or more event backbones. This style of design can be deployed into multi-cloud execution environments and allows for scaling and agile deployment.</p> <p>Preparations for the event storming workshop include the following steps:</p> <ul> <li>Get a room big enough to hold at least 6 to 8 persons and with enough wall space on which to stick big paper sheets: you will need a lot of wall space to define the models.</li> <li>Obtain green, orange, blue, and red square sticky notes, black sharpie pens and blue painter's tape.</li> <li>Discourage the use of open laptops during the meeting.</li> <li>Limit the number of chairs so that the team stays focused and connected and conversation flows easily.</li> </ul>"},{"location":"methodology/event-storming/#concepts","title":"Concepts","text":"<p>Many of the concepts addressed during the event storming workshop are defined in the Domain Driven Design approach. The following diagrams present the elements used during the analysis.  The first diagram shows the initial set of concepts that are used in the process.</p> <p></p> <p>Domain events are also named business events.  An event is some action or happening which occurred in the system at a specific time in the past. The first step in the event storming process consists of these actions:</p> <ul> <li>Identifying all relevant events in the domain and specific process being analyzed,</li> <li>Writing a very short description of each event on a \"sticky\" note</li> <li>and placing all the event \"sticky\" notes in sequence on a timeline.</li> </ul> <p>The act of writing event descriptions often results in questions to be resolved later, or discussions about definitions that need to be recorded to ensure that everyone agrees on basic domain concepts.</p> <p></p> <p>A timeline of domain events is the critical output of the first step in the event storming process.  The timeline gives everyone a common understanding of when events take place in relation to each other.  You still need to be able to take this initial level of understanding and move it towards an implementation.  In making that step, you must expand your thinking to encompass the idea of a command, which is the action that kicks off the processing that triggers an event.  As part of understanding the role of the command, you will also want to know who invokes a command (actors) and what information is needed to allow the command to be executed.  This diagram show how those analysis elements are linked together:</p> <p></p> <p>One-View Figure.</p> <ul> <li>Actors consume data by using a user interface and use the UI to interact with the system via commands. Actors could also be replace by artificial intelligent agents.</li> <li>Commands are the result of some user decision or policy, and act on relevant data which are part of a Read model in the CQRS pattern.</li> <li>Policies (represented by lilac stickies) are reactive logics that take place after an event occurs, and trigger other commands. Policies always start with the phrase \"whenever...\". They can be a manual step a human follows, such as a documented procedure or guidance, or they may be automated. When applying the Agile Business Rule Development methodology it will be mapped to a Decision within the Decision Model Notation.</li> <li>External systems produce events.</li> <li>Data can be presented to users in a user interface or modified by the system.</li> </ul> <p>Events can be created by commands or  by external systems including IOT devices.  They can be triggered by the processing of other events or by some period of elapsed time. When an event is repeated or occurs regularly on a schedule, draw a clock or calendar icon in the corner of the sticky note for that event. As the events are identified and sequenced into a time line, you might find multiple independent subsequences that are not directly coupled to each other and that represent different perspectives of the system, but occur in overlapped periods of time. These parallel event streams can be addressed by putting them into separate swimlanes delineated by using horizontal blue painter's tape. As the events are organized into a timeline, possibly with swim lanes, you can identify pivotal events. Pivotal events indicate major changes in the domain and often form the boundary between one phase of the system and another. Pivotal events will typically separate (a bounded context in DDD terms). Pivotal events are identified with vertical blue painters tape (crossing all the swimlanes).</p> <p>An example of a section of a completed event time line with pivotal events and swimlanes is shown below.</p> <p></p>"},{"location":"methodology/event-storming/#conducting-the-workshop","title":"Conducting the workshop","text":"<p>The goal of the workshop is to better understand the business problem to address with a future application. But the approach can also apply to finding solutions to bottlenecks or other issues in existing applications. The workshop helps the team to understand the big picture of the solution by building a timeline of domain events as they occur during the business process life span. During the workshop, avoid documenting processing steps. The event storming method is not trying to specify a particular implementation. Instead, the focus in initial stages of the workshop is on identifying and sequencing the events that occur in the solution. The event timeline is a useful representation of the overall steps, communicating what must happen while remaining open to many possible implementation approaches.</p>"},{"location":"methodology/event-storming/#step-1-domain-events-discovery","title":"Step 1: Domain events discovery","text":"<p>Begin by writing each domain event on an orange sticky note with a few words and a verb in a past tense. Describe What's happened. At first just \"storm\" the events by having each domain expert generate an individual lists of domain events. You might not need to initially place the events on the ordered timeline as they write them.  The events must be worded in a way that is meaningful to the domain experts and business stakeholder. You are explaining what happens in business terms, not what happens inside the implementation of the system.</p> <p></p> <p>You don't need to describe all the events in your domain, but you must cover the process that you are interested in exploring from end to end. Therefore, make sure that you identify the start and end events and place them on the timeline at the beginning and end of the wall covered with paper. Place the other events that you identified between these two endpoints in the closest approximation that the team can agree to a sequential order. Don\u2019t worry about overlaps at this point; overlaps are addressed later.</p>"},{"location":"methodology/event-storming/#step-2-tell-the-story","title":"Step 2: Tell the story","text":"<p>In this step, you retell the story by talking about how to relate events to particular personas. A member of the team (often the facilitator, but others can do this as well)  acts this out by taking on the perspective of a persona in the domain, such as a \"manufacturer\" who wants to ship a widget to a customer, and asking which events follow which other events. Start at the beginning of that persona's interaction and ask \"what happens next?\". Pick up and rearrange the events that the team storms.  If you discover events that are duplicates, take those off the board.  If events are in the wrong order, move them into the right order.</p> <p>When some parts are unclear, add questions or comments by using the red stickies.. Red stickies indicate that the team needs to follow up and clarify issues later. Likewise you want to use this time to document assumptions on the definition stickies. This is also a good time to rephrase events as you proceed through the story. Sometimes you need to rephrase an event description by putting the verbs in past tense, or adjusting the terms that are used to relate clearly to other identified events. In this step you focus on the mainline \"happy\" end-to-end path to avoid getting bogged down in details of exceptions and error handling. Exceptions can be added later</p>"},{"location":"methodology/event-storming/#step-3-find-the-boundaries","title":"Step 3: Find the Boundaries","text":"<p>The next step of this part of the process is to find the boundaries of your system by looking at the events. Two types of boundaries can emerge; the first type of boundary is a time boundary. Often specific key \"pivotal events\" indicate a change from one aspect of a system to another. This can happen at a hand-off from one persona to another, but it can also happen at a change of geographical, legal, or other type of boundary. If you notice that the terms that are used on the event stickies change at these boundaries, you are seeing a \"bounded context\" in Domain Driven Design terms. Highlight pivotal events by putting up blue painter\u2019s tape vertically behind the event.</p> <p>The second type of boundary is a subject boundary. You can detect a subject boundary by looking for the following conditions:</p> <ul> <li>You have multiple simultaneous series of events that only come together at a later time.</li> <li>You see the same terms being used in the event descriptions for a particular series of events.</li> <li>You can \u201cread\u201d a series of events from the point of view of a different persona when you are replaying them.</li> </ul> <p>You can delineate these different sets of simultaneous event streams by applying blue painter\u2019s tape horizontally, dividing the board into different swim lanes.</p> <p>Below is an example of a set of ordered domain events with pivotal events and subject swim lanes indicated.  This example comes from applying event storming to the domain of container shipping process. When the reefer container is plugged to the Vessel, it starts to emit telemetries, we change context.</p> <p></p>"},{"location":"methodology/event-storming/#step-4-locate-the-commands","title":"Step 4: Locate the Commands","text":"<p>In this step you shift from analysis of the domain to the first stages of system design.  Up until this point, you are simply trying to understand how the events in the domain relate to one another - this is why the participation of domain experts is so critical.  However, to build a system that implements the business process that you are interested in, you have to move on to the question of how these events come into being.</p> <p>Commands are the most common mechanism by which events are created.  The key to finding commands is to ask the question: \"Why did this event occur?\". In this step, the focus of the process moves to the sequence of actions that lead to events. Your goal is to find the causes for which the events record the effects. Expected event trigger types are:</p> <ul> <li>A human operator makes a decision and issues a command</li> <li>Some external system or sensor provides a stimulus</li> <li>An event results from some policy - typically automated processing of a precursor event</li> <li>The completion of some determined period of elapsed time.</li> </ul> <p>The triggering command is identified in a blue (sticky) note. Command may become a microservice operation exposed via API. The human persona issuing the command is identified and shown in a yellow note. Some events may be created by applying business policies. The diagram below illustrates the manufacturer actor using the place a shipment order command to create a shipment order placed event, as well as .</p> <p></p> <p>It is possible to chain events and commands as presented in the \"one view\" figure above in the concepts section.</p>"},{"location":"methodology/event-storming/#step-5-describe-the-data","title":"Step 5: Describe the Data","text":"<p>You can't truly define a command without understanding the data that is needed for the command to execute in order to produce the event. You can identify several types of data during this step.  First, users (personas) need data from the user interface in order to make decisions before executing a command.  That data forms part of the read model in a CQRS implementation. For each command and event pair, you add a data description of the expected attributes and data elements needed to take such a decision. Here is a simple example for a <code>shipment order placed</code> event created from a <code>place a shipment order action</code>.</p> <p></p> <p>Another important part of the process that becomes more fully fleshed out at this step is the description of policies that can trigger the generation of an event from a previous event (or set of events).</p> <p>Assess if the data element is a main business entity, uniquely identified by a key, supported by multiple commands. It  has a life span over the business process. This will lead to develop an entity life cycle analysis.</p> <p>This first level of data definition helps to assess the microservice scope and responsibility as you start to see commonalities emerge from the data used among several related events.  Those concepts become more obvious in the next step.</p>"},{"location":"methodology/event-storming/#step-6-identify-the-aggregates","title":"Step 6: Identify the Aggregates","text":"<p>In DDD, entities and value objects can exist independently, but often, the relations are such that an entity or a value object has no value without its context.  Aggregates provide that context by being those \"roots\" that comprise one or more entities and value objects that are linked together through a lifecycle. The following diagram illustrates a detailed example of aggregates for shipment of a temperature sensitive product overseas.</p> <p></p> <p>In event storming, we may not be able to get this level of detail during the first workshop, but aggregates emerge through the process by grouping events and commands that are related together.  This grouping not only consists of related data (entities and value objects) but also related actions (commands) that are connected by the lifecycle of that aggregate. Aggregates ultimately suggest microservice boundaries.</p> <p>In the container shipment example, you can see that you can group several commands and event pairs (with their associated data) together that are related through the lifecycle of an order for shipping.</p> <p></p>"},{"location":"methodology/event-storming/#step-7-define-bounded-context","title":"Step 7: Define Bounded Context","text":"<p>In this step, you define terms and concepts with a clear meaning valid in a clear boundary and you define the context within which a model applies. (The term definition can change outside of the business unit for which an application is developed). The following items may be addressed:</p> <ul> <li>Which team owns the model?</li> <li>Which part of the model transit between team organization?</li> <li>What are the different code bases foreseen we need to implement?</li> <li>What are the different data schema ? (database or json or xml schemas)</li> </ul> <p>Here is an example of bounded context that will, most likely, lead to a microservice:</p> <p></p> <p>Keep the model strictly consistent within these bounds.</p>"},{"location":"methodology/event-storming/#step-8-looking-forward-with-insight-storming","title":"Step 8: Looking forward with insight storming","text":"<p>In event storming for Event Driven Architecture (EDA) solutions it is helpful to include an additional method step at this point identifying useful predictive analytics insights.</p> <p>Insights storming extends the basic methodology by looking forward and considering what if you could know in advance that an event is going to occur. How would this change your actions, and what would you do in advance of that event actually happening? You can think of insight storming as extending the analysis to Derived Events.  Rather than being the factual recording of a past event, a derived event is a forward-looking or predictive event, that is, \"this event is probably going to happen at some time in the next n hours\u201d.</p> <p>By using this forward-looking insight combined with the known business data from earlier events,   human actors and event triggering policies can make better decisions about how to react to new events as they occur. Insight storming amounts to asking workshop participants the question: \"What data would be helpful at each event trigger to assist the human user or automated event triggering policy make the best possible decision of how and when to act?\"</p> <p>An important motivation that drives the use of an event-driven architecture is that it simplifies design and realization of highly responsive systems that react immediately and intelligently, that is, in a personalized and context-aware way, and optimally to new events as they occur.  This immediately suggests that predictive analytics and models to generate predictive insights have an important role to play. Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the likely properties of those events. These probabilistic statements are typically generated by using models created by data scientists or using AI or ML. Correlating or joining independently gathered sources of information can also generate important predictive insights or be input to predictive analytic models.</p> <p>Business owners and stakeholders in the event storming workshop can offer good intuitions in several areas:</p> <ul> <li>Which probabilistic insights are likely to lead to improved or optimal decision making and action?<ul> <li>The action could take the form of an immediate response to an event when it occurs.</li> <li>The action could be proactive behavior to avoid an undesirable event.</li> </ul> </li> <li>What combined sources of information are likely to help create a model to predict this insight?</li> </ul> <p>With basic event storming, you look backwards at each event because an event is something that has already happened. When you identify data needed for an actor or policy to decide when and how to issue a command, there is a tendency to restrict consideration to properties of earlier known and captured business events. In insight storming you extend the approach to explicitly look forward and consider what is the probability that a particular event will occur at some future time and what would be its expected property values? How would this change the best action to take when and if this event occurs? Is there action we can take now proactively in advance of an expected undesirable event to prevent it happening or mitigate the consequences?</p> <p>The insight method step amounts to getting workshop participants to identify derived events and the data sources needed for the models that generate them.  Adding an insight storming step using the questions above into the workshop will improve decision making and proactive behavior in the resulting design. Insights can be published into a bus and subscribed to by any decision step guidance.</p> <p>By identifying derived events, you can integrate analytic models and machine learning into the designed solution. Event and derived event feeds can be processed, filtered, joined, aggregated, modeled and scored to create valuable predictive insights.</p> <p>Use the following new notations for the insight step:</p> <ul> <li>Pale blue stickies for derived events.</li> <li>Parallelogram shape to show when events and derived events are combined to enable deeper insight models and predictions. Identify predictive insights as early as possible in the development life cycle. The best opportunity to do this is to add this step to the event storming workshop.</li> </ul> <p>The two diagrams below show the results of the insight storming step for the use case of container shipment analysis.  The first diagram captures insights and associated linkages for each refrigerated container, identifying when automated changes to the thermostat settings can be made, when unit maintenance should be scheduled and when the container contents must be considered spoiled.</p> <p></p> <p>The second diagram captures insights that could trigger recommendations to adjust ship course or speed in response to expected severe weather forecasts for the route ahead or predicted congestion and expected docking and unloading delays at the next port of call.</p> <p></p>"},{"location":"methodology/event-storming/#design-iteration","title":"Design iteration","text":"<p>Attention we are not proposing to apply a waterfall approach, but before starting the deeper implementation with iterations, we want to spend sometime to define in more details what we want to build, how to organize the CI/CD projects and pipeline, select the development, test and product platform, and define epics, user stories, components, microservices... This iteration can take from few hours to a week, depending on the expected MVP goals.</p> <p>For an event-driven solution a MVP for a single application should not take more than 3 to 4 iterations.</p>"},{"location":"methodology/event-storming/#event-storming-to-user-stories-and-epics","title":"Event storming to user stories and epics","text":"<p>In agile methodology, creating user stories or epics is one of the most important elements in project management. The commands and policies related to events can be easily described as user stories, because commands and decisions are done by actors. The actor could be a system as well. For the data you must model the \"Create, Read, Update, Delete\" operations as user stories, mostly supported by a system actor.</p> <p></p> <p>An event is the result or outcome of a user story. Events can be added as part of the acceptance criteria of the user stories to verify that the event really occurs.</p>"},{"location":"methodology/event-storming/#some-practice-notes","title":"Some practice notes","text":"<ul> <li>We can apply event storming at different level: for example at the beginning of a project to understand the high level process at stake. with a big group of people, we will stay at the high level. But it can be used to model a lower level microservice, to assess event consumed and produced.</li> </ul>"},{"location":"methodology/event-storming/#further-readings","title":"Further Readings","text":"<ul> <li>Introduction to event storming from Alberto Brandolini </li> <li>Event Storming Guide</li> <li>Wikipedia Domain Driven Design</li> <li>Eric Evans: \"Domain Driven Design - Tacking complexity in the heart of software\"</li> <li>Domain drive design with event storming introduction video</li> <li>Patterns related to Domain Driven Design by Martin Fowler</li> </ul>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/","title":"Vaccine Delivery Design Session","text":"<p>This section outlines how to apply our design thinking methodology combined with event storming to understand the business problem to address, the main stakeholders, the vaccine delivery process, and then design the future components</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#business-scenario","title":"Business scenario","text":"<p>2020 - As the world awaits the release of the COVID-19 vaccines; we\u2019ve learned four things about the vaccines themselves:</p> <ol> <li>The vaccine once manufactured needs to be in a stable very cold state up until inocculation.</li> <li>The vaccine travels through various sites across the globe from manufacturing sites to remote clinical trial sites, to actual distribution sites, to clinical settings and other points of administration to the population</li> <li>The vaccines when ready for release may rise to be the #1 counterfeit product in the world</li> <li>There will be a need for billions of vaccine doses, so the vaccines will be manufactured in multiple locations and delivered worldwide</li> </ol>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#problem-statement","title":"Problem statement","text":"<p>How do we involve business users, subject matter experts, and developers to put in place an innovative solution to improve shipping vaccine at scale?</p> <p>How to quickly start an implementation without doing three months of requirements gathering and design up-front?</p> <p>This is where Design Thinking, Event Storming and Domain-driven design help enterprises to innovate and implement cloud native solution with best practices. </p> <p>We start our jounrey with the design thinking workshop as illustrated in the following agenda:</p> <ul> <li> <p>Understand</p> <ul> <li>Assess business opportunity</li> <li>Develop empathy map</li> <li>Define challenges and pains</li> <li>Review as-is process with Event Storming</li> </ul> </li> <li> <p>Explore</p> <ul> <li>Big ideas</li> <li>Vision Statement</li> <li>To-Be scenario with Event Storming</li> </ul> </li> <li> <p>Define</p> <ul> <li>Domain-driven design</li> <li>MVP statement</li> <li>High level architecture views</li> <li>Architecture decisions</li> </ul> </li> </ul>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#vaccine-delivery","title":"Vaccine delivery","text":"<p>Opportunity the Covid vaccine delivery needs to maintain the cold chain at very low temperature, so lots of vaccine need to be carried in special containers.</p> <ul> <li>Define the three personas you think are relevant for the process of delivering vaccine worldwide</li> <li>Select one main persona for which the new solution will bring more value</li> <li>For that persona, define the empathy maps</li> <li>You'll also probably have a few ideas about how to solve the persona's problems, so try using ideation to generate as many ideas as possible. Then, organize those ideas into clusters and decide which clusters have the greatest promise.</li> </ul> <p>See next sessions for possible approach</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#design-thinking-applied","title":"Design Thinking applied","text":"<p>Enterprise Design Thinking starts by bringing together a series of traditional design techniques, such as personas, empathy maps, as-is scenarios, design ideation, to-be scenarios, hypothesis-driven design, and minimum viable product (MVP) definition.</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#empathy-maps","title":"Empathy maps","text":"<p>The following three personas were identified:</p> <ul> <li>Julia is the vaccine delivery coordinator, who addres vaccine orders and ensure the end to end delivery runs well</li> <li>Ganesh is the in country minister of health official, focusing to get vaccines for his population</li> <li>Maria the medical coordinator to administer the vaccine to patiens.  </li> </ul> <p></p> <p></p> <p></p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#pain-points","title":"Pain points","text":"<p>The following figure illustrates potential pain points discovered during the design thinking workshop:</p> <p></p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#vision-statement","title":"Vision statement","text":"<p>The team arrives to some vision for two personas:</p> <p></p> Warming <p>It is important to note in the context of this simple exercise, we did not document a full blend design thinking workshop and all the artifacts we develop during such real workshop. The current diagrams illustrate just enough content to give you context for the implementation of the solution.</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#event-storming-applied","title":"Event Storming applied","text":"<p>After the design thinking workshop, we want to model the <code>to be</code> scenario for the vaccine delivery. We use the event storming approach as presented in this article.</p> <p>Starting from the high level process outlined in the problem statement introduction, we have worked with the subject matter experts to discover the end-to-end process using an event point of view. Recalls that events are fact of what happen within the process, and we organize them over a time line running from left to right. Orange stickers represent Events.</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#lab-exercice","title":"Lab exercice","text":"<ol> <li>Focusing on the delivery part, start by an epidemic hot spot is identified,  an order to ship vaccine is created in the ordering system.</li> <li>Brainstorm and put orange stickers on a time line from left to right</li> </ol> <p>Recall: the elements of the event storming sementic are described below. Domain events and actors are the primary elements to consider during the first phase of the workshop </p> <p></p> <p>The first three figures below, represent the start of this business process:</p> <ul> <li>Process start, with raw material and bulk antigen steps</li> </ul> <p></p> <ul> <li>formulation and filling</li> </ul> <p></p> <ul> <li>Vaccine packaging and lot release and order intiation:</li> </ul> <p></p> <p>While the last part of the process, shipment and cold chain monitoring, is addressing the lot delivery up to the target hospital or medical institution:</p> <p></p> <p>Adding blue lines to separate transition between teams or organizartions, helps to define the different domains of the process. In the lot packaging and delivery, we can see the following sub-domains:</p> <ul> <li>The vaccine manufacturing domain</li> <li>The Order management domain</li> <li>The regulatory domain</li> <li>Reefer monitoring domain</li> </ul> <p></p> <p>While for the shipment we have:</p> <ul> <li>The transportation logistic domain, airplane and truck being different domains</li> <li>The order management domain</li> <li>The cold chain monitoring and anomaly detection domain</li> <li>The refrigerator container maintenance</li> </ul> <p>For the minimum viable product we will support part of the order and cold chain monitoring.</p> <p>For the discovered events, we follow the methodology by adding commands, actors and business entities for the lot release and cold chain monitoring domains.</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#domain-driven-design","title":"Domain-driven design","text":"<p>After the event discovery we have events organized over time as a process flow. We can copy those events to  add to actors, commands and entities to start doing some domain driven design.</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#domain-sub-domains","title":"Domain - sub domains","text":"<p>Extracting the domain from the events is not easy, and looking at Pivotal events, and swim lanes should help a lot. It is not that important to do domain representation in details, the most important is to get the SME point of view. When going to term definitions review, it may help that when a term changes of definition, it may demonstrate a change to the domain.</p> <p>We could burn a lot time to do analysis of the relationship between domains.</p> <p>Starting from the domain diagram above, it is important to focus on the domains, we want to address during a MVP: order, reefer, cold chain monitoring.</p> <p>The order needs reefers that are allocaded via an optimum scheduling to maximize refrigerator container usage,  and reduce shipping cost.</p> <p>The vaccine lots produced by the different manufacturers are allocated to an order.</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#entities","title":"Entities","text":"<p>Once we have events, it is quite interesting to look at the type of information related to those events. In the context of components generating those events, there will be a business entity with some states. State changes should generate events. So looking at events can help assess the main business entities.</p> <p>Looking from the order domain, we can derive the following entity diagram.  </p> <p></p> <p>Order will ship lots that includes vaccines. A reefer, or refrigerator containers includes one to many lots.</p> <p>From a reefer monitoring point of view we are interested by sensors and telemetries and lots inside the refrigerator.</p> <p>When doing the order fulfillment planning, we are interested by transportation characteristics, lot inventory, order and reefer inventory.</p> <p></p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#bounded-contexts","title":"Bounded contexts","text":"<p>We have done just enough entity modeling, to start considering the order entity, as part of the order domain, to become an aggregate which is an important concept to define bounded context.</p> <p>Defining bounded context within a domain is not an easy exercise, using aggregate is a good approach and apply the practice of clear separation of concern. The blue stickers represent commands, which may be mapped to API verb, or batch job, or user interface widget... The Yellow stickers are for actors. </p> <p></p> <p>Doing the same from a reefer domain, we get:</p> <p></p> <p>Those two things mean we want to have two separate services to manage order and reefer.</p> <p>Now the vision statement and the deeper analysis of the order domain, have lead the architect to think about an optimization component to compute the  Order fulfillment plan taking into consideration lot inventory, order, reefer inventory, transportation constraints... Now the bounded context is becoming functional and team oriented. We need to engage Operational Research specialist, and bundle the developed mathematical model as a function or service exposed via APIs and consumable by the order management service. We can see from the second entity diagram that the aggregage is becoming the  Order fulfillment plan, so a new service for that makes a lot of sense.</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#cold-chain-monitoring-sub-domain","title":"Cold chain monitoring sub domain","text":""},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#commands-to-apis","title":"Commands to APIs","text":"<p>For the order bounded context we have identified a set of commands that can lead to a development of an open APIs specifications. </p> <p></p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#event-driven","title":"Event Driven?","text":"<p>Well, event storming helps to focus on events. Also a lot of the processing is asynchronous and some of the operation could be long running. Adopting an event-driven implementation of the above services (order and reefer) will make a lot of sense. Also using the event sourcing pattern may help to have an audit trail of what happen to an order, a reefer and the vaccine lots.</p>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#system-context","title":"System context","text":"<p>The application system context looks like in the following diagram.</p> <p></p> <p>The actors are the same as discovered during the design thinking session:</p> <ul> <li>Manufacturing manager</li> <li>Manufacturing QA engineer</li> <li>Refrigerator manager</li> </ul> <p>The external systems to integrate are:</p> <ul> <li>Email server: for the MVP we will not use this integration</li> <li>Flight Tracking: for the MVP we will not use this integration, but it could be to integrate with flightstats.com</li> <li>Manufacturing processing: an ERP based platform, for the MVP we will not use this integration.</li> <li>Other Manufacturer partners</li> </ul>"},{"location":"methodology/event-storming/vaccine-dt-es-ddd/#solution-component-view","title":"Solution component view","text":"<p>From a design point of view, it is interresting to apply the top 2 or 3 diagrams from the C4 models. We use the container as components view for the level of elements that are deployable as a standalone service / docker container.</p> <p></p> <ul> <li>Vaccine manager service: responsibles of managing the vaccine as an entity, and support CRUD operations on the vaccine lots.</li> <li>Order management service: support the operations to manage a Vaccine Shipping Order</li> <li>Reefer manager service: to manage the refrigerator container life cycle.</li> <li>Vaccine Order &amp; Reefer Optimization: to manage an optimized delivery plan for each vaccine order</li> <li>Refrigerator container IoT as a simulator to help for the demonstration</li> <li>Kafka event backbone and event store</li> <li>Reefer monitoring agent: subscribe to telemetry events to assess cold chain violation and detect refrigerator anomalies</li> <li>Anomaly detection scoring: Deployed as a ML service, this is the scoring for predicting a reefer failure.</li> <li>Reefer maintenance business process to dispatch field engineers to do the maintenance of the reefer when anomaly was detected.</li> </ul> Not a complete story <p>Once again we do not pretend to cover the full coverage of a domain driven design approach for this vaccine problem. We focused on a very minimal viable product to get us started.</p>"},{"location":"patterns/","title":"Distributed systems common design patterns","text":"<p>In this set of articles, we will detail some of the most important event-driven design patterns that can be used during an event-driven microservice implementation.</p>"},{"location":"patterns/#microservice-challenges","title":"Microservice challenges","text":"<p>As we have seen in the introduction, modern business application needs to responds to events in real time, as the events happen, so it can deliver better user experiences and apply business logic on those events. The key is to be able to act quickly on those facts. Acting may involve computing real-time analytics or use predictive scoring with machine trained models. </p> <p>On top of that, a modern cloud native application needs to be reactive, responsive by adopting the reactive manifesto. </p> <p>When adopting microservice implementation, the bounded context is defined with events and main business entity.  Each microservice is responsible to manage the operations of creating, updating and reading the data from a  main business entity. This clear separation, leads to exchange data between services, where they used to be integrated in the same monolytic application before.</p> <p>A web application, single page app (SPA), accesses the different microservices using RESTful APIs,  to get the different data views, or create new data elements within one of the service.The following diagram illustrates a simple view of the microservice challenges:</p> <p></p> <p>When the user interface exposes entry form to create new data for one of the business entity,  it calls a REST end point with a HTTP POST operation, then data are saved to data store: document oriented database or SQL based RDBMS.</p> <p>When a microservice (1) needs to access data from another service, it calls another service end point via an HTTP GET. Coupling still exists, via the protocol and the data schema definition level:  a change to the data model, from the source microservice, impacts the service contract and so all the API consumers. This may be acceptable when there is few microservices, but could become a real pain when the number increase.</p> <p>When the microservice dependencies grows in size and complexity, as illustrated by the following figure from Jack Kleeman's Monzo study,  we can see any change to the API from adding a new function will impact all the related services, and will increase cost to maintain such complexity:</p> <p></p> <p>Finally, imagine we need to join data coming from two different services to address an urgent business request. Who will implement the join, service A or B? May be the simplest is to add a service C and  implement a join: it will call two API end points, and try to reconcile data using primary keys  on both business entities. With this clear separation we can scale service C, but we still add coupling:</p> <p></p> <p>With event-driven microservices, the communication point becomes the Pub/Sub layer of  the event backbone. Adopting an event-based implementation, enhances the loose coupling nature of microservices because it decouples producers and consumers at the protocol level. The figure below illustrates, that microservices A and B produces facts about their business entities to topic in the pub/sub event backbone:</p> <p></p> <p>The microservice C consumes those facts to build, its own projections to support the join query.</p> <p>When adopting technology like Kafka as messaging backbone, the data sharing is done via an event log, which can be kept for a very long time period, and is replayable to improve resilience. </p> <p>These event-driven characteristic is an important decision we need to take when starting developing microservices. Microservice applications are a combination of synchronous API, and asynchronous, event-driven, communication. </p> <p>There is something important to add to the discussion, is the fact that there is a coupling by the data.  Messaging structures are defined with JSON schema or Avro schema and managed inside a Schema registry, so messaging applications can still manage the data contract.  </p> <p>The following figure presents a potential structure for event-driven microservice: </p> <p>In Java, APIs are defined using microprofile OpenAPI annotations in one or more JAXRS resource  classes. Those APIs can then be managed within an API management product:</p> <p></p> <p>The rest of the application structure reflects the DDD approach of onion architecture.  The business logic is in its own layer with DDD aggregate, ubiquitous language, services, business rules, etc\u2026 The repository layer supports persisting those aggregates to an external document-oriented  or SQL-based database. As most of the new microservices are message-driven, we are adding a messaging layer  that may use queues or topics. Use queue for request/response exactly once delivery  and topic for sharing facts in append log. In Java, the Microprofile Reactive Messaging is used to define the different publishing  channels, being queue, topic, or both. From the JSON or Avro schema defining the messages or events structure,  developers can build an AsyncAPI specification which may also be managed by an API product.</p>"},{"location":"patterns/#common-patterns","title":"Common Patterns","text":"<p>Adopting messaging (Pub/Sub or queueing) as a microservice communication approach involves using, at least, the following patterns:</p> <ul> <li>Decompose by subdomain: The domain-driven design approach is useful to identify and classify business functions and the corresponding microservices that would be associated with them. With the event storming method, aggregates help to find those subdomains of responsibility. (Source Chris Richardson - Microservices Patterns)</li> <li>Database per service: Each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other services when schema changes occur in the database. The chosen database technology is driven by business requirements. (Source Chris Richardson - Microservices Patterns) The implementation of transactions that span multiple services is complex and enforces using the Saga pattern. Queries that span multiple entities are a challenge and CQRS represents an interesting solution.</li> <li>Strangler pattern: Used to incrementally migrate an existing, monolithic, application by replacing a set of features to a microservice but keep both running in parallel. Applying a domain driven design approach, you may strangle the application using bounded context. But then as soon as this pattern is applied, you need to assess the co-existence between existing bounded contexts and the new microservices. One of the challenges will be to define where the write and read operations occurs, and how data should be replicated between the contexts. This is where event driven architecture helps.</li> <li>Scatter-gather to dispatch work among microservice and gather their results to build an aggregated answer.</li> <li>Event sourcing: persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events.</li> <li>Choreography to do decentralized coordination between services.</li> <li>Orchestration to centralize the coordination between distributed systems with compensation flow.</li> <li>Command Query Responsibility Segregation: helps to separate queries from commands and help to address queries with cross-microservice boundary.</li> <li>Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable.</li> <li>Event reprocessing with dead letter: event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone.</li> <li>Transactional outbox: A service command typically needs to update the database and send messages/events. The approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns)</li> </ul>"},{"location":"patterns/#strangler-pattern","title":"Strangler pattern","text":""},{"location":"patterns/#problem","title":"Problem","text":"<p>How to migrate a monolithic application to a microservice based architecture without doing the huge effort of redeveloping the application from a blank slate. Replacing and rewriting an existing application can be a huge investment. Rewriting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes.</p> <p>The figure below illustrates a typical mainframe application, with external Java based user interface connected to the mainframe via iop/corba and with three different applications to manage product, order and customer.</p> <p></p>"},{"location":"patterns/#solution","title":"Solution","text":"<p>The approach is to use a \"strangler\" interface to dispatch a request to new or old features. Existing features to migrate are selected by trying to isolate sub components.</p> <p>One of main challenges is to isolate the data store and disover how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model.</p> <p>The facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP).</p> <p>The following figure illustrates an implementation using an event driven solution with data replication to synchronize the write model to the read model on the mainframe.</p> <p></p>"},{"location":"patterns/#choreography","title":"Choreography","text":"<p>Service choreography and service orchestration are two different approaches to coordinating the interactions and behaviors of distributed services. </p> <p>Service choreography refers to a decentralized coordination approach where each participating service in a system collaborates autonomously, following predefined rules or processes. Services interact directly with each other, exchanging messages and performing actions based on the received messages.There is no central coordinator. It emphasizes the autonomy and independence of each service, allowing for more flexibility and scalability. Service choreography can be compared to a dance, where each participant knows their steps and responds to the cues from others.</p> <p>It is more complex to implement to manage the exception and error recovery. The process model on top of this choreography is more difficult to understand and model. Reverse engineering the service traces to build the story of what happened to a given business transaction.</p> <p>With event routing based middlewares, the event pushes to services participing to the choreography is done via routing rules. So the business process flow is controlled by the routing policies.</p> <p>Here is an example of choreography with squares representing events from the different services, and topics to keep events related to the same entity: CarRideOrders, AutonomousCar, Customers, Payments</p> <p></p> <p>The detail of the flow is described in this design note</p>"},{"location":"patterns/#orchestration","title":"Orchestration","text":"<p>Service orchestration, is a centralized coordination approach where a central entity, known as an orchestrator or process engine, controls and coordinates the interactions between the participating services. The orchestrator defines the flow of activities, sequences, and conditions that govern the execution of the services. It actively manages the interactions, determining the order of service invocations, handling exception and compensation scenarios, and ensuring the overall process logic is followed. Service orchestration provides a higher level of control and visibility over the execution flow but may introduce a single point of failure or performance bottleneck. In the world of SOA, BPEL engines were used for orchestration, with long running transactions. Java supports </p>"},{"location":"patterns/#scatter-gather","title":"Scatter-gather","text":"<p>This is a common pattern to facilitate parallel processing and aggregation of results from multiple sources.  It was created with the adoption of distributed systems, and is well adapted for microservice solutions. The data to process is scattered across different nodes or microservices, processed independently, and then gathered or collected back together for further processing or presentation.</p> <p>The pattern consists of two main phases: the scatter phase and the gather phase.</p> <ul> <li> <p>Scatter Phase: In this phase, a central entity (often referred to as the scatterer) divides the incoming data into smaller chunks and distributes them across multiple processing units or nodes. When using event the data is the event payload. Each processing unit is responsible for handling a portion of the data independently, without requiring coordination with the others. This enables parallel processing and helps in optimizing performance.</p> </li> <li> <p>Gather Phase: Once the scatter phase completes, the processing units return their results to the central entity or a designated gathering component. The gatherer collects the intermediate results and combines them to produce the final result or output. The combination can involve simple aggregation, such as summing up values, or more complex operations like merging and sorting.</p> </li> </ul> <p></p>"},{"location":"patterns/#transactional-outbox","title":"Transactional outbox","text":"<p>When distributed transaction is not supported by the messaging middleware (like current Kafka version), it is important to ensure consistency between the records in the database and the events published. In the reference implementation we used the approach to publish to the topic as soon as an order is received via the API and then the same code, is consuming this events to persist to the database. With this approach if write to the topic operation fails, the application can return an error to the user, if the write operation to the database fails, the code can reload from the non-committed record. </p> <p>But there is another solution presented by the transactional outbox. For detailed information about this pattern see the documentation of the pattern in Chris Richardson's site: Transactional outbox.</p> <p>To summarize this pattern, the approach is to use an <code>outbox</code> table to keep the messages to sent and a message relay process to publish events inserted into database to the event backbone. In modern solution this relay is a change data capture agent. The following schema illustrates the approach:</p> <p></p>"},{"location":"patterns/#integration-patterns","title":"Integration patterns","text":""},{"location":"patterns/#one-way-asynchronous","title":"One-way asynchronous","text":"<p>The sender does not expect a response, and share its information to a message channel</p> <p></p>"},{"location":"patterns/flow-architectures/","title":"Flow architecture","text":"<p>From the James Urquhart's book: Flow architecture and personal studies.</p> <p>As more and more of our businesses \u201cgo digital\u201d, the groundwork is in place to fundamentally change how real-time data is exchanged across organization boundaries. Data from different sources can be combined to create a holistic view of a business situation.</p> <p>Flow is networked software integration that is event-driven, loosely coupled, and highly adaptable and extensible.</p> <p>Value is created by interacting with the flow, and not just the data movement.</p> <p>Since the beginning of IT as an industry, we are digitizing and automating the exchanges of value, and we spend a lot of time and money to execute key transactions with less human intervention. However, most of the integrations we execute across organizational boundaries today are not in real time. Today, most, perhaps all\u2014digital financial transactions in the world economy still rely on batch processing at some point in the course of settlement.</p> <p>There is no consistent and agreed-upon mechanism for exchanging signals for immediate action across companies or industries.</p> <p>It is still extremely rare for a company to make real-time data available for  unknown consumers to process at will.</p> <p>This is why modern event-driven architecture (EDA) will enable profound changes in  the way companies integrate. EDAs are highly decoupled architectures, meaning there  are very few mutual dependencies between the parties at both ends of the exchange.</p>"},{"location":"patterns/flow-architectures/#1-flow-characteristics","title":"1- Flow characteristics","text":"<ul> <li>Consumer applications requests data streams through self-service interfaces, and get the data continuously.</li> <li>Producers maintain control of relevant information to transmit and when to transmit.</li> <li>Event packages information of data state changes, with timestamp and unique ID.  The context included with the transmitted data allows the consumer to better understand the nature of that data. CloudEvent helps defining such context.</li> <li>The transmission of a series of events between two parties is called an event stream.</li> </ul> <p>The more streams there are from more sources, the more flow consumers will be drawn to  those streams and the more experimentation may be done. Over time, organizations will find  new ways to tie activities together to generate new value.</p> <p>Composable architectures allow the developer to assemble fine grained parts using  consistent mechanisms for both inputting data and consuming the output. In contextual architectures, the environment provides specific contexts in  which integration can happen. Developer must know a lot about the data that is available,  the mechanism by which the data will be passed, the rules for coding and deploying   the software.</p> <p>EDA provides a much more composable and evolutionary approach for building event and data streams.</p>"},{"location":"patterns/flow-architectures/#2-business-motivations","title":"2- Business motivations","text":"<ul> <li>Do digital transformation to improve customer experiences. Customers expect their data to  be used in a way that is valuable to them, not just to the vendors. Sharing data between organizations  can lead to new business opportunities. This is one of the pilard of Society 5.0. </li> </ul> <p>The Japan government defined Society 5.0 as \"A human-centered society that balances economic  advancement with the resolution of social problems by a system that highly integrates cyberspace and physical space\".</p> <ul> <li>Improve process automation, to drive efficiencies and profitability. The most limiting constraint in the  process hides any improvements made to other steps. Finding constraints is where value stream mapping shines: it uses lead time (queue time) and actual time to do the work. EDA will help to get time stamp and data  for steps in the process that are not completely in scope of a business process: may be cross business boundaries.</li> <li>Extract innovative value from data streams. Innovation as better solution for existing problem, or as new solution to emerging problems.</li> </ul> <p>To improve process time, software needs accurate data at the time to process the work. As business evolve, having a rigid protocol to get the data, impacts process time. A business will need to experiment with new data sources  when they are available and potentially relevant to their business.</p> <p>Stream processing improves interoperability (exchange data)</p> <p>Innovation is not adaptation. Companies must adapt constantly just to survive, like adding features on a product to pace with competition. Digital transformation aimed at avoiding competitive disruption is not innovation.</p> <p>As the number of stream options grows, more and more business capabilities will be  defined in terms of stream processing. This will drive developers to find easier ways  to discover, connect to, and process streams.</p>"},{"location":"patterns/flow-architectures/#enabler-for-flow-adoption","title":"Enabler for flow adoption","text":"<ul> <li>Lowering the cost of stream processing: Integration costs dominate modern IT budgets. For many integrations, the cost of creating interaction between systems is simply too high for what little value is gained. With common interfaces and protocols that enable flows, the integration cost will be lower and people will find new uses for streaming that will boost the overall demand for streaming technologies. The Jevons paradox at work</li> <li>Increasing the flexibility in composing data flows: \"pipe\" data streams from one processing  system to another through common interfaces and protocols.</li> <li>Creating and utilizing a rich market ecosystem around key streams. The equities markets have all moved entirely to electronic forms of executing their marketplaces. Health-care data streams for building services around patient data. Refrigerators streaming data to grocery delivery services. </li> </ul> <p>Flow must be secure (producers maintain control over who can access their events),  agile (change schema definitions),  timely (Data must arrive in a time frame that is appropriate for the context to which it is being applied),  manageable and retain a memory of its past. </p> <p>Serverless, stream processing, machine learning, will create alternative to batch processing.</p>"},{"location":"patterns/flow-architectures/#3-market","title":"3- Market","text":"<p>SOA has brought challenges for adoption and scaling. Many applications have their own interfaces and even protocols to expose their functionality, so most integrations need protocol and  data model translations. </p> <p>The adoption of queues and adaptors to do data and protocol translation was a scalable solution.  Extending this central layer of adaptation was the Enterprise Service Bus, with intelligent pipes / flows. </p> <p>Message queues and ESBs are important to the development of streaming architectures but to support scalability and address complexity more decoupling is needed between  producers and consumers.</p> <p>For IoT MQTT is the standard for messaging protocols in a lightweight pub/sub  transport protocol. MQTT supports 3 service levels: 0 - at most once, 1- at least once, 2 - exactly once. It allows for messaging between device to cloud and cloud to device. It supports for persistent sessions  reduces the time to reconnect the client with the broker. The MQTT broker manages a list of topics, which enable it to identify groups of subscribers interested in a collection of messages.</p> <p>For event processing three type of engines:</p> <ul> <li>Functions (including low-code or no-code processors): AWS lambda, Knative eventing, Flink, Storm. Mendix and Vantiq have event-driven low code platform.</li> <li>log-based event streaming platforms: Apache Kafka, Apache Pulsar, AWS Kinesis, and Microsoft Azure Event Hubs. Topic becomes a system of record, as an event-sourcing pattern implementation.</li> <li>real-time stateful systems: Digital twins are software agents supporting the problem domain in a stateful manner. Behavior is supported by code or rules, and relationship between agents.  Agents can monitor the overall system state. Swim.ai builds its model dynamically from the event stream and provides built-in machine learning capabilities that enable both continuous learning and high performance model execution.</li> </ul> <p>Mainstream adoption of flow itself will be five to ten years from now (2020). Flow will have to prove that  it meets security criteria for everything from electronic payments, to health-care data, to classified  information. The CNCF\u2019s CloudEvents specification, for instance, strongly suggests payloads be encrypted. There is no single approach to defining an event with encryption explicitly supported  that can be read by any event-consuming application (MQTT, AMQP, have different encryption and TLS add more for  TCP connection).</p> <p>Consumers need assurances that the data they receive in an event is valid and accurate, a practice  known as data provenance.  </p> <p>Data provenance is defined as \u201ca record of the inputs, entities,  systems, and processes that influence data of interest, providing a historical record of the  data and its origins\"</p> <p>Provenance has to be maintained by the producer as a checksum number created by parsing the event data, and encrypted by the producer's key. CloudEvent has metadata about the message. When sent to Kafka they are  immutable record. Now the traceability of the consumers in kafka world is a major challenge. Blockchain may also be used to track immutable record with network parties attest its accuracy.</p> <p>Applying the concept of data loose value over time, it is important to act on data as early as possible, close to creation time. After a period of time data becomes less valuable.</p> <p>Two time factors are important in this data processing: latency (time to deliver data to consumers) and retention (time to keep data). For latency try to reduce the number of network segment between producer and consumers. Considering edge computing as a way to bring event processing close to the source. The event processing add time to the end to end latency. Considering constraining the processing time frame.</p> <p>Retention is a problem linked to the business requirements, and we need to assess for each topic how long an event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state,  projection views... keeping for too long, increase the cost of storage, but also the time to rebuild data  projection. </p> <p>Finally, producers will want to trust that only authorized consumers are using the events they produce. Also it may be possible to imagine a way to control the intellectual property of the data so producer can keep  its ownership. Data consumption should be done via payment like we do with music subscription.</p>"},{"location":"patterns/flow-architectures/#flow-patterns","title":"Flow patterns:","text":""},{"location":"patterns/flow-architectures/#collector-pattern","title":"Collector pattern","text":"<p>The Collector pattern is a pattern in which a single consumer subscribes to topics from multiple producers. </p>"},{"location":"patterns/flow-architectures/#distributor-pattern","title":"Distributor pattern","text":"<p>Each event in a stream is distributed to multiple consumers. It could be a hard problem to solve when doing it across geographically distributed systems. Edge computing can be used to distribute  streaming endpoints closer to the consumers that need those streams. Alternate  is to move the event processing close to the source. For many Distributor use cases,   partitioning the data by region is probably smart, and flow interfaces will need to take   this into account.</p>"},{"location":"patterns/flow-architectures/#signal-pattern","title":"Signal pattern","text":"<p>The Signal pattern is a general pattern that represents functions that exchange data between  actors based on a distinct function or process, in can be seen as a traffic cop.   It supports multiple producers and multiple consumers. The signal pattern is supported  by multiple event processing each handling one aspect of the event processing.</p> <p>Stream processing may route event streams between several distributed edge computing services as  well as core shared services, but then we need management layer to get global view of the systems. They need to be integrated into observability tool. But the \"single pane of glass\" is often a lure as distributed systems require distributed decision-making. More local solutions are more agile, flexible and better address local problems for improved resilience.  </p> <p>One of the challenge of complex adaptive systems is that any agent participating in  the system has difficulty seeing how the system as a whole operates, because of its limited connections to other neighbor agents.</p>"},{"location":"patterns/flow-architectures/#facilitator-pattern","title":"Facilitator pattern","text":"<p>A specialized form of Signal pattern, facilitator is a \"broker\" to match producers' events to consumers' demands. It is like matching sellers with buyers.</p>"},{"location":"patterns/flow-architectures/#4-identifying-flow-in-your-business","title":"4- Identifying flow in your business","text":"<p>The classical usee case categories:</p> <ul> <li> <p>Addressing and discovery: In modern complex systems environments, multiple systems need to be informed of the new entity, be able to utilize to assign work to it. Addressing and discovery happens across organizational boundaries (for example in real-time inventory SKU is used to identify item for both supplier and retailers). To seek such use cases, look at tracking problems like who or what is involved in a problem domain that is difficult to scale. With event stream centric approach, A&amp;D is done via a registry service used by new agents to indicate their existence, and the service publishes an event to a topic to broadcast the information about the new agent. A second option is to use a discovery service to watch specific event stream for certain transmissions that indicate the presence of an agent. Swim.ai continuously process and analyze  streaming data in concert with contextual data to inform business-critical, operational decisions. See also SwimOS or Apache Flink.</p> </li> <li> <p>Command and control: sources are connected to key decision-making and action-taking services to complete a business task. So they are everywhere in any business. A typical example of such use case, is the Supervisory Control And Data Acquisition, used in manufacturing, or energy production. Try to ask: where does the organization depend on timely responses to changes in state? C&amp;C can be supported by centralized control with events come from multiple sources to stateless or stateful services, to apply real-time analysis and decision-making algorithms to those streams. Output events are published to sinks for future processing. Scaling with a centralized control approach is not straightforward, as getting the right events to the right processing instances can be a challenge. Also when we need the compute global aggregates by looking at the state of various agents in the systems is more complex, as it needs to integrate with stateful stream processing. Actions can be triggered by state changes, triggers that fire at specific times, or even API requests from other applications or services.</p> </li> </ul> <p>An alternate is to use distributed control, like applying the decision-making logic at the edge. </p> <ul> <li>Query and observability: querying or monitoring individual agents or specific groups of agents. The problem is to locate the right agent target of the query, and get current state or history from that agent. </li> <li>Telemetry and analytics: focuses on understanding systems behavior, and get real-time big data insights (e.g. Click streams).  Need to assess which insights require understanding the emerging behavior of a system of agents emitting vast amounts of data.</li> </ul> <p>Interesting presentations:</p> <ul> <li>The Voxxed Athens 2018 - Eventing, Serverless, and the Extensible Enterprise by Clemens Vasters:</li> </ul> <p></p>"},{"location":"patterns/flow-architectures/#5-model-flow","title":"5- Model Flow","text":"<p>Use Event storming to build a timeline of events that are required to complete a complex task, and to get and understanding of the people, systems, commands and policies that affect the event flow.  The Event Storming process is a highly interactive endeavor that :brings subject matter experts  together to identify the flow of events in a process or system</p> <ol> <li>Identify the business activities that you would like to model in terms of event flow</li> <li>Begin by asking the group to identify the events that are interesting and/or required for that business activity</li> <li>Place events along a timeline from earliest action to latest action</li> <li> <p>Capture:</p> <ul> <li>The real-world influences on the flow, such as the human users or external systems that produce or consume events</li> <li>What commands may initiate an event</li> <li>What policies are activated when an event takes place. A policy usually initiates a new command. Events always result in a policy action unless the event is generating output.</li> <li>What are the outputs from the event flow.</li> </ul> </li> </ol> <p>When designing the solution assess:</p> <ul> <li>When the event is simply to broadcast facts for consumption by interested parties. The producer contract is  simply to promise to send events as soon as they are available.</li> <li>If consumers can come and go, and experiment with the consumption of a stream with little risk of consequences if they choose not to continue</li> <li>When event is part of an interaction around an intent, requiring a conversation with the consumer</li> <li>Is the event a stand-alone communication, discrete, or is it only useful in a context that includes a series of events.  Series applications are where log-based queueing shines</li> <li>Is the processing involve one simple action per event, or is there a group of related actions, a workflow, required to complete processing</li> </ul> <p>When building a flow-ready application for messaging, the \u201ctrunk-limb-branch-leaf\u201d pattern is a  critical tool to consider: use edge computing to distribute decision-making close to the related  groups of agents, computing local aggregates, and propagate to larger more central flows. Using messaging middleware to manage interaction between agents, to isolate message distribution to just the needed servers and agents, and propagate aggregates to the trunk, greatly reducing traffic between the original agents and the core.</p> <p>Another consideration is to assess if the consumers need to filter events from a unique topic before doing its own processing, in this case the event payload may include metadata and URL to get the payload. If the metadata indicates an action is required, the consumer can then call the data retrieval URL.</p> <p>Whether or not you include payload data depends a bit on the volume of events being published and the security and latency requirements of your application.</p> <p>Log-based queues can play the role of \u201csystem of record\u201d for both event values and sequence, especially for systems that need both the most recent event and an understanding of the recent history of events received</p> <p>For single action processing, serverless, knative eventing are technologies to consider. Solution needs to route events to the appropriate processor. But if your event processing needs require  maintaining accurate state for the elements sending events then stateful streaming platform are better fit.</p> <p>For workflow, modern solutions, simplify creating and managing process definitions independent of the actions taken in that process. It supports for stepping an event through multiple interdependent actions. Workflow may require to wait for another related event occurs or a human completes his action.</p>"},{"location":"patterns/flow-architectures/#6-today-landscape","title":"6- Today landscape","text":"<ul> <li>Standards are important for flow:  TLS, WebSockets, and HTTP from IETF, MQTT and AMQP from OASIS,  CloudEvents and the Serverless Working Group from CNCF</li> <li> <p>Open sources projects: </p> <ul> <li>Apache Kafka and Apache Pulse for log-based queueing</li> <li>Apache Beam, Flink, Heron, Nifi, Samza, and Storm for stream processing</li> <li>Apache Druid as a \u201cstream-native\u201d database</li> <li>gRPC may play a key role in any future flow interface</li> <li>NATS.io, a cloud-native messaging platform</li> <li>Argo, a Kubernetes-based workflow manager that theoretically could act as the core of an event-driven process automation bus</li> </ul> </li> <li> <p>Opportunities:</p> <ul> <li>Data provenance and security for payloads passed between disparate parties</li> <li>Tracking event data distribution across the world wide flow. Where does the data generated by an event end up being consumed or processed?</li> <li>Platforms for coordinating event processing, aggregation, and synchronization between core data center event processors, edge computing environments, and end-user or IoT devices</li> <li>Monetization mechanisms for all types of event and messaging streams</li> </ul> </li> </ul> <p>The adoption of a technology is not the delivery that makes it valuable, but the ecosystem that consumes it.</p> <p>Look at existing streams and determine how to add value for the consumers of that stream.  Can you automate valuable insights and analytics in real time for customers with shared needs?  Would it be possible to recast the stream in another format for an industry that is currently  using a different standard to consume that form of data? </p>"},{"location":"patterns/api-mgt/","title":"API Management - AsyncAPI","text":"<p>This chapter summarizes some of the best practices for introducing API Management in development practices and architecture patterns within an enterprise setting.</p>"},{"location":"patterns/api-mgt/#moving-from-a-pure-api-gateway-to-an-api-management-system","title":"Moving from a Pure API Gateway to an API Management system","text":"<p>An API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. A gateway is used in the following patterns:</p> <ul> <li>As a Security Gateway, placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels.</li> <li>As an API Gateway, both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring.</li> <li>To provide connectivity (HTTP) and mediation (XML, JSON) services in the internal network, close to the system of record. </li> </ul> <p>API Management gives enterprises greater flexibility when reusing the functionality of API integrations and helps save time and money without trading off security. An API Management system supports a broader scope of features for API lifecycle management, including: </p> <ul> <li>API lifecycle management to activate, retire, or stage an API product.</li> <li>API governance with security, access, and versioning.</li> <li>Analytics, dashboards, and third party data offload for usage analysis.</li> <li>API socialization based on a portal for the developer community that allows self-service and discovery.</li> <li>An API developer toolkit to facilitate the creation and testing of APIs.</li> </ul>"},{"location":"patterns/api-mgt/#classical-pain-points","title":"Classical Pain Points","text":"<p>Some of the familiar pain points that indicate the need for a broader API Management product include:</p> <ul> <li>Current API details like endpoints, request/response message format, error conditions, test messages, and SLAs are not easily available or not well documented.</li> <li>Difficult to tell which subscribers are really using the API and how often, without building a custom solution.</li> <li>Difficult to differentiate between business-critical subscribers versus low value subscribers.</li> <li>Managing different lines of business and organizations is complex.</li> <li>No dynamic scaling built into the solution, which often means making hardware investments for maximum load or worst availability scenarios.</li> <li>Difficult to evolve the APIs, such as moving from SOAP based web services to RESTful services to GraphQL</li> <li>No support for AsyncAPI to automate and formalize the documentation or code generation of any event-driven APIs.</li> <li>The need to ensure consistent security rules</li> <li>Integrating CI/CD pipelines with the API lifecycle</li> </ul>"},{"location":"patterns/api-mgt/#enterprise-apis-across-boundaries","title":"Enterprise APIs across boundaries","text":"<p>If you consider a typical API Management product, it includes a set of components as presented in the figure below that could be deployed on an on-premise Kubernetes-based platform or in several Cloud provider regions. APIs served by different applications or microservices can be deployed in multiple regions but still be managed by one central API Management server.</p> <p></p> <p>Here is how it works:</p> <ol> <li>An API developer signs on to the API Management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the sync API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to the API Management system. He can also create Async APIs from a messaging system by binding channels to topics or queues and define the message payload.</li> <li>The API Product Manager or API owner signs on to the API Management cloud services account and accesses the API Management component. She includes the sync API endpoint to existing API products, and plans and specifies access controls. She publishes the API to the developer portal for external discovery by application developers.</li> <li>An application developer accesses the developer portal and uses search to discover any available APIs.</li> <li>The application developer uses the API in an application and deploys that application to the device</li> <li>The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway also validates access policies with API Management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the backend. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API Management system.</li> <li>API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics.</li> </ol> <p>Deployment across cloud providers could look like the diagram below, using API Connect in Cloud Pak for Integration:</p> <p></p> <p>On the left side (green boxes), the consumers of the API register to a Developer portal to get the metadata about the API they want to consume.  The Developer portal also allows them to test the APIs to better understand how they work. The consumers register their applications as API subscribers. These applications can run on the cloud or on-premise. </p> <p>The API Gateway services colocated with the target application services to reduce latency. These would be deployed as StatefulSet on an OpenShift cluster, which means as a set of pods with consistent identities. Identities are defined as:</p> <ul> <li>Network: A single stable DNS and hostname.</li> <li>Storage: As many VolumeClaims as requested.</li> </ul> <p>The StatefulSet guarantees that a given network identity will always map to the same storage identity. </p> <p>An API Gateway acts as a reverse proxy, and, in this case, exposes the <code>Inventory APIs</code>, enforcing user authentication and security policies,  and handling traffic monitoring, rate limiting, and statistics. The API Gateway can also perform  transformations and aggregate various services to fulfill a request. </p> <p>The Developer Portals can be separated, or centralized depending on API characteristics exposed from different  clouds (e.g. different Developer Portals for internal and external APIs). In the example above, the portal  is deployed onto the Cloud Provider as a container inside an OpenShift cluster. The Analytic service is   also a StatefulSet and gets metrics from the gateway.</p> <p>The application microservices are accessing remote services and this traffic can also go to the API gateway.  Those services will also integrate with existing backend services running on-premise, whether they are deployed  or not on OpenShift. </p> <p>In the diagram above, the management service for the API Management product is depicted as running on-premise  to illustrate that it is a central deployment to manage multiple gateways.</p> <p>Gateway services, Developer Portal services, and Analytics services are scoped to a single region, unlike the  Management System, which can communicate across availability zones.</p> <p>The different API Management services run on OpenShift which can help ensure high availability of each of  the components. </p>"},{"location":"patterns/api-mgt/#open-api","title":"Open API","text":"<p>Within the API Management system, the OpenAPI document can be created top-down using a Swagger-based UI or bottom up using Annotation in the Java JAXRS resource classes.  Either way, the API can be uploaded to the API Management product.</p> <p>The important parts are to define the operations exposed and the request / response structure of the data model. </p>"},{"location":"patterns/api-mgt/#support-for-async-api","title":"Support for Async API","text":"<p>Cloud Pak for Integration (CP4I) 2021.1, which includes APIConnect V10, also provides some support for the AsyncAPI specification. </p> <p>AsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs.</p> <p>The AsyncAPI specification (currently at 2.0.0) establishes standards for events and EDAs, covering everything \"from documentation to code generation,  and from discovery to event management\" asyncapi.com/docs. </p> <p>The goal is to enable the creation of better tooling in the message-driven space, better governance of asynchronous APIs,  and standardization in documentation for asynchronous APIs. In short, Async API is to Message-driven architectures  what OpenAPI is to REST APIs. </p> <p>While OpenAPI is the recommended practice for RESTful APIs, adopting AsyncAPI is the recommended practice  for event-driven APIs.</p>"},{"location":"patterns/api-mgt/#asyncapi-documents","title":"AsyncAPI Documents","text":"<p>An AsyncAPI document is a file in either YAML or JSON format that defines and annotates the different components of an event-driven API.  For example, AsyncAPI can formally describe how to connect to a Kafka cluster, the details of the Kafka topics (channels in AsyncAPI),  and the type of data in messages. AsyncAPI includes both formal schema definitions and space for free-text descriptions See this blog.</p> <p>Here is what it looks like: </p> <p></p> <p>You may have noticed the similarities with OpenAPI. AsyncAPI was initially an adaptation of OpenAPI, which does not include support for the Message Queuing Telemetry Transport (MQTT) and the Advanced Message Queuing Protocol (AMQP). </p> <p>The creator of the AsyncAPI specification, Fran M\u00e9ndez, describes what he did at first with just OpenAPI to make up for the lack of MQTT and AMQP support: \"paths were AMQP topics, GET was SUBSCRIBE, and POST was PUBLISH--and ugly hack at best...\". This forced him to write additonal code to support the necessary EDA-based documentation and code generation.</p> <p>Many companies use OpenAPI, but in real-world situations, systems need formalized documentation and code generation support for both REST APIs and events. </p> <p>Here are the structural differences (and similarities) between OpenAPI and AsyncAPI:</p> <p></p> <p>Source: https://www.asyncapi.com/docs/getting-started/coming-from-openapi</p> <p>Note a few things:</p> <ul> <li>AsyncAPI is compatible with OpenAPI schemas, which is quite useful since the information flowing in the events is very similar to the one the REST APIs have to handle in requests and responses.</li> <li>The message payload in AsyncAPI does not have to be an AsyncAPI/OpenAPI schema; it can be any value such as an Apache Avro schema, which is considered to be one of the better choices for stream data, where data is modeled as streams (see the section titled Why Use Avro for Kafka below). </li> <li>The AsyncAPI server object is almost identical to its OpenAPI counterpart with the exception that scheme has been renamed to protocol and AsyncAPI introduces a new property called protocolVersion.</li> <li>AsyncAPI channel parameters are the equivalent of OpenAPI path parameters, except that AsyncAPI does not have the notion of query and cookie, and header parameters can be defined in the message object. </li> </ul>"},{"location":"patterns/api-mgt/#describing-kafka-with-asyncapi","title":"Describing Kafka with AsyncAPI","text":"<p>It is also important to understand how to use AsyncAPI from the perspective of a Kafka user. The following section summarizes what is described in more detail in this article written by Dale Lane.</p> <p>First, there are some minor differences in terminology between Kafka and AsyncAPI that you should note: </p>"},{"location":"patterns/api-mgt/#comparing-kafka-and-asyncapi-terminology","title":"Comparing Kafka and AsyncAPI Terminology","text":""},{"location":"patterns/api-mgt/#the-asyncapi-document","title":"The AsyncAPI Document","text":"<p>Considering the structure in the diagram above, let's look at some of the parts of an AsyncAPI document:</p>"},{"location":"patterns/api-mgt/#info","title":"Info","text":"<p>The Info section has three parts which represent the minimum required information about the application: title, version, and description (optional), used as follows:</p> <pre><code>asyncapi: 2.0.0\n...\ninfo:\n  title: Account Service\n  version: 1.0.0\n  description: This service is in charge of processing user signups\n</code></pre> <p>In the description field you can use markdown language for rich-text formatting.</p>"},{"location":"patterns/api-mgt/#id","title":"Id","text":"<p>The Id is the application identifier. It can be a URN (recommended) or URL. The important thing is that it must be a unique ID for your document. Here is an example:</p> <pre><code>asyncapi: '2.0.0'\n...\nid: 'urn:uk:co:usera:apimgmtdemo:inventoryevents'\n...\n</code></pre>"},{"location":"patterns/api-mgt/#servers","title":"Servers","text":"<p>The servers section allows you to add and define which servers client applications can connect to,  for sending and receiving messages (for example, this could be a list of server objects, each uniquely identifying a Kafka broker).</p> <p>Here is an example, where you have three Kafka brokers in the same cluster:</p> <pre><code>servers:\n  broker1:\n    url: andy-broker-0:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the first broker\n  broker2:\n    url: andy-broker-1:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the second broker\n  broker3:\n    url: andy-broker-2:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the third broker\n</code></pre> <p>The example above uses the <code>kafka</code> protocol for the different brokers, which is a popular protocol for streaming solutions, but the protocol can be any.  For example, the most common protocols include: <code>mqtt</code> which is widely adopted by the Internet of Things and mobile apps, <code>amqp</code>, which is popular  for its reliable queueing, <code>ws</code> for WebSockets, frequently used in browsers, and <code>http</code>, which is used in HTTP streaming APIs.</p>"},{"location":"patterns/api-mgt/#difference-between-the-amqp-and-mqtt-protocols","title":"Difference Between the AMQP and MQTT Protocols","text":"<ul> <li> <p>AMQP was mainly popularized by RabbitMQ. It provides reliable queuing, topic-based publish-and-subscribe messaging, flexible routing, transactions, and security.  The main reasons to use AMQP are reliability and interoperability. AMQP exchanges route messages directly in fanout form, by topic, and also based on headers.</p> </li> <li> <p>MQTT The design principles and aims of MQTT are much more simple and focused than those of AMQP. it provides publish-and-subscribe messaging (no queues, in spite of the name) and was specifically designed for resource-constrained devices and low bandwidth,  high latency networks such as dial up lines and satellite links. Basically, it can be used effectively in embedded systems.</p> </li> </ul> <p>When using a broker-centric architecture such as Kafka or MQ, you normally specify the URL of the broker.  For more classic client-server models, such as REST APIs, your server should be the URL of the server.</p> <p>One limitation in AsyncAPI documents is that you cannot include multiple Kafka clusters (such as a production cluster and clusters for dev, test, and staging environments)  in a single document.</p> <p>One workaround is to list all brokers for all clusters in the same document, and then rely on the description or extension  fields to explain which ones are in which cluster. This is, however, not recommended because it could  interfere with code  generators or other parts of the AsyncAPI ecosystem which may consider them as all being  members of one large cluster.</p> <p>So, as a best practice, avoid this workaround and stick to one cluster per AsyncAPI document.</p> <p>NOTE: As with OpenAPI, you can add additional attributes to the spec using the x- prefix, which identifies an entry  as your own extension to the AsyncAPI specs.</p>"},{"location":"patterns/api-mgt/#security","title":"Security","text":"<p>If the Kafka cluster doesn\u2019t have auth enabled, the protocol used should be <code>kafka</code>. Otherwise,  if client applications are required to provide credentials, the protocol should be <code>kafka-secure</code>. </p> <p>To identify the type of credentials, add a security section to the server object. The value you put there  is the name of a securityScheme object you define in the components section.</p> <p>The types of security schemes that you can specify aren\u2019t Kafka-specific. Choose the value that describes  your type of approach to security.</p> <p>For example, if you\u2019re using SASL/SCRAM, which is a username/password-based approach to auth, you could describe  this as <code>userPassword</code>. Here is an example:</p> <pre><code>asyncapi: 2.0.0\n...\nservers:\n  broker1:\n    url: localhost:9092\n    description: Production server\n    protocol: kafka-secure\n    protocolVersion: '1.0.0'\n    security:\n    - saslScramCreds: []\n...\ncomponents:\n  securitySchemes:\n    saslScramCreds:\n      type: userPassword\n      description: Info about how/where to get credentials\n      x-mykafka-sasl-mechanism: 'SCRAM-SHA-256'\n</code></pre> <p>The description field allows you to explain the security options that Kafka clients need to use.  You could also use an extension (with the -x prefix).</p>"},{"location":"patterns/api-mgt/#channels","title":"Channels","text":"<p>All brokers support communication through multiple channels (known as topics, event types, routing keys, event names or other terms depending on the system). Channels are assigned a name or identifier.</p> <p>The channels section of the specification stores all of the mediums where messages flow through.  Here is a simple example:</p> <pre><code>channels:\n  hello:\n    publish:\n      message:\n        payload:\n          type: string\n          pattern: '^hello .+$'\n</code></pre> <p>In this example, you only have one channel called hello. An app would subscribe to this channel  to receive hello {name} messages. Notice that the payload object defines how the message must be structured.  In this example, the message must be of type string and match the regular expression <code>'^hello .+$'</code> in the  format hello {name} string.</p> <p>Each topic (or channel) identifies the operations that you want to describe in the spec.  Here is another example:</p> <pre><code>asyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      operationId: someUniqueId\n      summary: Interesting messages\n      description: You can get really interesting messages from this topic\n      tags:\n      - name: awesome\n      - name: interesting\n      ...\n</code></pre> <p>For each operation, you can provide a unique id, a short one-line text summary, and a more detailed description  in plain text or markdown formatting.</p>"},{"location":"patterns/api-mgt/#bindings","title":"Bindings","text":"<p>AsyncAPI puts protocol-specific values in sections called bindings.</p> <p>The bindings sections allows you to specify the values that Kafka clients should use to perform the operation.  The values you can describe here include the consumer group id and the client id.</p> <p>If there are expectations about the format of these values, then you can describe those by using  regular expressions: </p> <pre><code>asyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      ...\n      bindings:\n        kafka:\n          groupId: \n            type: string\n            pattern: '^[A-Z]{10}[0-5]$'\n          clientId:\n            type: string\n            pattern: '^[a-z]{22}$'\n          bindingVersion: '0.1.0'\n</code></pre> <p>You can instead specify a discrete set of values, in the form of enumerations:</p> <pre><code>asyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      ...\n      bindings:\n        kafka:\n          groupId: \n            type: string\n            enum:\n            - validone\n            - validtwo\n          clientId:\n            type: string\n            enum:\n            - validoption\n          bindingVersion: '0.1.0'\n</code></pre>"},{"location":"patterns/api-mgt/#messages","title":"Messages","text":"<p>A message is how information is exchanged via a channel between servers and applications. According to the AsyncAPI specifications, a message must contain a payload and may also contain headers. The headers may be subdivided   into protocol-defined headers and header properties defined by the application which can act as supporting  metadata. The payload contains the data, defined by the application, which must be serialized into a supported format (JSON, XML, Avro, binary, etc.). Because a message is a generic mechanism, it can support multiple interaction patterns such as event, command, request, or response.</p> <p>As with all the other levels of the spec, you can provide background and narrative in a description field for the message:</p> <pre><code>asyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    ...\n    subscribe:\n      ...\n      message:\n        description: Description of a single message\n</code></pre>"},{"location":"patterns/api-mgt/#summary","title":"Summary","text":"<p>In short, the following diagram summarizes the sections described above:</p> <p></p> <p>For more information, the official AsyncAPI specifications can be found here.</p>"},{"location":"patterns/api-mgt/#why-use-avro-for-kafka","title":"Why Use Avro for Kafka?","text":"<p>Apache Avro is \"an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks\" (https://www.confluent.io/blog/avro-kafka-data/). Avro is a great fit for stream data because it has the following features:</p> <ul> <li>Direct mapping to and from JSON, but typically much faster than JSON, with much smaller encodings</li> <li>Compact format, making it more efficient for high-volume usage</li> <li>Bindings for a wide variety of programming languages</li> <li>A rich, extensible schema language defined in pure JSON</li> </ul>"},{"location":"patterns/api-mgt/#a-simple-api-management-demo","title":"A simple API Management Demo","text":"<p>Besides a new, event-driven approach to its API model, ACME Inc needs a way to securely provide self-service access to different versions of its APIs,  to enable their developers to discover and easily use these APIs, and to be able to redirect API calls based on several criteria.</p> <p>IBM API Connect (APIC) is a complete and scalable API Management platform that allows them to do these things, in addition to exposing, managing, and monetizing  APIs across clouds. API Connect is also available with other capabilities as an IBM Cloud Pak\u00ae solution.</p> <p>The API Management demo demonstrates three main areas of interest: </p> <ul> <li>Version Control</li> <li>API Documentation &amp; Discovery</li> <li>Redirecting to different APIs based on certain criteria</li> </ul>"},{"location":"patterns/api-mgt/#types-of-apis","title":"Types of APIs","text":"<p>An API is a set of functions that provide some business or technical capability and can be called by applications by using a defined protocol.  Applications are typically mobile or web applications, and they use the HTTP protocol. An API definition is composed of paths, and can be one  of the following types:</p>"},{"location":"patterns/api-mgt/#rest-api","title":"REST API","text":"<p>A REST API is a defined set of interactions that uses the HTTP protocol, typically by using JSON or XML as the data format that is exchanged.  For example, a data request might use an HTTP GET method, and a data record might use an HTTP POST method. The choice of data format depends  on the type of application that is calling the API. JSON is commonly used for web pages or mobile applications that present a user interface (by using JavaScript or HTML), whereas XML has been traditionally used for machine-to-machine scenarios, although that is changing.</p> <p>In IBM API Connect, you can create REST APIs by using user interface functions to create models and data sources (top-down).  These are then used by your REST API definition and exposed to your users. API Connect also supports a bottom-up approach where  you can import existing APIs into the system and benefit from the API Management capabilities of the product.</p> <p>Alternatively, you can expose and secure your existing APIs by using a Proxy or Invoke policy.</p> <p>In either case, you can configure your API definition either by using the API Manager, or by writing an OpenAPI  definition file and publishing it using either API Manager or the command line interface.</p>"},{"location":"patterns/api-mgt/#soap-api","title":"SOAP API","text":"<p>API Connect also allows you to create SOAP API definitions that are based on an existing Web Services Description Language (WSDL) file.  You can use this facility to benefit from the capabilities that are provided by API Connect, which include analytics and mapping between variables.  You can also expose the API by using the Developer Portal for any existing SOAP services in your organization, including any SOAP services that are part of a service-oriented architecture (SOA) or Enterprise Service Bus (ESB) infrastructure.</p> <p>You can create SOAP API definitions through either the command line interface, or through the API Manager UI.</p>"},{"location":"patterns/api-mgt/#graphql","title":"GraphQL","text":"<p>GraphQL is a query language for APIs that gives an application client greater control over what data it retrieves in an API request when compared with a REST API request. IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs. See this dedicated section.</p>"},{"location":"patterns/api-mgt/#asyncapi","title":"AsyncAPI","text":"<p>We already address in detail in previous sections.</p>"},{"location":"patterns/api-mgt/#api-manager","title":"API Manager","text":"<p>You can manage your APIs by using API Connect's API Manager UI:</p> <p></p> <p>The API Manager UI allows you to manage private internal APIs as well as public external APIs. API Manager is an on-premises offering that provides the capabilities required to externalize and manage your services as REST or SOAP APIs.</p>"},{"location":"patterns/api-mgt/#developer-portal","title":"Developer Portal","text":"<p>The Developer Portal is a convenient place to share APIs with application developers. After a Developer Portal has been enabled through the API Manager, and one or more API Products have been published,  application developers can browse and use the APIs from the Developer Portal dashboard, as shown below:</p> <p></p> <p>The Developer Portal can be used as is when it is first enabled, or it can be customized to fit the corporate branding and design requirements of a particular organization.  You can configure the Developer Portal for test and development purposes, or for internal use only.</p>"},{"location":"patterns/api-mgt/#create-capability","title":"Create Capability","text":"<p>Developer can leverage API Connect's Create capability, to build APIs (top-down) with a built-in Swagger (OpenAPI) editor or using a simple guided model-driven approach.  APIC allows developers to create a new API from scratch or an API based on the schema of an existing data source, such as a database.</p>"},{"location":"patterns/api-mgt/#creating-a-rest-proxy-api-from-an-existing-target-service","title":"Creating a REST proxy API from an existing target service","text":"<p>As mentioned earlier, it is also possible to create a REST proxy API from an existing target service (bottom-up) that you import into the API Connect system. Essentially, if you have an existing REST service that you want to expose through an IBM API Connect API definition, you can create a proxy API and specify the target endpoint by using the API Manager.</p>"},{"location":"patterns/api-mgt/#explore-capability","title":"Explore Capability","text":"<p>Developers can use API Connect's Explore capability to quickly examine their new APIs and try their operations.</p> <p>Developers can add their APIs to the API Connect server and have the choice of running them on the cloud or on-premise, as mentioned above.  This can be done through API Connect's UI or with the provided CLI. </p> <p>Developers and Administrators can then connect to the API Connect Server and see their running APIs, including those that were created as well as  those that were added based on existing data sources. </p>"},{"location":"patterns/api-mgt/#subscribing-to-an-api-in-the-developer-portal","title":"Subscribing to an API in the Developer Portal","text":"<p>To subscribe to an API, from the Developer Portal, the develper clicks <code>API Products</code> to find and subscribe to any available APIs:</p> <p></p> <p>In the example above, an API called FindBranch Version 2.0, which is contained in the FindBranch Auto Products product is available. Clicking <code>Subscribe</code> enables the developer to use the API:</p> <p></p> <p>Under the Application heading, the developer can click <code>Select App</code> for the new application:</p> <p></p> <p>From the next screen below, the developer can click Next:</p> <p></p> <p>Doing this shows the screen below, which confirms to the developer that his or her applicatin is now subscribed to the selected API under the selected plan. Pressing <code>Done</code> in the next screen completes the subscription.</p> <p></p>"},{"location":"patterns/api-mgt/#testing-an-api-in-the-developer-portal","title":"Testing An API in the Developer Portal","text":"<p>To test an API, the developer clicks <code>API Products</code> in the Developer Portal dashboard to show all available products:</p> <p></p> <p>Clicking a product, such as FindBranch auto product, for example, and then clicking the FindBranch API from the provided list shows the available API operations. </p> <p></p> <p>The developer can click <code>GET/details</code>, for instance, to see the details of the GET operation: </p> <p></p> <p>Clicking the <code>Try it</code> tab and pressing <code>Send</code> allows the developer to test the API to better understand how it works:</p> <p></p> <p>As you can see above, this shows the request/response from invoking the API. API Connect shows a returned response of 200 OK and the message body, indicating that the REST API operation call was successful.</p>"},{"location":"patterns/api-mgt/#api-product-managers","title":"API Product Managers","text":"<p>Administrators or API Product Managers can make changes such as adding corporate security polices or transformations before publishing the APIs. They can also control the visibility and define the rate limit for the APIs. </p> <p>Once the APIs are ready, API Product Managers can expose them for developer consumption and self-service in the Developer Portal. Developers who have signed up can discover and use any APIs which were exposed. </p> <p>Because the APIs were documented while they were being created using OpenAPI notation, developers can not only view the APIs but also try them out, as demonstrated in the demo. API Connect provides source code examples  in several different languages that show how to use the API operations. Languages supported include Curl, Ruby, Python, Java, Go, and Node.js.</p>"},{"location":"patterns/api-mgt/#testing","title":"Testing","text":"<p>Besides the small individual tests that you can run from the Developer Portal to understand how an API works, as explained above, it is possible to use API Connect Test and Monitor to effortlessly generate  and schedule API test assertions. This browser based tool also provides dashboards for monitoring API test assertions and for receiving alerts on the health of your APIs.</p> <p>You can use an HTTP Client within API Connect Test and Monitor to generate a request to an API and get the expected results:</p> <p></p> <p>You enter any required parameters and authorization token and press <code>Send</code> to send the request. </p> <p>API Connect Test and Monitor then shows you the response payload as a formatted JSON object:</p> <p></p> <p>To generate a test, you then click <code>Generate Test</code>, and in the dialog that appears name the test and add it to a project.  When you click the checkmark in the upper right corner of the dialog, API Connect Test and Monitor automatically generates test assertions based on the response payload.</p> <p>You can add, delete, or modify these assertions, or even add new ones from a broad selection:</p> <p></p> <p>You can also reorder the assertions by dragging and dropping them:</p> <p></p> <p>Although coding is not required, if you wish, you change the underlying code by clicking <code>CODE</code> to enter the code view:</p> <p></p> <p>Once you have your test arranged the way you want it, you can run it to generate a test report, which highlights successes and failures:</p> <p></p>"},{"location":"patterns/api-mgt/#test-scheduling","title":"Test Scheduling","text":"<p>IBM API Connect Test and Monitor also lets you automate test scheduling at regular intervals. To do this, you first save any test you may have open and exit out of the composer. Then publish your test to prepare it for scheduling:</p> <p></p> <p>Once your test is published, you can click <code>Schedule</code> to schedule regular test intervals. Test and Monitor allows you to tweak the test schedule as necessary. You can create a new run, name it, and choose the times you wish to run your tests:</p> <p></p> <p>Clicking <code>Save Run</code> schedules the tests. </p> <p>Finally, you can access the dashboard to monitor the health of your API:</p> <p></p>"},{"location":"patterns/api-mgt/#graphql-apis","title":"GraphQL APIs","text":"<p>IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API.  The Developer Portal also supports testing GraphQL APIs. </p>"},{"location":"patterns/api-mgt/#advantages-of-graphql-over-rest-apis","title":"Advantages of GraphQL over REST APIs","text":"<p>GraphQL provides the following particular advantages over REST APIs:</p> <ul> <li>The application client can request only the data that it needs. For example, when retrieving bank account records, request only the account number and current balance for each account, but not the customer name and address details.  With a REST API request, either the backend REST service must provide separate endpoints for different data subsets, or the application client must retrieve the complete records and then discard the unwanted data.</li> <li>The application client can retrieve multiple related resources in a single request. For example, a customer's bank account record might include an array that references other finance products that the customer holds.  If an application client wants to retrieve the bank account details for a specific customer, and details of the other finance products for that customer, then with a REST API the client would first retrieve the bank account details,  then make separate requests for each of the other products. A GraphQL API can be designed to allow the client to retrieve all this information in a single request.</li> <li>However, this flexibility presents rate limiting challenges, because two seemingly very similar requests might return vastly different amounts of data, and what might have required multiple REST API requests, each counting towards the rate limit, might require only a single GraphQL API request. It is important therefore that rate limiting controls are imposed that reflect the amount of data that is retrieved. API Connect extends the GraphQL standard by providing, in a GraphQL API definition, the ability to configure a range of settings that are used to calculate the complexity of a GraphQL request and an associated cost that counts towards the rate limit.</li> </ul>"},{"location":"patterns/api-mgt/#creating-a-graphql-proxy-api","title":"Creating a GraphQL proxy API","text":"<p>The  demo also showed how API Connect supports GraphQL. To create a GraphQL proxy API from an existing GraphQL server, click <code>Add</code>, then <code>API (from REST, GraphQL or SOAP)</code>:</p> <p></p> <p>Then select, <code>From existing GraphQL service (GraphQL proxy)</code>:</p> <p></p> <p>Give it a name and complete the rest of the parameters, especially the GraphQL Server URL field which, of course, cannot be left blank. </p> <p></p> <p>Notice that, once you enter the URL, API Connect will find the service and automatically detect the schema. </p> <p></p> <p>You can click <code>Next</code> and API Connect will attempt to connect to the GraphQL server, introspect it, and pull in any objects (in this case, queries and operations) that it can find on that server:</p> <p></p> <p>The gateway will sometimes provide warnings (17 in the example above), which are really just recommendations on how you can optimize the query. </p> <p>Clicking Next allows you to then activate the API for publishing:</p> <p></p> <p>Finally, clicking <code>Next</code> shows a screen such as the one below with checkmarks:</p> <p></p> <p>This shows that the gateway was able to connect to the GraphQL server and detect its schema, paths, and operations, validate them, and automatically populate the canvas with different policies, including security policies:</p> <p></p>"},{"location":"patterns/cqrs/","title":"Command-Query Responsibility Segregation (CQRS)","text":""},{"location":"patterns/cqrs/#problems-and-constraints","title":"Problems and Constraints","text":"<p>A domain-model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured.</p> <p>When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used?</p> <p>The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed.</p> <p>This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadaquate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data may unknowlingly conflict with each other.</p> <p>For example, in a microservices architecture, each microservice should store and manage its own business entity, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed to transform data, memory consumed by intermediate objects) and must be repeated each time the data is accessed.</p> <p>Another example is an enterprise database managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even stop.</p> <p>Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model.</p> <p>When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed.</p>"},{"location":"patterns/cqrs/#solution-and-pattern","title":"Solution and Pattern","text":"<p>Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently.</p> <p>The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both.</p> <p>This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, adpating to requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely.</p> <p>The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases.</p> <p>The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or selecting the most appropriate stage without considering the others:</p> <ol> <li>Stage 0: Typical application data access</li> <li>Stage 1: Separate read and write APIs</li> <li>Stage 2: Separate read and write models</li> <li>Stage 3: Separate read and write databases</li> </ol>"},{"location":"patterns/cqrs/#typical-application-data-access","title":"Typical application data access","text":"<p>Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model.</p> <p></p> <p>The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities\u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates\u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole.</p> <p>Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like <code>findCustomer()</code>, <code>placeOrder()</code>, <code>transferFunds()</code>, and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works.</p>"},{"location":"patterns/cqrs/#separate-read-and-write-apis","title":"Separate read and write APIs","text":"<p>The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs.</p> <p></p> <p>The two APIs share the existing domain model but split the behavior:</p> <ul> <li>Read: The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only.</li> <li>Write: The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data store.</li> </ul> <p>This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both):</p> <ul> <li>Query: Returns a result, a view of the domain model. Does not change the system\u2019s state nor cause any side effects that change the state.</li> <li>Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure.</li> </ul> <p>With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other.</p> <p>When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications.</p> <p>This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models.</p>"},{"location":"patterns/cqrs/#separate-read-and-write-models","title":"Separate read and write models","text":"<p>The second step, in applying the CQRS pattern, is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the application.</p> <p></p> <p>Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database.</p> <p>The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules.</p> <p>Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines Data Transfer Objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes.</p> <p>The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects.</p> <p>This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database.</p>"},{"location":"patterns/cqrs/#separate-read-and-write-databases","title":"Separate read and write databases","text":"<p>The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus.</p> <p></p> <p>The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only.</p> <p>The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale.</p> <p>The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently.</p> <p>This solution offers the following advantages:</p> <ul> <li>Scaling: The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access.</li> <li>Performance: The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins.</li> </ul> <p>Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent.</p> <p>CQRS employs a couple of design features that support keeping the databases synchronized:</p> <ul> <li>Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command (Design Patterns) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more.</li> <li>Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync.</li> </ul> <p>The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern, which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command:</p> <ul> <li>Logs the change</li> <li>Updates the database with the change</li> <li>Creates an update event describing the change and publishes it to the event bus</li> </ul> <p>The changes that are logged can be the commands from the command bus or the update events published to the event bus</p>"},{"location":"patterns/cqrs/#considerations","title":"Considerations","text":"<p>Keep these decisions in mind while applying this pattern:</p> <ul> <li>Client impact: Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs.</li> <li>Riskiness: A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization.</li> <li>Eventual consistency: Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases.</li> <li>Command queuing: Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.)</li> <li>Change log: The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay.</li> <li>Create keys: Strick interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key.</li> <li>Messaging queues and topics: While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors.</li> <li>Query database persistence: The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record.</li> <li>Security: Controls on reading data and updating data can be applied separately using the two parts of the solution.</li> </ul>"},{"location":"patterns/cqrs/#combining-event-sourcing-and-cqrs","title":"Combining event sourcing and CQRS","text":"<p>The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence.</p> <p>As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries:</p> <p></p> <p>The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins.</p> <p>The event backbone, uses a pub/sub model, to share events to different consumers interested in them.</p> <p>With this structure, the <code>Read model</code> microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the <code>voyage</code>, container <code>metrics</code>, and <code>order</code> topics to be able to answer this question. This is where CQRS shines.</p> <p>We can note that, we can separate the API definition and management in a API gateway.</p> <p>Some implementation items to consider:</p> <ul> <li>Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable.</li> <li>Scalability: Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries.</li> <li>Availability: The write model sacrafices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services.</li> </ul> <p>With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand.</p> <p>CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex.</p> <p>Some challenges to always consider:</p> <ul> <li>How to support event structure version management?</li> <li>How much data to keep in the event store (history)?</li> <li>How to adopt data duplication which results to eventual data consistency?.</li> </ul> <p>The CQRS pattern was introduced by Greg Young:</p> <p></p> <p>and described in Martin Fowler's work on microservices.</p> <p>As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist <code>Order</code> in its own database and then sends <code>OrderCreated</code> event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section.</p>"},{"location":"patterns/cqrs/#keeping-the-write-model-on-mainframe","title":"Keeping the write model on Mainframe","text":"<p>A lot of transactional systems are based on Mainframe computers. It is very important to note that the system of records and transaction processing is still easier to run on mainframe to support strong consistency. But with the move to cloud native development, it does not mean we have to move all the system of records to the cloud. Data pipelines can be put in place, but CQRS should help by keeping the write model on the current system of records and without impacting the current MIPS utilization move data in the eventual consistency workd of the cloud native, distributed computing world.</p> <p></p> <p>In the figure above, the write model follows the current transaction processing on the mainframce, change data capture pushes data to Event backbone for getting real-time visibility into the distributed world. The read is the costly operation, dues to the joins to be done to build the projection views needed to expose data depending of the business use cases. This applies with distributed microservices. All the write operations for the business entities kept in the mainframe's system of records are still done via the transaction. </p> <p>Reference data can be injected in one load job to topic and persisted in the event store so streaming applications can leverage them by joining with transactional data. </p>"},{"location":"patterns/cqrs/#the-consistency-challenges","title":"The consistency challenges","text":"<p>As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing.</p> <p>With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail.</p> <p>With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below.</p> <p></p> <p>The steps for synchronizing changes to the data are:</p> <ol> <li>The write model creates the event and publishes it.</li> <li>The consumer receives the event and extracts its payload.</li> <li>The consumer updates its local datasource with the payload data.</li> <li>If the consumer fails to process the update, it can persist the event to an error log or dead letter queue.</li> <li>Each error in the log can be replayed.</li> <li>A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation</li> </ol> <p>This implementation causes a problem for the <code>createOrder(order): string</code> operation: The Order Service is supposed to return the new order completed event with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and uses that as the order's key in the database.</p>"},{"location":"patterns/cqrs/#cqrs-and-change-data-capture","title":"CQRS and Change Data Capture","text":"<p>There are other ways to support this dual operations level:</p> <ul> <li>When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka.</li> <li>There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others.</li> <li>Write the order to the database and in the same transaction write to an event table (\"transactional outbox pattern\"). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent.</li> <li>Use the Change Data Capture from the database transaction log and generate events from this log. </li> </ul> <p>The CQRS implementation using CDC will look like in the following diagram:</p> <p></p> <p>What is important to note is that the event needs to be flexible on the data payload. </p> <p>On the view side, updates to the view part need to be idempotent.</p>"},{"location":"patterns/cqrs/#delay-in-the-view","title":"Delay in the view","text":"<p>There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale.</p> <p>When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application, the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation:</p> <pre><code>@POST\npublic Response create(OrderCreate dto) {\n    Order order = new Order(UUID.randomUUID().toString(), dto.getProductID(),...);\n    // ...\n    return Response.ok().entity(order).build()\n}\n</code></pre>"},{"location":"patterns/cqrs/#schema-change","title":"Schema change","text":"<p>What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures.</p>"},{"location":"patterns/cqrs/#further-readings","title":"Further readings","text":"<ul> <li>https://www.codeproject.com/Articles/555855/Introduction-to-CQRS</li> <li>http://udidahan.com/2009/12/09/clarified-cqrs</li> <li>https://martinfowler.com/bliki/CQRS.html</li> <li>https://microservices.io/patterns/data/cqrs.html</li> <li>https://community.risingstack.com/when-to-use-cqrs</li> <li>https://dzone.com/articles/concepts-of-cqrs</li> <li>https://martinfowler.com/bliki/CommandQuerySeparation.html</li> <li>https://www.martinfowler.com/eaaCatalog/domainModel.html</li> <li>https://dddcommunity.org/learning-ddd/what_is_ddd/</li> <li>https://martinfowler.com/bliki/EvansClassification.html</li> <li>https://martinfowler.com/bliki/DDD_Aggregate.html</li> <li>https://martinfowler.com/eaaCatalog/dataTransferObject.html</li> <li>https://en.wikipedia.org/wiki/Command_pattern</li> <li>https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html</li> </ul>"},{"location":"patterns/data-lineage/","title":"Data lineage","text":"<p>When it comes to adopting a streaming data platform, there are multiple components that work together to handle data, encompassing both online and offline processing. </p> <p>In this chapter, we will explore these components and concepts that are crucial for understanding the data lineage within such a platform. To provide visual clarity, the following figure illustrates the key components involved:</p> <p></p> <ol> <li>Event Sources: Our streaming data platform receives events from various sources, including mobile applications, web single page applications, IoT devices, microservice applications, change data capture (not depicted here), and SaaS offerings. These sources generate valuable data that forms the foundation of our real-time analytics.</li> <li>Event Backbone: To ensure data durability and reliability, we persist the incoming events in an append log within the event backbone. This backbone acts as a central hub, storing events with a long retention time (typically spanning multiple days). This approach allows us to maintain a complete and historical record of the data.</li> <li>Stream Applications: Using streaming technologies such as Kafka Streams and Apache Flinks, our stream applications perform stateful operations to process and analyze the incoming events in real-time. These applications also enable us to perform complex event processing, providing valuable insights and actionable information.</li> <li>Next Best Action: On the right side of the architecture, consumers have the ability to trigger the next best action based on specific events. This action could be a business process, some product recommendations, alerts, or any other relevant response. By leveraging the insights derived from the stream applications, we can make informed decisions and take proactive steps in real-time.</li> <li>Long-Term Data Storage: While the event backbone retains data for a limited period, we often require the ability to persist data for longer durations. To achieve this, we leverage storage solutions like S3 buckets, which offer full cloud availability. Whether it's Cloud Object Storage or on-premise deployments, these storage options allow us to securely store and access data for extended periods.</li> <li>Big Data Processing: To extract further value from the accumulated data, we employ big data processing platforms like Apache Spark. These platforms enable us to perform batch processing and map-reduce operations on the data at rest. By leveraging the data ingested through the event backbone, we can derive valuable insights and patterns that can drive informed decision-making.</li> <li>Business Dashboards: To provide a comprehensive view of the data, our streaming data platform integrates with business dashboards. These dashboards enable users to query both static data at rest and dynamic data in motion. By utilizing interactive and streaming queries, users gain real-time access to critical information, empowering them to make data-driven decisions.</li> </ol>"},{"location":"patterns/data-lineage/#data-lineage-requirements","title":"Data lineage requirements","text":"<p>Data lineage is a comprehensive concept that encompasses various aspects of data management. It describes the origins, movements, characteristics, ownership, and quality of data. As part of a larger data governance initiative, data lineage may also include considerations such as data security, access control, encryption, and confidentiality.</p> <p>To effectively implement data lineage, organizations should consider the following requirements:</p> <ol> <li> <p>Data Source Identification: Clearly identify and document all data sources, including internal systems, external providers, and any other origins of data.</p> </li> <li> <p>Data Ownership: Establish clear ownership and responsibilities for each dataset. Assign data owners who are accountable for the accuracy, completeness, and quality of the data.</p> </li> <li> <p>Data Access Control: Define and enforce access controls to ensure that only authorized individuals or systems can access specific datasets. Implement role-based access control (RBAC) and maintain an audit trail of data access activities.</p> </li> <li> <p>Data Quality Measures: Establish data quality metrics and implement processes to monitor and maintain the quality of data throughout its lifecycle. This may include data validation, cleansing, and reconciliation procedures.</p> </li> <li> <p>Data Transformation Tracking: Keep track of all data transformations and modifications that occur as data moves through the platform. Document the logic, algorithms, and any manual interventions applied to the data.</p> </li> <li> <p>Data Lineage Visualization: Utilize tools and technologies that provide visual representations of data lineage, making it easier to understand the flow of data and identify dependencies.</p> </li> </ol> <p>By addressing these requirements and implementing robust data lineage practices, organizations can gain a clear understanding of their data landscape, ensure data quality, and make informed decisions based on reliable and traceable data.</p>"},{"location":"patterns/data-lineage/#contracts","title":"Contracts","text":"<p>In the world of REST APIs and Service-Oriented Architecture (SOA), request/response interactions are defined using standards such as OpenAPI (formerly known as Swagger) or Web Services Description Language (WSDL). These standards provide a way to describe the structure and format of the data being exchanged between the client and the server.</p> <p>Similarly, in the realm of event-driven and streaming architectures, AsyncAPI has emerged as a specification for defining message-driven APIs. AsyncAPI allows you to describe the schema of the messages being exchanged and the middleware bindings used for communication.</p> <p>AsyncAPI is designed to be protocol-agnostic, meaning it can be used with various messaging protocols such as MQTT, AMQP, Kafka, WebSocket, and more. It provides a standardized way to define the structure and format of the messages, as well as the channels or topics on which the messages are published and subscribed.</p> <p>By using AsyncAPI, you can:</p> <ol> <li>Define the message payload structure using JSON Schema or Avro.</li> <li>Specify the channels or topics for publishing and subscribing to messages.</li> <li>Describe the message exchange patterns, such as publish/subscribe or request/reply.</li> <li>Define the security mechanisms and authentication requirements for the API.</li> <li>Generate documentation, code snippets, and client libraries based on the AsyncAPI specification.</li> </ol> <p>AsyncAPI promotes interoperability, and easier integration between different systems and languages as it enables the creation of tooling and frameworks that can generate code, perform validation, and provide testing capabilities based on the AsyncAPI specification.</p>"},{"location":"patterns/data-lineage/#openapi","title":"OpenAPI","text":"<p>We do not need to present OpenAPI but just the fact that those APIs represent request/response communication and may be managed and integrated with the development life cycle. Modern API management platform should support their life cycle end to end but also support new specifications like GraphQL and AsynchAPI.</p>"},{"location":"patterns/data-lineage/#asynchapi","title":"AsynchAPI","text":"<p>Without duplicating the specification is the specification to define schema and middleware binding. we want to highlight here what are the important parts to consider in the data governance:</p> <ul> <li>The Asynch API documentation which includes:<ul> <li>The server definition to address the broker binding, with URL and protocol to be used. (http, kafka, mqtt, amqp, ws)</li> <li>The channel definition which represents how to reach the brokers. It looks like a Path definition in OpenAPI</li> <li>The message definition which could be of any value. Apache Avro is one way to present message but it does not have to be.</li> <li>Security to access the server via user / password, TLS certificates, API keys, OAuth2...</li> </ul> </li> <li>The message schema</li> </ul> <p>The format of the file describing the API can be Yaml or Json. </p>"},{"location":"patterns/data-lineage/#schema","title":"Schema","text":"<p>Schema is an important way to ensure data quality by defining a strong, but flexible, contract between producers and consumers and to understand the exposed payload in each topics. Schema definitions improve applications robustness as downstream data consumers are protected from malformed data, as only valid data will be permitted in the topic. Schemas need to be compatible from version to version and Apache Avro supports defining default values for non existing attribute of previous versioned schema. Schema Registry enforces full compatibility when creating a new version of a schema. Full compatibility means that old data can be read with the new data schema, and new data can also be read with a previous data schema.</p> <p>Here is an example of such definition:</p> <p><pre><code>\"fields\": [\n     { \"name\": \"newAttribute\",\n       \"type\": \"string\",\n       \"default\": \"defaultValue\"\n     }\n]\n</code></pre> Remarks: if you want backward compatibility, being able to read old messages, you need to add new fields with default value.To support forward compatibility you need to add default value to deleted field, and if you want full compatibility you need to add default value on all fields.</p> <p>Metadata about the event can be added as part of the <code>field</code> definition, and should include source application reference, may be a unique identifier created at the source application to ensure traceability end to end, and when intermediate streaming application are doing transformation on the same topic, the transformer app reference. Here is an example of such data:</p> <pre><code>\"fields\": [\n     { \"name\": \"metadata\",\n       \"type\": {\n           \"name\": \"Metadata\",\n           \"type\": \"record\",\n           \"fields\": [\n               {\"name\": \"sourceApp\", \"type\": \"string\"},\n               {\"name\": \"eventUUID\", \"type\": \"string\"},\n               {\"name\": \"transformApps\", \"type\": \n                { \"name\" : \"TransformerReference\",\n                  \"type\": \"array\",\n                  \"items\": \"string\"\n                }\n               }\n           ]\n       }\n     }\n]\n</code></pre> <p>The classical integration with schema registry is presented in the figure below:</p> <p></p> <p>Schema registry can be deployed in different data centers and serves multi Kafka clusters. For DR, you need to use a 'primary' server and one secondary in different data center. Both will receive schema update via DevOps pipeline. One of the main open source Schema Registry is Apicurio, which is integrated with Event Streams and in most of our implementation. Apicurio can persist schema definition inside Kafka Topic and so schema replication can also being synchronize via Mirror Maker 2 replication. If Postgresql is used for persistence then postgresql can be used for replicating schemas.</p> <p>We recommend reading our  schema registry summary. </p> <p>Integrating schema management with code generation and devOps pipeline is addressed in this repository.</p>"},{"location":"patterns/data-lineage/#conclusion","title":"Conclusion","text":"<p>Understanding the key components of a streaming data platform is essential for comprehending the data lineage within such an environment. By effectively leveraging event sources, event backbones, stream applications, long-term data storage, big data processing, and business dashboards, organizations can harness the full potential of their streaming data to drive insights, make informed decisions, and gain a competitive edge in today's data-driven landscape.</p>"},{"location":"patterns/dlq/","title":"Dead Letter Queue","text":""},{"location":"patterns/dlq/#event-reprocessing-with-dead-letter-pattern","title":"Event reprocessing with dead letter pattern","text":"<p>With event driven microservice, it is not just about pub/sub: there are use cases where the microservice needs to call existing service via an HTTP or RPC call. The call may fail. So what should be the processing to be done to retry and gracefully fail by leveraging the power of topics and the concept of dead letter.</p> <p>This pattern is influenced by the adoption of Kafka as event backbone and the offset management offered by Kafka. Once a message is read from a Kafka topic by a consumer the offset can be automatically committed so the consumer can poll the next batch of events, or in most the case manually committed, to support business logic to process those events.</p> <p>The figure below demonstrates the problem to address: the <code>reefer management microservice</code> get order event to process by allocating a reefer container to the order. The call to update the container inventory fails.</p> <p></p> <p>At first glance, the approach is to commit the offset only when the three internal operations are succesful: write to reefer database (2), update the container inventory using legacy application service (3), and produce new event to <code>orders</code> (4) and <code>containers</code> (5) topics. Step (4) and (5) will not be done as no response from (3) happened.</p> <p>In fact a better approach is to commit the read offset on the orders topic, and then starts the processing: the goal is to do not impact the input throughput. In case of step (2) or (3) fails the order data is published to an <code>order-retries</code> topic (4) with some added metadata like number of retries and timestamp.</p> <p></p> <p>A new order retry service or function consumes the <code>order retry</code> events (5) and do a new call to the remote service using a delay according to the number of retries already done: this is to pace the calls to a service that has issue for longer time. If the call (6) fails this function creates a new event in the <code>order-retries</code> topic with a retry counter increased by one. If the number of retry reaches a certain threshold then the order is sent to <code>order-dead-letter</code> topic (7) for human to work on. A CLI could read from this dead letter topic to deal with the data, or retry automatically once we know the backend service works. Using this approach we delay the call to the remote service without putting too much preassure on it.</p> <p>For more detail, we recommend reading this article from Uber engineering: Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka.</p>"},{"location":"patterns/event-sourcing/","title":"Event Sourcing","text":"Updates <p>Created 2019 - Updated 03/2024</p>"},{"location":"patterns/event-sourcing/#problems-and-constraints","title":"Problems and Constraints","text":"<p>Most business applications rely on a state-based persistence model. This means changes to an entity (like an order) update its existing record in the database, overwriting the previous state. While this approach is simple, it becomes cumbersome when you need to:</p> <ul> <li>Understand history: Explain how an entity reached its current state. Traditional models lack historical context, making it difficult to track changes and analyze trends.</li> <li>Audit trails: Maintain a record of all actions for legal or compliance reasons. Deleting data is often not an option, requiring complex \"log tables\" to capture historical changes.</li> </ul> Order Management with Event Sourcing <p>Consider the traditional Order model with separate tables for orders, addresses and product items. </p> <p></p> <p>If we need to implement a query that looks at what happened to the order over a time period, we need to change the model and add historical records, basically building a append log table. Event sourcing would replace this with a stream of events capturing each action on the order:</p> <ul> <li>OrderPlaced(orderId, customerId, address)</li> <li>ItemAdded(orderId, itemId, quantity)</li> <li>AddressUpdated(orderId, newAddress)</li> </ul> <p>By replaying these events, we can reconstruct the current state of the order at any point in time, including historical details.</p> <p>Designing a service to manage the life cycle of this OrderEntity will, most of the time, add a \"delete operation\" to remove data.  For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability.</p>"},{"location":"patterns/event-sourcing/#solution-and-pattern","title":"Solution and Pattern","text":"<p>Event sourcing offers an alternative approach. Instead of storing the \"current state\" of an entity, it focuses on a sequence of immutable events that represent all the actions performed on that entity. Here's how it works:</p> <ol> <li>Events Capture Changes: Each change to an entity (e.g., order placed, item added, address updated) is captured as an event with relevant details (timestamp, data, etc.).</li> <li>Event Stream Persistence: These events are stored in an append-only log, creating a complete historical record.</li> <li>Rebuilding State: To retrieve the current state of an entity, we replay the entire event stream and apply each event sequentially. </li> </ol> <p>Event sourcing has its roots in the domain-driven design community. Below is an example of an Order Entity state change events as a sequence of immutable \"facts\", ordered over time. </p> <p></p> <p>When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions.  The state-change event is immutable stored in an event log or event store in time order.  The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business.</p> <p>We can see the \"removing an item\" event in the log is a new event. With this capability, we can count how often a specific product is removed for the shopping cart.</p> <p>In some cases, the event sourcing pattern is implemented completely within the event backbone middleware. Apache Kafka is a popular platform for implementing event sourcing. It uses topics and partitions to manage event streams. Producers publish events to topics, while consumers subscribe and process them. </p> <p>However, we can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. </p> <p>An event store needs to store only three pieces of information:</p> <ul> <li>The type of event or aggregate.</li> <li>The sequence number of the event, can be a ZT timestamp.</li> <li>The data as a serialized entity.</li> </ul> <p>More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design which may be heavily optimized for appending and retrieving sequences of records.</p> <p>With a central event logs, as provides by Kafka, producers append events to the log, and consumers read them from an offset (a sequence number).</p> <p></p> <p>To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\".</p>"},{"location":"patterns/event-sourcing/#benefits-of-event-sourcing","title":"Benefits of Event Sourcing","text":"<ul> <li>Enhanced Auditability: Provides a complete, tamper-proof record of all actions performed on an entity.</li> <li>Improved Analytics: Enables historical analysis and trend identification by replaying the event stream for specific time frames. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event occurrence.</li> <li>Scalability: Easier to scale horizontally as event streams are naturally append-only and can be distributed efficiently.</li> <li>Resilience: Event streams are immutable, making them inherently resistant to accidental data modifications. It is possible to reverse the state and correct data with new events.</li> </ul>"},{"location":"patterns/event-sourcing/#considerations","title":"Considerations","text":"<p>When replaying events to reconstruct an entity's state, it's crucial to avoid unintended side effects. A common example is sending notifications \u2013 this action should be a separate process triggered by the event, not part of the replay itself.</p> <p>Event consumers should be designed to handle different scenarios: </p> <ul> <li>The Queries: Consumers focused on answering questions like \"what happened to order ID #75 over time?\" don't generate side effects. They simply process events to generate a report.</li> <li>Business Logic: Other consumers might trigger actions based on events, but these actions should be separate steps outside the replay process.</li> </ul> <p>Replaying hundreds of events can be time-consuming. To address this, event sourcing can leverage snapshots: Periodically, or at key points in time, a snapshot capturing the current state of an entity is created. When reconstructing an entity's state, the system can start from the most recent snapshot and then replays only the events that occurred after that point.</p> <p></p> <p>Snapshots are optimization techniques, not a requirement for all event sourcing implementations. They are particularly beneficial when dealing with high volumes of state change events.</p> <p>Kafka is supporting the event sourcing pattern with the topic and partition: the partition key needs to be mapped to the business entity key, so that the partition is a unique append log where each events related to the same entity are in the same log. Repartitioning will loose the reordering.  </p> <p>The event sourcing pattern is well described in this article on microservices.io. It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth.</p> <p>See also this event sourcing article from Martin Fowler, where he is also using ship movement examples. </p> <p>Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems.</p>"},{"location":"patterns/event-sourcing/#command-sourcing","title":"Command sourcing","text":"<p>Command sourcing shares similarities with event sourcing, but with a key twist. Instead of persisting the events that modify an entity's state, it persists the actual commands themselves. This enables asynchronous processing, which is beneficial for commands that take a long time to execute.</p> <p>However, command sourcing introduces its own challenges:</p> <ul> <li>Immutable: Command persisted needs to be immutable to keep process flow integrity.</li> <li>Idempotency: Commands need to be designed to produce the same outcome even if executed multiple times (especially in case of failures). This ensures data integrity.</li> <li>Command Validation: Validating commands before persisting them helps prevent storing invalid commands in the queue. For example, an <code>AddItem</code> command might be validated as <code>AddItemValidated</code> before being persisted. Once successfully stored, it could then trigger an <code>ItemAdded</code> event. Combining commands with event sourcing is a common approach.</li> </ul>"},{"location":"patterns/event-sourcing/#understanding-transactional-consistency-in-microservices","title":"Understanding Transactional Consistency in Microservices:","text":"<p>Unlike traditional database transactions (ACID), business transactions involving multiple microservices are more like a series of steps. Each step is handled by a dedicated microservice responsible for updating its owned entity. This results in \"eventual data consistency,\" where data consistency across the system is eventually achieved, but not necessarily guaranteed immediately.</p> <p>The event backbone (messaging system) needs to ensure events are delivered to microservices at least once. It's the responsibility of individual microservices to manage their position within the event stream and handle inconsistencies, such as detecting and processing duplicate events.</p> <p>Maintaining Consistency Within Microservices: To prevent data inconsistencies in case of crashes, updating data and emitting events within a microservice should be an atomic operation. This can be achieved in a few ways:</p> <ul> <li> <p>Event Table: Add an eventTable to the microservice database. An event publisher reads this table regularly and marks events as published after successfully sending them.</p> </li> <li> <p>Database Transaction Log: Implement a reader or \"miner\" for the database transaction log, responsible for publishing events whenever a new row is added to the log.</p> </li> </ul>"},{"location":"patterns/event-sourcing/#some-technologies-to-consider","title":"Some technologies to consider","text":"<ul> <li>Any database that can scale to millions of records may be used as event store for most business application. When scaling requirements are higher that one a single database can support, you need to look at other solutions.</li> <li>Change data capture is often used to a mechanism to propagate events from an event store to messaging middleware.</li> <li>IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. </li> <li>AWS Dynamodb time series can be used as persistence layer to keep events for long period of time with high scaling characteristics.</li> </ul>"},{"location":"patterns/event-sourcing/#more-readings","title":"More Readings","text":"<ul> <li>Martin Fowler - event sourcing pattern</li> <li>Microservice design pattern from Chris Richardson</li> <li>Greg Young video on event sourcing at the GOTO 2014 conference:</li> </ul> <p>&gt;&gt;&gt; Next: Command-Query Responsibility Segregation (CQRS)</p>"},{"location":"patterns/events-versus-messages/","title":"Event Streaming versus Queuing","text":"<p>Info</p> <p>Updated 11/21/2023</p> <p>Consider queue system for:</p> <ul> <li>Exactly once delivery, and to participate into two-phase commit XA transaction.</li> <li>Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second component to do something on its data. This is a command pattern with delay on the response.</li> <li>Recall messages in queue are kept until consumer(s) got them, which fits well in a command pattern as we do not want to get the command done twice.</li> </ul> <p>Consider streaming system, like Kafka, as pub/sub and persistence system for:</p> <ul> <li>Publish events as immutable facts, with a timestamp, of what happened in an application.</li> <li>Get continuous visibility of the data Streams.</li> <li>Keep data a longer time in the persistence layer, once consumed, so future consumers can consume those messages, or for replay-ability.</li> <li>Scale the message consumption horizontally.</li> </ul>"},{"location":"patterns/events-versus-messages/#events-and-messages","title":"Events and Messages","text":"<p>There is a long history for messaging in IT systems.  We can easily see an event-driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering:</p> <ul> <li>Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed.</li> <li>Events: Events are persisted as a replayable stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (We can't change history.)</li> </ul> <p></p>"},{"location":"patterns/events-versus-messages/#messaging-versus-event-streaming","title":"Messaging versus event streaming","text":"<p>We recommend reading this article and this one, to get insight on messaging (focusing on operations / actions to be performed by a system or service) versus events (focusing on the state / facts of a system with no knowledge of the downstream processing).</p> <p>To summarize messaging is to support:</p> <ul> <li>Transient Data: data is only stored until a consumer has processed the message, or it expires.</li> <li>Request / reply most of the time.</li> <li>Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support.</li> <li>Time Coupled producers and consumers: consumers can subscribe to queue, but message can be remove after a certain time or when all subscribers got the message. The coupling is still loose at the data model level, and at interface definition level.</li> </ul> <p>For events:</p> <ul> <li>Stream History: consumers are interested in historic events, not just the most recent ones.</li> <li>Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow.</li> <li>Immutable Data</li> <li>Loosely coupled / decoupled producers and consumers: strong time decoupling as consumer may come at anytime. Some coupling at the message definition level, but schema management best practices and schema registry reduce frictions.</li> </ul> <p>See also the MQ in an event-driven solution context article</p> <p>See the code of a Store sale simulator to produce messages to different middleware: RabbitMQ, IBM MQ or Kafka.</p>"},{"location":"patterns/governance/","title":"Event driven solution governance","text":"<p>Digital businesses are characterized by real-time responsiveness, scalability, and flexibility, all while focusing on delivering an outstanding customer experience. The adoption of APIs and microservices has played a significant role in enabling this transformation, supporting real-time interactions and increasing levels of agility through the decoupling of applications. </p> <p>However, digital business requires more than just these capabilities. It needs to become more time-sensitive, contextual, and event-driven in nature. Events are the fundamental building blocks of how modern digital businesses operate, and an event-driven architecture allows IT to align with and support this way of working.</p> <p>By emphasizing events, digital businesses can better respond to changing circumstances, anticipate customer needs, and adapt their operations accordingly. This event-driven approach helps organizations become more nimble, responsive, and customer-centric \u2013 key attributes for success in the digital age.</p> <p>Governing the development, deployment, and maintenance of an event-driven solution involves considering a variety of viewpoints. In this section, we'll cover what we believe are the important factors to address</p> <p>The first crucial question to answer is: do we truly need to use an event-driven solution? This is all about ensuring the chosen approach is a good fit for the specific requirements and challenges at hand.</p> <p>Most event-driven architecture (EDA) adoption starts from the adoption of a new programming model based on the reactive manifesto, or by selecting a modern middleware platform like Kafka to support loosely coupled microservices. However, this is typically done within the context of a single business application.</p> <p>To effectively govern an event-driven solution, organizations need to take a broader, enterprise-wide perspective. They must consider the overall strategic alignment, technical feasibility, and organizational readiness before committing to an EDA approach.</p> <p>After the initial successful implementation of an event-driven solution, it's important to consider adopting a more strategic and enterprise-wide approach. This includes:</p> <ul> <li>Technology Selection: Establishing a standardized set of technologies for event-driven architecture, rather than ad-hoc choices for each project.</li> <li>Common Architecture and Design Patterns: Defining and consistently applying architectural and design patterns to ensure scalability, maintainability, and reusability of the EDA.</li> <li>Data Governance and Lineage: Implementing robust data governance practices to control data models and maintain comprehensive data lineage across the event-driven ecosystem.</li> <li>Streamlined Onboarding and Deployment: Adopting common methodologies and processes to quickly onboard new business initiatives and deploy new capabilities on top of the EDA, ideally in a matter of days or weeks.</li> </ul>"},{"location":"patterns/governance/#fit-for-purpose","title":"Fit for Purpose","text":"<p>In the context of event-driven architecture (EDA), the 'fit for purpose' assessment needs to help answer several high-level questions:</p> <ol> <li>When should we use an event-driven solution for a new application? This involves evaluating the specific requirements, challenges, and benefits that an event-driven approach can address.</li> <li>When should we use a modern pub/sub event backbone versus traditional queuing products? See this dedicated note on comparing those messaging approaches</li> <li>What data stream processing capabilities are required? Determining the appropriate data stream processing tools and techniques is crucial for deriving insights and responding to events in a timely manner.</li> <li>What are the different use cases and benefits of event-driven architecture? Clearly articulating the various applications and advantages of EDA helps justify its adoption and guide the implementation strategy.</li> </ol>"},{"location":"patterns/governance/#architecture-patterns","title":"Architecture patterns","text":"<p>We have already described in this chapter as set of event-driven architecture patterns that can be leveraged while establishing EDA practices which includes how to integrate with data in topic for doing feature engineering. </p> <p>Legacy integration and coexistence between legacy applications or mainframe transactional application and microservices is presented in this section.</p> <p>The modern data pipeline is also an important architecture pattern where data ingestion layer is the same as the microservice integration middleware and provide buffering capability as well as stateful data stream processing.</p> <p>From an implementation point of view the following design patterns are often part of the EDA adoption:</p> <ul> <li>Strangler pattern</li> <li>Event sourcing</li> <li>Choreography</li> <li>orchestration</li> <li>Command Query Responsibility Segregation</li> <li>Saga pattern</li> <li>Transactional outbox</li> <li>Event reprocessing with dead letter</li> </ul> <p>TBC</p>"},{"location":"patterns/governance/#getting-developers-on-board","title":"Getting developers on board","text":""},{"location":"patterns/governance/#data-lineage","title":"Data lineage","text":"<p>Data governance is a well established practices in most companies. Solutions need to address the following needs:</p> <ul> <li>Which sources/producers are feeding data?</li> <li>What is the data schema?</li> <li>How to classify data</li> <li>Which process reads the data and how is it transformed?</li> <li>When was the data last updated?</li> <li>Which apps/ users access private data?</li> </ul> <p>In the context of event-driven architecture, one focus will be to ensure re-use of event topics,  control the schema definition, have a consistent and governed ways to manage topic as service,  domain and event data model definition, access control, encryption and traceability.  As this subject is very important, we have started to address it in a separate 'data lineage' note.</p>"},{"location":"patterns/governance/#deployment-strategy","title":"Deployment Strategy","text":"<p>Hybrid cloud, Containerized</p> <p>security and access control Strimzi - cruise control active - active topic as self service</p>"},{"location":"patterns/governance/#ongoing-maintenance","title":"Ongoing Maintenance","text":"<p>Procedures performed regularly to keep a system healthy Cluster rebalancing  Add new broker - partition reassignment</p> <p>Product migration</p>"},{"location":"patterns/governance/#operational-monitoring","title":"Operational Monitoring","text":"<p>assess partition in heavy load assess broker in heavy load assess partition leadership assignment</p> <p>Problem identification System health confirmation</p>"},{"location":"patterns/governance/#error-handling","title":"Error Handling","text":"<p>Procedures, tools and tactics to resolve problems when they arise</p>"},{"location":"patterns/model/","title":"Different Data Models","text":"<p>The advent of modern business applications such as cloud-native microservices, long-running business processes, decision services, and AI, among others, has led to an increased complexity in data modeling, data transformation, and data view. While adopting a canonical model presents certain advantages, it also entails counterproductive effects.</p> <p>Microservice adoption and the application of domain driven design make defining a bounded context for microservices easier, but as soon as we begin coding, we encounter immediate questions about the information model's shape, and the appropriate canonical model to use, which is typically maintained at the messaging or integration layer to facilitate one-to-one mappings between models.</p> <p>The goal of this document is to apply the architecture principal of clear separation of concern, focusing on data needed to implement the business logic, and adapt interface definition for effortless consumption and reuse.</p> <p>The following figure illustrates a potential business solution using different components, supported on top of different software products.</p> <p></p> <ul> <li>Single Page applications are running in Web Browser and use different javascript library to support HTML rendering, business logic implementation and data model as JSON documents. The model view is focusing to expose data to users and to support form entry. The app interacts with one Backend For Frontend app's REST API or other services with CORS support. The model and API are designed for this user centric application.</li> <li>Microservices are exposing model via their APIs, in Java EE, it used to be named Data Transfer Object, and it is still used and named this way in microservice. The focus on this model is to enforce reusability, and expose APIs about the main business entity or aggregate (Domain Driven Design - Aggregate). This entity is persisted in different formats, and when using traditional SQL based technology, Object Relational Mapping is needed. In Java, hibernate ORM is used with JPA and JTA to support transaction and entity annotation. Quarkus Panache is a nice way to map business entity transparently to DB. But still we need to design those models. Within the microservice, we will also find specific implementation model used to support the business logic the service implements.</li> <li>The adoption of JSON document to get a more flexible, schemaless model, helps at the technology selection to be more agile as the model can evolve easily, but still we need to think and design such models.</li> <li>Modern business applications are using Artificial Intelligence model. To make it more generic, we defined a ML model service which computes a score between 0 to 1, given a feature set represented as tuples. The model is exposed with an API with a flat data model.</li> </ul> <p>In the middle of the figure, we have an integration layer:</p> <ul> <li>The modern event backbone supports event streaming and message queueing. The event model is defined with schemas, and any applications producing such events, needs to have a specific model to represent fact, of what happens in their boundary.</li> <li>Enterprise service buses are still used in IT architecture, as service gateway, doing interface mapping, and heteregeonous integrations. The model definition will better fit to the adoption of canonical model to facilitate the one to one mapping between source and destination models.  </li> <li>Those integrations can do data mapping with Software as a Service applications like a CRM, ERP systems. The name ASBO, for Application Specific Business Object, was used in the past, and it still applies in modern solutions. We need to consider them when doing model mapping.</li> <li>Some business applications need to get human involved, in the form of workflow, where process application implements the workflow and define process variables to keep data between process tasks. Those data are persisted with process instances within the Business Process Management datasource. The business process can be triggered by user interface (claiming to start a process), by exposing a service, in this case we are back to the DTO definition. When the business process is triggered asynchronously via messages we can use a message model.</li> <li>Externalizing business rules, is still a relevant pattern, and in the case of decision modeling, a specific data model to support the rules inference and simplify the rule processing is needed. This is what we called RBO: Rule Business Object in the Agile Business Rule Development methodology.</li> </ul> <p>The persisted information model (in system of records or DB) is different than the process variable model in BPM, which is different than the rule business object model used to make decisions.</p> <p>A good architecture leverages different information model views to carry the data to the different layers of the business application: the user interface, the database, the service contract, the business process data, the messaging structure, the events, the business rules, the raw document, the AI scoring input data.... There is nothing new in this, but better to keep that in mind anyway when developing new solutions.</p> <p>The information model for such service has to be designed so there is minimum dependency between consumers and providers. The approach of trying to develop a unique rich information model to support all the different consumers, may lead having the consumers dealing with complex data structure sparsely populated, and will increase coupling between components.</p> <p>When defining service specifications it is important to assess if each business entities supported by the service operations is going to have different definitions or if the same definition applies to the whole service portfolio?</p> <p>The scope of reuse of data types is an important architecture decision; there is no one-size-fits-all solution. However, a few different patterns are emerging:</p> <ul> <li>One object model per interface: Using an independent data model for every service interface assures the highest level of decoupling between services. As negative, the consumers have to understand different representations of the same business entities across multiple services and have to cope with all the relative data transformations. This strategy can be very effective for coarse-grained service operations that do not exchange large amount of data with consumers.</li> <li>One object model per business domain: the service information models are organized in domains, every domain sharing the same data model. The downside of this approach, is once domains are defined, changing their boundaries can be costly.</li> <li>A single object model for the whole enterprise: the approach is to define a single common set of data structures shared across all the enterprise service interfaces. The amount of model customization is kept to a minimum and its management is centralized. The negatives are having overly complicated model the consumers need to process. As of now it becomes a bad practices to adopt such canonical model at the API, persistence and eventing level. </li> </ul>"},{"location":"patterns/reactive/","title":"EDA support for Reactive Systems implementation","text":"<p>This chapter describes how event-driven architecture addresses the implementation of reactive systems and presents the most recent technologies to implement such event-driven responsive solutions.</p>"},{"location":"patterns/reactive/#overview","title":"Overview","text":""},{"location":"patterns/reactive/#cloud-native-apps","title":"Cloud native apps","text":"<p>The 12 factors app defines  the needs for modern business service apps or webapps to be a good citizen in cloud deployment. The factors are:</p> <ul> <li>Codebase: One codebase tracked in version control; many deploys.</li> <li>Dependencies: Explicitly declare and isolate dependencies.</li> <li>Config: store config in the environment.</li> <li>Backing services: Treat backing services as attached resources.</li> <li>Build, release, run: Strictly separate build and run stages.</li> <li>Processes: Execute the app as one or more stateless (which may be challenged in case of event streaming and real-time analytics) processes</li> <li>Port binding: Export services via port binding.</li> <li>Concurrency: Scale out via the process model.</li> <li>Disposability: Maximize the robustness with fast startup and graceful shutdown.</li> <li>Dev/prod parity: Keep development, staging, and production as similar as possible.</li> <li>Logs: Treat the application logs as event streams.</li> <li>Admin processes: Run admin/management tasks as one-off processes.</li> </ul> <p>Achieving cloud native is not an as easy task as it looks, involving clear separation of concerns, infrastructure as code, CI/CD, multi-region deployment, design for scaling and failure... </p>"},{"location":"patterns/reactive/#distributed-systems","title":"Distributed systems","text":"<p>From Leslie Lamport's definition: </p> <p>A distributed system is one in which the failure of a computer you didn\u2019t even know existed can render your own computer unusable.</p> <p>We need to design application for transient, intermittent, or complete failure and resilience. Complex exchanges involving multiple services cannot expect all the participants and the network to be operational for the complete duration of that exchange. </p> <p>Always ask ourselves: How would we detect failures? How would we handle them gracefully?</p> <p>When designing a distributed solution, we need to keep in mind that the CAP theorem prevents  a distributed data store from simultaneously providing more than two  out of the three guarantees of Consistency, Availability and Partition tolerance. </p> <p>With simple synchronous calls between services leads to time coupling and enforces programming in sequence. The code must gracefully handle faulty responses and the absence of response.  Quarkus/Vert.x has a router that can be intercepted to simulate communication loss, wrong response or delay, this is vertically helpful to test for failure. See Clement Escoffier's code sample to illustrate those concepts.</p> <p>Combining time-out and retries, is a common development practice, but we can\u2019t assume the service was not invoked and retrying may reprocess the same request multiple times. Service needs to support idempotency: multiple requests with same content, result in the same state and same output. Idempotency can be implemented using unique identifiers added to the payload so consumer can identify same request. But server applications need to keep state of those ids, which means using Storage, which is under the CAP theorem. We may include an in-memory data grid such as Infinispan or Hazelcast, an inventory service such as Apache ZooKeeper, a distributed cache as Redis. </p>"},{"location":"patterns/reactive/#reactive-systems","title":"Reactive systems","text":"<p>Modern business applications embrace the strong need to be responsive, bringing immediate  response and feedbacks to the end user or system acting on it at the moment it needed. </p> <p>Modern solution based on microservices needs to support load increase and failure; and developers are adopting the reactive manifesto  and use modern programming libraries and software to support  the manifesto characteristics. </p> <p>The reactive manifesto defines four characteristics modern cloud native application needs to support:</p> <p></p> <ul> <li>Responsive: deliver a consistent quality of service to end users or systems, react quickly and consistently to events happening in the system.</li> <li>Elastic:\u00a0The system stays responsive under varying workload, it can scale up and down the resource utilization depending of the load to the system.</li> <li>Resilient: stay responsive in the face of failure, this is a key characteristics. It implies distributed systems.</li> <li>Message driven: the underlying behavior is to have an asynchronous message driven backbone, to enable loose coupling of the application components by exchanging asynchronous messages to minimize or isolate the negative effects of resource contention, coherency delays and inter-service communication network latency. It is the base to support the other reactive characteristics. It also helps for isolation and support location transparency.</li> </ul> <p>Reactive architecture is an architecture approach aims to use asynchronous messaging or event driven architecture to build Responsive, Resilient and Elastic systems.  Relying on message passing enables the responsive characteristics and more, like flow control, by monitoring the messages in the systems and applying backpressure when necessary.</p> <p>Under the \"reactive\" terms we can see two important caveats:</p> <ul> <li>Reactive systems is a group of application components which can heal and scale automatically. It addresses data consistency, cross domain communication, orchestration, failure, recovery... </li> <li>Reactive programming is a subset of asynchronous programming and a paradigm where the availability of new information drives the logic forward rather than having control flow driven by a thread-of-execution. This is the adoption of non-blocking IO and event-based model.</li> </ul> <p>The following figure illustrates well how those two paradigms work together to deliver business value:</p> <p></p>"},{"location":"patterns/reactive/#commands-and-events","title":"Commands and Events","text":"<p>Those two concepts are very fundamental to implement distributed applications. </p> <ul> <li>Commands: represent action a user or system wants to perform. HTTP APIs pass commands. Commands are sent to a specific service and result is sent back to the caller.</li> <li>Events: are actions that have successfully completed. An event represents a fact. They are immutable. </li> </ul> <p>By looking at our solution in terms of commands and events, we focus on the behavior, workflow, instead of the structure.</p> <p>Events are wrapped into Messages. But Commands can also being passed via messaging and asynchronous communication. Most likely strong consistency is needed, so for asynchronous communication queuing systems are used as message brokers.</p>"},{"location":"patterns/reactive/#is-it-for-me","title":"Is it for me?","text":"<p>We have learnt from years of point to point microservice implementations, that embrassing  asynchronous communication helps a lot to support scaling, integration, coupling and failover.  So adopting reactive design and implementation may look complex at first but is becoming a  necessity in the long run. In e-commerce, a lot of monolithic applications were redesigned  to adopt reactive manifesto characteristics to support scaling the business needs and respond  to sporadic demand. In the world of big data, collecting, aggregating, applying real time  analytics, decisions and AI need to scale and respond to events at the time of occurence. </p>"},{"location":"patterns/reactive/#eda-and-reactive-systems","title":"EDA and reactive systems","text":"<p>The adoption of event-driven microservice implementation fits well into the reactive manifesto, where most of the work presented in this git repository started by adopting Kafka as event backbone, it is too reductor to think EDA is just Kafka. EDA supports reactive systems at large, and developing event-driven microservice should use reactive libraries to support non-blocking IO and event backbone for inter process communication. Also microservices is part of the game, functions / serverless are also in scope and with serverless 2.0,  Knative eventing is one of the new kid in the play.</p> <p>The manifesto stipulates \"message driven\", while EDA is about events and commands.  Events represent unmmutable data and facts about what happened, and components subscribe to those event streams. Command demands the consumer to process the content data sent and gives an answer.</p> <p>Both are sent via messages, and transported and managed by brokers.  For sure we define event-driven implementations to cover both. And we should not be purist and opinionated about messaging versus eventing: it will not make any sense to say: we are using queue to exchange message while we produce events to topic. </p> <p>The following figure illustrates the combination of synchronous communication, sending commands to reactive system, supported by reactive applications link together via messaging.</p> <p></p> <p>With messaging applications can scale horizontally giving the elastic need of the reactive manifesto. They are also more resilient as messages are kept until consumed or for a long period of time and consumer can restart from where they were in the ordered log. </p> <p>Reactive systems are not only exchanging messages. Sending and receiving messages must be done efficiently and Reactive promotes the use of nonblocking I/Os. Which leads to reactive programming and supporting libraries, like Vert.x, Mutiny, reactive messaging...</p>"},{"location":"patterns/reactive/#reactive-technologies-review","title":"Reactive technologies review","text":""},{"location":"patterns/reactive/#concurrency","title":"Concurrency","text":"<p>The following figure illustrates the traditional Java multi-threading approach to handle request and  access I/Os on a two CPUs computer. When the second thread starts working on the IO the CPU is locked and the CPU yellow is supporting 2 threads (1 and 3)</p> <p></p> <p>On public clouds, the blocking I/O approach inflates our monthly bill; on private clouds,  it reduces the deployment density.</p> <p>Non blocking IO framework or library adopts the <code>reactor pattern</code> where requests are internally asynchronous events processed, in order, by an event loop running in one thread, and handlers  (or callbacks) are used to process the response.</p> <p></p> <p>The above figure and next one are coming from Clement Escoffier's book  Building reactive microservice in Java.</p> <p>In multi CPUs, cores and threading computer, the reactors can run in parallel, with one event  loop per core:</p> <p></p> <p>With non-blocking I/O the I/O operation can be allocated to another pool and the allocation of CPU to thread is well balanced: </p> <p></p> <p>Vert.X is the open source library to build such non-blocking I/O app, and using <code>vertices</code> to support scalable concurrent processor, which executes one event loop thread. <code>vertices</code> communicate asynchronously via an event bus.  </p>"},{"location":"patterns/reactive/#reactive-programming","title":"Reactive programming","text":"<p>Reactive programming is about observing asynchronous streams. Streams can be seen as a pipe in which events flow. </p> <p>We observe the events flowing\u2014such as items, failures, completion, cancellations\u2014and implement side effects.</p> <p>We structure our code around streams and build chains of transformation, also called pipeline. Reactive programming libraries offer countless operators that let us create, combine, filter, and transform the object emitted by streams.</p> <p>One important consideration is the speed of items processing by the consumer. If it is too slow compare to the producer, there will be a big problem, that  can be solved efficiency by using backpressure protocol. Reactive Streams is such backpressure protocol. It defines the concept of Subscriber who requests the Publisher a certain amount of items. The consumer controls the flow, as when items are received and processed, consumers can ask for more.</p> <p>Producer is not strongly coupled to the consumer as they participate together to the stream processing. Producer uses a Subscription object to act as a contract between the participants.</p> <p>In Java, different libraries support the Reactive Streams protocol, like Mutiny, RxJava, Project Reactor...</p>"},{"location":"patterns/reactive/#vertx","title":"Vert.x","text":"<p>We do not need to reintroduce Vert.X, but with the large adoption of Quarkus to develop new JVM based microservice, Vert.x is an important library to understand. The main concepts used are:</p> <ul> <li>An application would typically be composed of multiple vertices running in the same Vert.x instance and communicate with each other using events via the <code>event bus</code>.</li> <li>Vertices remain dormant until they receive a message or event.</li> <li>Message handling is ideally asynchronous, messages are queued to the event bus, and control is returned to the sender.</li> <li>Regular vertices are executed in the event loop.</li> <li>Worker vertices are not executed on the event loop, which means they can execute blocking code.</li> <li>Two event loops per CPU core thread.</li> <li>No thread coordination mechanisms to manipulate a verticle state.</li> <li>A verticle can be deployed several times as part of a container for example.</li> <li><code>Event bus</code> is used by different vertices to communicate through asynchronous message passing  (JSON) (point to point, pub / sub, req / resp)</li> <li>We can have in memory event bus or clustered cross nodes, managed by Vert.x with a TCP protocol.</li> </ul> <p>Quarkus HTTP support is based on a non-blocking and reactive engine (Vert.x and Netty). All the HTTP requests our application receives, are handled by\u00a0event loops\u00a0(IO Thread) and then are routed towards the code that manages the request. Depending on the destination, it can invoke the code managing the request on a worker thread (Servlet, Jax-RS) or use the IO Thread (reactive route).</p> <p></p> <p>(Images src: quarkus.io)</p> <p>The application code should be written in a non-blocking manner using SmallRye Mutiny or RsJava libraries.</p> <p>So when interactive with different services using kafka as an inter service communication layer the producer and consumer are handlers and the internal processing can be schematized as:</p> <p></p> <p>Vert.x and reactive messaging applications may be combined to byild reactive systems to support the reactive manifesto:</p> <p></p> <ul> <li>To achieve resilience and responsiveness, microservice can scale vertically using vertices inside the JVM and horizontally via pod scaling capability within a Kubernetes cluster or AWS ECS cluster. The inter-communication between vertices is done via <code>event bus</code> and managed by the Vert.x library using virtual addresses, service discovery and event bus. </li> <li>Internal service communication (even cross pods) and external cross service boundary are message driven. Using Kafka, they are also durable improving resilience and recovery.</li> <li>Kubernetes enforces part of the reactive manifesto at the container level: elasticity and resilience with automatic pod recovery and scheduling.</li> </ul>"},{"location":"patterns/reactive/#microprofile-reactive-messaging","title":"MicroProfile reactive messaging","text":"<p>The MicroProfile Reactive messaging specification  aims  to  deliver  applications  embracing  the characteristics of reactive systems as stated by reactive manifesto. It enables non-blocking, asynchronous message passing between services, giving them the ability to scale, fail, and evolve independently.</p> <p>To summarize the main concepts, developer declares channels as way to get incoming or outgoing messages between CDI Java beans and connector to external message brokers:</p> <p></p> <p>The potential matching declarations for the connector, for the above figure, may look like below:</p> <pre><code># Kafka connector to items topic mapped to the item-channel\nmp.messaging.incoming.item-channel.connector=smallrye-kafka\nmp.messaging.incoming.item-channel.topic=items\n</code></pre> <p>For code which defines the 'outhoing' and 'incoming' message processing see this quarkus guide, the EDA quickstart code templates for producer and consumer</p> <pre><code>public class OrderService {\n\n    @Channel(\"orders\")\n    public Emitter&lt;OrderEvent&gt; eventProducer;\n\n        public OrderEntity createOrder(OrderEntity order) {\n            try {\n            // build orderPayload based on cloudevent from order entity\n            Message&lt;OrderEvent&gt; record = KafkaRecord.of(order.getOrderID(),orderPayload);\n            eventProducer.send(record);\n            logger.info(\"order created event sent for \" + order.getOrderID());\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n}\n</code></pre> <p>JMS and message driven bean, were the messaging APIs to asynchronously communicate with other applications.  They support transaction and so it is an heavier protocol to use. They do not support asynchronous IO. </p> <p>When building microservices, the CQRS and event-sourcing patterns provide an answer to the data sharing requirement between microservices. Reactive Messaging can also be used as the foundation to CQRS and Event-Sourcing  mechanisms.</p> <p>When we use <code>@Incoming</code> and <code>@Outgoing</code> annotations, the runtime framework (Open Liberty or Quarkus) creates a Reactive Streams component for  each method and joins them up by matching the channel names.</p> <p>A simple guide from Quarkus web site with integration with Kafka. </p> <p>Open Liberty supports this specification implementation.</p>"},{"location":"patterns/reactive/#mutiny","title":"Mutiny","text":"<p>Mutiny is a modern reactive programming library to provide more natural, readable reactive code. It supports asynchrony, non-blocking programming and streams, events, back-pressure and data flows.</p> <p>With Mutiny both <code>Uni</code> and <code>Multi</code> classes, expose event-driven APIs: we express what we want to do upon a given event (success, failure, etc.). These APIs are divided into groups (types of operations) to make it more expressive and avoid having 100s of methods attached to a single class.</p> <p>This section of the product documentation goes over some examples on how to use Uni/ Multi.</p>"},{"location":"patterns/reactive/#amqp","title":"AMQP","text":"<p>Advanced Message Queueing Protocol is an international standard for interoperability between messaging middlewares. IBM MQ supports AMQP client via specific AMQP channel. Clients can connect to the queue manager and send / receive messages to / from queue. </p>"},{"location":"patterns/reactive/#knative-eventing","title":"Knative eventing","text":"<p>Knative is Kubernetes based platform to develop serverless. Major value proposition is a simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. </p> <p>Knative consists of the following components:</p> <ul> <li>Eventing - Management and delivery of events</li> <li>Serving - Request-driven compute that can scale to zero</li> </ul> <p>See the RedHat Knative cookbook for a simple tutorial.</p>"},{"location":"patterns/reactive/#code-samples","title":"Code samples","text":"<ul> <li>Vert.x kafka client</li> <li>Experiences writing a reactive Kafka application</li> </ul>"},{"location":"patterns/reactive/#more","title":"More...","text":"<ul> <li>Reactive Systems Explained - Book from Grace Jansen - Peter Gollmar</li> <li>Reactive Java Modules show how to build an event-driven, streams-optimized Kafka-based Java application: You will use the Eclipse MicroProfile Reactive Messaging API and Open Liberty to build it and then you'll learn how to test it in true-to-production environments by using containers with MicroShed testing.</li> <li>Clement Escoffier's book Building reactive microservice in Java</li> </ul>"},{"location":"patterns/saga/","title":"Saga: long running transaction for distributed systems","text":""},{"location":"patterns/saga/#problems-and-constraints","title":"Problems and Constraints","text":"<p>With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices, which are per nature, distributed. With event backbone technology two-phase commit is not an option.</p>"},{"location":"patterns/saga/#solution-and-pattern","title":"Solution and Pattern","text":"<p>Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern helps to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions.</p> <p>With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure illustrates the happy path for a CarRide order transaction where each services are participant of the transaction:</p> <p></p> <p>When the car ride order is created, the business process says, we need to compute an ETA for arrival at destination. ETA is computed by looking at the closest autonomous car from the pickup location. We suppose the customer accept the ETA notification, and <code>CarManager</code> allocates one \"autonomous car\". The process continues when the traveler(s) reaches destination,the CarRideManager complete the <code>CarRide</code>, and the event is processed by the PaymentService to  initiated the payment, the car goes back to the pool and ride rating is initiated (not illustrated in the flow). </p> <p>Those actions / commands are chained. The final state (in this schema, not in the reality, as the process may have more steps) is the <code>CarRide</code> is in completed state in the <code>CarRide</code> Manager microservice.</p> <p>With a monolytic application, the consistency integrity between CarRide, Car, Customer and Payment tables will be done via transactions. But with distributed systems we could not easily apply strong two-phase commit transactions, this is where the Saga pattern helps to keep context to the conversation and be able to compensate in case of failure.</p> <p>SAGA pattern supports two types of implementation: Choreography and Orchestration.</p>"},{"location":"patterns/saga/#services-choreography","title":"Services choreography","text":"<p>With Choreography each service produces and listens to other service\u2019s events and decides, from the event content, if an action should be taken or not. As we talked about events, we will use an event bus to asynchronously exchange messages between the distributed components. From the previous example we do not need the autonomous car events as they most likely being aggregated by the CarManager service:</p> <p></p> <p>The traveler is using a mobile application, and defines where he wants the pickup, at what time, and for which destination. In the diagram above events are in orange, commands are in yellow.</p> <p>The first service, <code>CarRideManager</code>, executes a transaction to its own data store and then publishes an event ( <code>CarRideOrderCreated</code> event (1)) as a fact about its business entity (CarRide) update.  It maintains the business entity status, (CarRide.status) to the <code>Pending</code> state until the saga is completed. This event is listened by one or more services which execute local transaction and publish new events (CarDispatched (2), CarInMotion, CustomerNotified (3), PaymentProcessed (4)).</p> <p>The distributed transaction ends when the last service executes its local transaction and publishes a confirmation event. The main entity interested by the Saga manages its state accordingly. Here the CarRide is this entity.  When one of the service does not publish any event within an expected time, or publish a negative state (PaymentNotCompleted, CarNoMoreDispatched) the Saga should trigger a compensation. </p> <p>In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. Supposes the customer never acts when he/she receives the notification for payment, the car dispatching is already done so it is not idempotent, a compensation logic may be to route the transaction to a call center so they can reach by sms and phone call the customer. May be the contract stipulates that there will be a charge done anyway as credit card information exists on the account. Now if the failure comes from the payment processing, again a specific compensation process will trigger to get the payment completed. </p> <p></p> <p>In the diagram above we suppose When a message from any service is missing, the source service, needs to trigger a compensation process:</p> <p></p> <p>Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the CarRide example, in the rare case where one of the service is not able to provide a positive response, then the CarRide needs to change to 'Uncompleted' status, and an event to the CarRides topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above).  Another classical business flow is the order, payment and fullfilment flow, where failure to pay impact fullfilment step.  Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 above).</p> <p>Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the CarRideOrder is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level.</p>"},{"location":"patterns/saga/#services-orchestration","title":"Services orchestration","text":"<p>With orchestration, one service is responsible to drive each participant on what to do and when. This orchestration can be done with different tools and protocols. Workflow engine with synchronous calls, is a classical implementation in the SOA world. There are library supporting long running transaction like the Java With EDA, we do not want to loose any message as part of this orchestration, the technology of choice, to support stronger consistency and exactly once delivery, is to use queueing system with persistence, as illustrated by the following figure:</p> <p></p> <p>The pattern uses the req/res queues for each service participating into the Saga. The message may be typed and structured to map different commands. </p> <p>For that purpose we are no more in the Event world, but more on the commands supported by asynchronous messaging system.</p> <p>In the Kafka world, we can use topic configuration to represent the queue semantic: Kafka producer uses full acknowledge to be sure no message lost, idempotency to avoid duplicate at the broker level, and a batch size of 1. For the topic configuration, use one partition, one consumer in each consumer group, manual commit, poll one message at a time. The pattern implementation uses the different topics to control the Saga by issuing command messages to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received.</p> <p>If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key. Each  participant will undo its previous operations. Orchestrator is\u00a0a State Machine where each transformation corresponds to a command or message.</p> <p>See also this article from Chris Richardson on the Saga pattern.</p>"},{"location":"patterns/saga/#repositories-to-demonstrate-the-saga-patterns","title":"Repositories to demonstrate the Saga patterns","text":"<ul> <li>Saga choerography implementation with Kafka</li> <li>Saga orchestration on top of queueing (AWS SQS)</li> </ul>"},{"location":"solutions/autonomous-car/","title":"Index","text":""},{"location":"solutions/autonomous-car/#physical-deployment","title":"Physical Deployment","text":""},{"location":"solutions/autonomous-car/#carridemanager","title":"CarRideManager","text":"<p>The component is a Java Quarkus Application running as container inside a ECS cluster with Fargate runtime. The following figure illustrates the main components defined in the cdk definition.</p> <p></p> <p>The RDS Postgresql database is in the private subnet. Database admin user is created in AWS Secret Manager.</p>"},{"location":"solutions/autonomous-car/es-duplicate-evt/","title":"Handle duplicate delivery with AWS EventBridge","text":"<p>Problem statement: how to handle duplicate records when using AWS EventBridge as event backbone?. </p> <p>This is a common problem in many messaging systems, where producer retries can generates the same message multiple times or consumer retries on the subscribed topic, may generate duplicate processing of the received message. </p> <p>Producer retries because it did not received an acknowledge message from the receiver (which in our case is EventBridge), may be due to network failure.   </p> <p>The following diagram presents the potential problem. The left lambda function exposes an API to create \"CarRide\" entity and then to send message to Event Bus that the CarRide was created. The same applies if the CarRide is updated, like for example, the customer accepts the proposed deal so a autonomous car can be dispatched:</p> <p></p> <p>EventBridge is not a technology where the broker supports a protocol to avoid duplication, like Kafka does with the idempotence configuration for producer. So producer may generate duplicates. As we will see in producer section below, we can add event id specific to the application to trace duplicate records end to end (we use idempotencyID which has no business meaning, in real life it may be relevant to do a combined key with busines transaction ID). If the producer timeout after not receiving the acknowledge, it will send the message again. If the returned answer from EventBridge includes errors, the only things that can be done in producer code is to send messages not processed by the Event Bus to a Dead Letter Queue. </p> <p>As duplicates will exist, we need to have consumers supporting idempotency. As Woolf and Hohpe wrote in their \"Enterprise Integration Patterns book\" , supporting idempotency is to be able to process a message with the same effect, whether it is received once or multiple times.</p> <p>Also remember that in a pure EDA point of view, consumer can be added, as subscribers, over time to address new use cases. Producers should not be aware of those consumers, they just publish events as their internal business entity state changes. </p> <p>Amazon EventBridge has the following important characteristics that we can leverage to address de-duplication:</p> <ul> <li>Routing rules to filter or fan-out to a limited set of targets.</li> <li>Message Persistence with possible replays.</li> <li>Event replications to event bus in another region.</li> <li>Pipes to do point to point integration between source and destination.</li> </ul>"},{"location":"solutions/autonomous-car/es-duplicate-evt/#focus-on-producer-processing","title":"Focus on producer processing","text":"<p>Producer to EventBridge may generate duplicate messages while retrying to send a message because of communication issue or not receiving an acknowledgement response. The producer code needs to take into account connection failure, and manage retries.</p> <p>The SDK for EventBridge includes a method called put_events to send 1 to 10 events to a given Event Bus URL. Each record sent includes an envelop with the following structure:</p> <pre><code>\"Entries\": [\n    { \n    \"Source\": \"reference of the producer\",\n    \"Resources\": \"aws ARN of the producer app\"\n    \"DetailType\": \"CarRideEventType\",\n    \"Time\": datetime.today().strftime('%Y-%m-%d'),\n    \"Detail\": data,\n    \"EventBusName\": targetBus\n    }\n]\n</code></pre> <p>The message is, in fact, including more parameters as a set of common parameters are defined for each different EventBridge API mostly for versioning and security token. </p> <p>A EventBridge's response Entries array can include both successful and unsuccessful entries.</p> <p>The returned response includes an event <code>Id</code> (created by EventBridge) for each entry sent processed successfully, or an error object with code and message: </p> <pre><code>[\n    { \n         \"ErrorCode\": \"string\",\n         \"ErrorMessage\": \"string\",\n    },\n    {  \"EventId\": \"string\" }\n]\n</code></pre> <p>As for each record, the index of the response element is the same as the index in the Entries array, it is then possible to identify the message in error and to resend it or to move it to a Dead Letter Queue. </p> <p>There are some errors reported that are due to the EventBridge service, like a <code>ThrottlingException</code> or <code>InternalFailure</code> that may be retried. Other errors should never be retried but message sent to a DLD queue or saved in a temporary storage for future processing. </p> <p>To better support an EDA approach, we need to assign a unique idempotency identifier to each message, with a sequencer and track the processed messages using this identifier and the current count. We should not leverage the eventID created by the event bus, as it is local to this bus, and we may need to go over other components or even another event bus in another region. Also this eventId is not idempotent.</p> <p>Here is an example of payload creation in python, with the idempotencyId (the one from the producer point of view and not the EventBridge created one) as part of the <code>attributes</code> element (the metadata part of CloudEvents.io): </p> <pre><code>def defineCloudEvent(data):\n    attributes = {\n        \"type\": \"acme.acr.CarRiderCreatedEvent\",\n        \"source\": \"acr.com.car-ride-simulator\",\n        \"idempotencyId\": str(uuid.uuid1()),\n        \"eventTime\": datetime.today().strftime('%Y-%m-%d %H:%M:%S.%f'),\n        \"eventCount\": 1\n    }\n    cloudEventToSend = { \"attributes\": attributes, \"data\": data}\n    entries = [\n                { \"Source\": attributes[\"source\"],\n                \"DetailType\": \"CarRideEvent\",\n                \"Time\": datetime.today().strftime('%Y-%m-%d'),\n                \"Detail\": json.dumps(cloudEventToSend),\n                \"EventBusName\": targetBus,\n                \"TraceHeader\": \"carRideProcessing\"\n                },\n            ]\n    return entries\n</code></pre> <p>This approach supports the event replication used when deploying in two regions, and uses EventBridge Global Endpoint capability:</p> <p></p> <p>If a failover is triggered by Route 53 health check error on the primary event bus, then messages are sent to secondary region, and duplicates can be managed the same way as in a mono-region. Except if dynamoDB is not accessible by itself from the first region. In this case Global Table may be used. </p> <p>For EventBridge configuration here is an example of producer configuration in a SAM template:</p> <pre><code>Resources:\n  CarRideEventBus:\n    Type: AWS::Events::EventBus\n    Properties:\n      Name: !Ref EventBusName\n CarRideSimulatorFunction:\n    Type: AWS::Serverless::Function \n    Properties:\n      Environment:\n        Variables:\n          EventBus: !Ref EventBusName \n    Policies:\n      - Statement:\n        - Sid: SendEvents\n          Effect: Allow\n          Action:\n          - events:PutEvents\n          Resource:\n            - !GetAtt CarRideEventBus.Arn\n</code></pre>"},{"location":"solutions/autonomous-car/es-duplicate-evt/#the-consumer-part","title":"The Consumer part","text":"<p>Only the consumer of the message can identify duplicate records. If the consumer is not using an idempotent backend for persistence, then it needs to track the messages that it has processed in order to detect and discard duplicates. </p> <p>We assume the consumer is looking at the events, from another bounded context, and it is interested to subscribe to the event bus to support its own joining queries or to participate in a long-running transaction. </p> <p>Below is the SAM template illustrating to the CarRides EventBus rule to push to a lambda consumer, using a DLQ in case of issue.</p> <pre><code>CarRidesToConsumerRule:\n    Type: AWS::Events::Rule\n    Properties:\n      Description: \"EventRule to move CarRides transactions to Lambda Consumer\"\n      EventBusName: !Ref CarRideEventBus\n      EventPattern:\n        source:\n          - acr.com.car-ride-simulator\n        detail-type:\n          - CarRideEvent \n      State: \"ENABLED\"\n      Targets:\n        - Arn: !GetAtt CarRidesConsumerFunction.Arn\n          Id: \"CarRidesLambdaConsumer\"\n          DeadLetterConfig:\n            Arn:  !GetAtt CarRidesDLQQueue.Arn\n</code></pre>"},{"location":"solutions/autonomous-car/es-duplicate-evt/#dynamodb-backend","title":"DynamoDB backend","text":"<p>If the consumer uses DynamoDB as persistence layer then it can use the updateItem API to edit the existing event's attributes, or use the get_items to validate presence of the event and then adds a new event to the table if the event does not already exist. </p> <p></p> <p>The approach is to declare a table using the idempotencyId as a key:</p> <pre><code>  CarRideEventsTable:\n    Type: AWS::Serverless::SimpleTable\n    Properties:\n      TableName: !Ref TableName\n      PrimaryKey: \n        Name: idempotencyId\n        Type: String\n</code></pre> <p>The code then uses something like the following snippet, doing nothing in case of exception</p> <pre><code>  existingEvent = None\n  try:\n      response = eventsProcessed.get_item(Key= {\"idempotencyId\": idempotencyId})\n      existingEvent = response[\"Item\"]\n  except ClientError as err:\n      logger.info(\"Could not find events %s\",idempotencyId)\n\n  if existingEvent is None:\n      # call process the new event only if there is no duplicate\n      processEvent(event)\n      eventsProcessed.put_item(\n          Item={\n              \"idempotencyId\" : idempotencyId,\n              \"payload\": payload\n          })\n</code></pre> <p>The DynamoDB configuration is mono-region multi AZ for high availability. </p>"},{"location":"solutions/autonomous-car/es-duplicate-evt/#demonstrate-with-a-simulator","title":"Demonstrate with a simulator","text":"<p>The code repository, supports the following deployment and instructions to deploy it using AWS Serverless Application Management</p> <p></p> <p>Here is an example of record persisted in the DynamoDB CarRideEvents table, illustrating that <code>eventCount</code> is always 1.</p> <p></p>"},{"location":"solutions/autonomous-car/es-duplicate-evt/#single-application-instance","title":"Single application instance:","text":"<p>The simplest in memory solution uses  a simple HashMap as cache. It may work for an application running continuously, with a single instance. The basic code may look like: </p> <pre><code>def lambda_handler(event, context):\n    idempotencyId = event[\"detail\"][\"attributes\"][\"idempotencyId\"]\n    payload = event[\"detail\"]\n    existingEvent = eventsProcessed.get(idempotencyId)\n    if existingEvent is None:\n        processEvent(event)\n        eventsProcessed[idempotencyId] = payload\n    else:\n        logger.info(\"%s most likely duplicate \", idempotencyId)\n        logger.info(event)\n</code></pre> <p>See the code here. and how to run it. </p>"},{"location":"solutions/autonomous-car/es-duplicate-evt/#using-caching-with-redis","title":"Using Caching with Redis","text":"<p>When we define multiple concurrent instances of the application, like lambda function, we need to use a distributed, clustered data cache like AWS MemoryDB for Redis.</p> <p>At the consumer side before processing a new message, the code checks if the <code>idempotencyId</code> already exists in the cache. If it does, consider it a duplicate and discard it. </p> <pre><code>\n</code></pre> <p>The problem is how to evict older events from the cache. A scheduled processing may be used to remove any messages older than to current timestamp - n minutes. The n can be computed by assessing the risk to get a duplicate messages within this time window, and any resource constraints like memory usage.</p>"},{"location":"solutions/autonomous-car/es-duplicate-evt/#alternates","title":"Alternates","text":""},{"location":"solutions/autonomous-car/es-duplicate-evt/#powertool-for-aws-lambda","title":"Powertool for AWS Lambda","text":"<p>Powertools for AWS Lambda (Python) supports idempotency constructs via annotations and persistence into DynamoDB which should help to support idempotency processing inside of a Lambda function. This is a valuable solution. Lambda oriented. To use CloudEvents there is a way to define <code>jmespath</code> to get idempotency key from the json message.  </p>"},{"location":"solutions/autonomous-car/es-duplicate-evt/#use-outbox-pattern","title":"Use Outbox pattern","text":"<p>Another, may be more elegant, implementation is to use the Outbox pattern apply from the producer database, and write the events to a table in DynamoDB, do change data capture on this outbox table, using DynamoDB Streams, use EventBridge pipe to process the streaming data and then to send it to SNS target, to scatter and gather to multiple consumers.</p> <p></p> <p>Other considerations:</p> <p>Message Time-To-Live (TTL): Set a Time-To-Live (TTL) value for messages in the queue. If a message remains in the queue beyond its TTL, consider it expired and discard it. This helps prevent the processing of stale or duplicate messages that might have been delayed or requeued due to failures.</p>"},{"location":"solutions/gitops_IaC/","title":"Kafka and Event-Driven deployment with GitOps and Infrastructure as Code","text":"Updates <p>Created 08/2024 from older notes</p> <p>The goal of this chapter is to go over the deployment considerations for a cloud-native event-driven application using GitOps / Infrastructure as Code practices.</p> <p>The core concept of GitOps is to maintain a single Git repository that consistently holds declarative descriptions of the desired infrastructure in the production environment. An automated process ensures that the production environment aligns with the state described in the repository.</p>"},{"location":"solutions/gitops_IaC/#context","title":"Context","text":"<p>In this chapter, we want to demonstrate the following components deployment: </p> <p></p> <ol> <li>Kafka cluster</li> <li>Schema registry</li> <li>Topic definitions</li> <li>Producer and Consumer Apps</li> <li>Avro schema deployment</li> </ol> <p>Which is an instantiation of the broader EDA blueprint as illustrated in the following diagram:</p> <p></p> <p>To better improve DevOps automation and the overall solution governance, GitOps and Intrastructure as Code practices should help deploying all those components as source code.</p> <p>We will also address some generic development practices for schema management.</p>"},{"location":"solutions/gitops_IaC/#components-for-gitops","title":"Components for GitOps","text":"<p>The following diagram illustrates the technical components involved in a typical production deployment of an event-driven solution.</p> <p></p> <p>The diagram organizes components according to when they are introduced during system development\u2014either early or late\u2014and whether they are high-level application-oriented components or low-level system-oriented components. For example, GitHub serves as a foundational system component essential for structuring the deployment of event-driven solutions. In contrast, streaming or event-driven applications are higher-level components that depend on the prior deployment of other components.</p> <p>The color coding indicates that blue components are part of the solution, red components are associated with GitOps on Kubernetes or RedHat OpenShift, and green components are external to the Kubernetes cluster, even if they could potentially be integrated. Helm and Kustomize.io representsa way to define deployments, while Sealed Secrets is a service for managing secrets.</p> Helm &amp; Helm Chart <p>Helm is a package manager for Kubernetes that simplifies the deployment and management of applications on Kubernetes clusters. Helm Chart is a collection of files that describe a related set of Kubernetes resources. It includes a Chart.yaml file containing metadata about the chart, templates for the Kubernetes manifests, and a values.yaml file for configuration settings. Helm charts allow users to package applications and share them easily, facilitating consistent deployment and versioning.</p> <p>Among the later components to deploy, we have included everything necessary to monitor both the solution and the infrastructure.</p>"},{"location":"solutions/gitops_IaC/#event-driven-applications","title":"Event-driven applications","text":"<p>Event-driven applications support business logic, are based on microservices, and utilize reactive messaging through message queues (MQ) or Kafka APIs. These applications offer OpenAPIs for mobile and web applications and provide AsyncAPI specifications for producing events to Kafka or messages to MQ. Both OpenAPI and AsyncAPI definitions are managed by the API manager and the event endpoint manager.</p> <p></p> <p>Schema definitions are managed by a Schema Registry.</p>"},{"location":"solutions/gitops_IaC/#event-streaming-applications","title":"Event-streaming applications","text":"<p>Event-streaming applications support stateful processing using Kafka Stream APIs or Apache Flink.</p>"},{"location":"solutions/gitops_IaC/#kafka-cluster","title":"Kafka Cluster","text":"<p>When using Cloud provider, managed service for Kafka, the cluster can be defined using infrastructure as code. When deploying Kafka onto Kubernetes platform Strimzi defines customer resources with operator to declare any Kafka components as Yaml manifests. See the Strimzi getting started to install the Strimzi Operator on a local kubernetes. Once the operator is running it will watch for new custom resources and create the Kafka cluster, topics or users that correspond to those custom resources.</p>"},{"location":"solutions/gitops_IaC/#queue-manager","title":"Queue manager","text":"<p>A queue manager provides queuing services through various MQ APIs. It hosts the queues that store messages produced and consumed by connected applications and systems. Queue managers can be interconnected via network channels, enabling messages to flow between disparate systems and applications across different platforms, including both on-premises and cloud environments.</p> <p>some note on labs. See deployment under eda-rt-inventory solution</p> <pre><code>kubectl get pod -n kafka --watch\nkubectl logs deployment/strimzi-cluster-operator -n kafka -f\n</code></pre>"},{"location":"techno/avro-schemas/","title":"Apache Avro, Data Schemas and Schema Registry","text":"Updates <p>Created 01/2019  Last update 9/05/2024 Work in progress</p> <p>This chapter describes why Avro and Schema registry are important elements of any event-driven solutions.</p>"},{"location":"techno/avro-schemas/#why-this-is-important","title":"Why this is important","text":"<p>Loosely coupling and asynchronous communication between applications does not negate the need for a contract that enforces certain constraints between producers and consumers.</p> <p>When we refer to a contract, schemas often come to mind, similar to how we used XSD. In the context of JSON, both JSON Schema and Avro schemas can be employed to define the data structure of messages. Given the importance of metadata in messaging, CloudEvents has become widely accepted as a specification for describing event data. Additionally, AsyncAPI establishes standards for events and messaging within the asynchronous landscape from an API perspective. It integrates message schemas, channels, and binding definitions, providing consumers with the critical information needed to access a data stream or queue.</p> <p>The contract is defined by a schema. From an event-driven architecture (EDA) design perspective, the producer is responsible for defining this schema, as it manages the life cycle of the main business entities from which business events are generated. The producer ensures that messages comply with the schema for serialization. While when using the command pattern, most likely with Queue, the consumer is the owner of the schema definition.</p> <p>In addition to these specifications, various technologies support contract management, including schema registries and API managers. The following figure illustrates the foundations for integration between producers, schema registries, and consumers.</p> <p></p> <p>The Schema Registry provides producer and consumer APIs that enable them to determine whether the event they intend to produce or consume is compatible with previous versions or aligns with the expected version. For this process, both producers and consumers must access the schema definition during serialization and deserialization.</p> <p>These can be done either by:</p> <ol> <li>Reading the schema from a local resource in the producer application such as a file, variable, property, or a Kubernetes construct like a ConfigMap or Secret.</li> <li>Retrieving the schema definition from the Schema Registry using a name or ID.</li> </ol> <p>When a producer wants to send an event to a Kafka topic, two actions occur:</p> <ol> <li>The producer checks that the event complies with the schema. If it does not, an error is raised.</li> <li>The producer verifies whether the event is compatible with previous versions or aligns with the expected version.</li> </ol> <p>For the first action, the producer has what it needs: the schema definition and the event that must comply with it, enabling the compliance check. For the second action, where the producer must ensure that the event's schema definition is compatible with any existing schema for the relevant topic (if one exists), it may need to consult the Schema Registry.</p> <p>Producers (and consumers) maintain a local cache of schema definitions and their versions, along with unique global IDs, all retrieved from the Schema Registry for the topics they wish to produce or consume events from. If the producer has a schema definition in its local cache that matches the required schema for the event's serialization, it can send the event without issue.</p> <p>However, if the producer does not have a matching schema definition in its local cache\u2014either due to version differences or a lack of definition\u2014it will contact the Schema Registry.</p> <p>If a matching schema definition for the relevant topic exists in the Schema Registry, the producer can retrieve the global unique ID for that schema definition and locally cache both the schema and its ID for future events. This avoids the need to contact the Schema Registry again.</p> <p>If no matching schema definition exists in the Schema Registry, the producer must register the schema definition it has for that topic. This ensures that the schema is available for any consumer wishing to consume events that comply with it.</p> <p>To achieve this, the producer must be configured for auto-registration of schemas and provided with the appropriate credentials, especially if the Schema Registry implements role-based access control (RBAC).</p> <p>If the producer is not configured for auto-registration or lacks the necessary credentials, the send event action will fail until a compatible schema definition for the relevant topic is registered in the Schema Registry.</p> <p>If the producer is configured for auto-registration and has the required credentials, the Schema Registry will validate the compatibility of the current schema definition with any existing definitions for the relevant topic. This ensures that if the schema being registered is a newer version, it remains compatible, preventing any negative impact on consumers.</p> <p>Once the schema definition or a new version of an existing schema is successfully registered in the Schema Registry, the producer retrieves its global unique ID to keep its local cache up to date.</p> <p>Warning</p> <p>We strongly recommend that producers be restricted from automatically register schema definitions in the Schema Registry, the reason is to ensure better governance and management. It is advisable that schema definition registration be handled by a designated individual with the appropriate role, such as an administrator, API manager, asynchronous API manager, development manager, or operator.</p> <p>Once the producer has a schema definition in its local cache, along with its global unique ID, that matches the schema required for the serialization of the event, it produces the event to the Kafka topic using the appropriate <code>AvroKafkaSerializer</code> class. The global unique ID of the schema definition is serialized alongside the event, eliminating the need for the event to carry its schema definition, as was the case in older messaging or eventing technologies and systems.</p> <p>By default, when a consumer reads an event, the schema definition for that event is retrieved from the Schema Registry by the deserializer using the global unique ID specified in the consumed event. The schema definition is retrieved from the Schema Registry only once, when an event contains a global unique ID that is not found in the consumer's local schema definition cache.</p> <p>The global unique ID can be located either in the event headers or within the event payload, depending on the producer application's configuration. When the global unique ID is located in the event payload, the data format begins with a magic byte, which serves as a signal to consumers, followed by the global unique ID and the message data as usual. For example:</p> <pre><code># ...\n[MAGIC_BYTE]\n[GLOBAL_UNIQUE_ID]\n[MESSAGE DATA]\n</code></pre> <p>Be aware that non-Java consumers may use a C library that also requires the schema definition for the deserializer, as opposed to allowing the deserializer to retrieve the schema definition from the Schema Registry, as explained above. The strategy should involve loading the schema from the Schema Registry via an API. See Kafka python reported issue</p>"},{"location":"techno/avro-schemas/#schema-serialization-approaches","title":"Schema Serialization Approaches","text":""},{"location":"techno/avro-schemas/#avro","title":"Avro","text":"<p>Apache Avro is an open source data serialization framework that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for the data, as well as map it to the programming language of your choice.</p> <p>It was developed within the Hadoop project and is particularly popular in big data ecosystems. The core values are:</p> <p>Here\u2019s a concise overview of the benefits of Apache Avro based on the Confluent blog post:</p> <ul> <li>Direct Mapping to JSON: Avro supports seamless conversion between its binary format and JSON, making it versatile for various applications.</li> <li>Compact Format: Unlike JSON, which repeats field names for every record, Avro\u2019s compact format minimizes data size, enhancing efficiency for high-volume usage. Schemas are readable, and as data is in binary format, it is well fitted for systems requiring efficient serialization.</li> <li>Dynamic typing: Avro supports dynamic typing, which can be more flexible than Protobuf's static typing in some scenarios.</li> <li>Self-describing data: Avro data files include the schema, making them self-describing and easier to work with in some contexts.</li> <li>Rich data structures: Avro supports complex data types like unions, which can be useful in certain scenarios.</li> <li>Speed: Avro is designed for fast serialization and deserialization, improving performance in data processing.</li> <li>Language Bindings: It offers extensive bindings for multiple programming languages, allowing developers to generate Java objects easily. Moreover, it enables generic tools for any data stream without needing code generation. While not as widespread as Protobuf, Avro has good language support, especially in the Java ecosystem.</li> <li>Rich Schema Language: Avro features a robust, extensible schema language defined in pure JSON, facilitating clear data definitions.</li> <li>Schema evolution: Avro provides an effective mechanism for managing schema evolution, ensuring that data formats can adapt over time without breaking compatibility.</li> </ul> <p>It has a maven integration to generate Java POJO from schema. There are also libraries for doing Avro with Python code, use Pydantic to Avro and Avro schema to Pydantic classes.</p>"},{"location":"techno/avro-schemas/#json","title":"Json","text":"<p>JSON Schema is another important tool for defining the structure of JSON data. The main values are the same than avro: data validation, human readable documentation of the data structure, code generation from JSON Schema, interoperability as it is supported by various programming language.</p> <p>Developers already familiar with JSON find it easy to adopt. But is verbose, schemas can become quite lengthy for complex data structures.</p> <p>The major issues are: </p> <ul> <li>Validation can be slower compared to binary formats like Avro or Protobuf.</li> <li>Schema evolution is not as seamless as with Avro.</li> </ul>"},{"location":"techno/avro-schemas/#protobuf","title":"Protobuf","text":"<p>Protobuf (Protocol Buffers) is a language-agnostic interface definition language developed by Google. </p> <p>Protobuf allows developers to define the structure of their data using a simple language-agnostic specification. This definition can then be used to generate code in various programming languages, enabling easy serialization and deserialization of structured data.</p> <p>The advantages:</p> <ul> <li>Efficiency: Protobuf serializes data into a compact binary format, resulting in smaller message sizes compared to text-based formats like JSON or XML.</li> <li>Speed: Binary serialization and deserialization are generally faster than text-based alternatives.</li> <li>Language-agnostic: Protobuf supports code generation for multiple programming languages, facilitating interoperability between different systems.</li> <li>Backward and forward compatibility: Protobuf's schema evolution rules make it easier to update message definitions without breaking existing systems.</li> <li>Strong typing: The schema definition provides clear data types, reducing errors and improving code quality.</li> <li>Tooling support: Many tools and frameworks support Protobuf, including gRPC for efficient RPC communication.</li> </ul> <p>Some challenges:</p> <ul> <li>Learning curve: Developers need to learn Protobuf's syntax and tools, which can be an initial hurdle.</li> <li>Human readability: The binary format is not human-readable, making debugging more challenging without proper tools.</li> <li>Limited built-in types: Protobuf has a smaller set of built-in data types compared to some other schema definition languages.</li> <li>Complexity for simple use cases: For very simple data structures or when human readability is crucial, JSON or XML might be more appropriate.</li> <li>Size overhead for small messages: While efficient for larger datasets, Protobuf can introduce some overhead for very small messages.</li> <li>Less flexibility: Compared to schema-less formats like JSON, Protobuf requires more upfront design and can be less flexible for rapidly changing data structures.</li> </ul> <p>Avro can be slower than Protobuf for serialization/deserialization</p>"},{"location":"techno/avro-schemas/#schema-registry-technologies","title":"Schema Registry Technologies","text":""},{"location":"techno/avro-schemas/#confluent-schema-registry","title":"Confluent Schema Registry","text":"<p>The Confluent Schema Registry is primarily designed for Kafka ecosystems but can be used with other systems. It supports Avro, JSON Schema, and Protobuf formats. It provides schema versioning and compatibility checks (backward, forward, full) and offers rich REST API for schema management and retrieval.</p> <ul> <li>Best for organizations using Kafka for event-driven architectures and streaming data pipelines.</li> <li>Suitable for managing complex schema evolution and compatibility scenarios.</li> <li>It is more platform-agnostic and integrates seamlessly with Confluent Kafka and other Kafka clients. </li> <li>Provides compatibility with various data streaming and processing frameworks beyond Kafka, such as Spark and Flink.</li> <li>Pricing can depend on whether you use the open-source version or the Confluent Cloud service.</li> <li>Confluent Cloud offers a managed service with a subscription model based on usage.</li> <li>Strong community support, especially for Kafka users.</li> <li>Comprehensive documentation and robust community resources</li> </ul> <p>The Confluent Schema Registry attaches the schema identifier by storing it as part of the Kafka message key or value payload after a Magic Byte, which increases the payload size by 5 bytes. </p> <p>It works as part of Confluent Platform or Confluent Cloud managed service. </p>"},{"location":"techno/avro-schemas/#apicurio","title":"Apicurio","text":"<p>Apicur.io includes a schema registry to store schema definitions. It supports Avro, Json, protobuf schemas, and an API registry to manage OpenApi and AsynchAPI.</p> <p>Apicur.io is a Cloud-native Quarkus Java runtime with low memory footprint and fast deployment times. It supports different persistences like Kafka, Postgresql, and supports different deployment models.</p> <p></p> <p>Can be deployed in Kubernetes platform, and managed by operator. It supports HA with leader-follower. It can persist states in local Kafka cluster, and can suppport remote Kafka clusters too.</p>"},{"location":"techno/avro-schemas/#registry-characteristics","title":"Registry Characteristics","text":"<ul> <li>The registry supports adding, removing, and updating the following types of artifacts: OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google protocol buffers, JSON Schema, Kafka Connect schema, WSDL, XML Schema (XSD).</li> <li>Schema can be created via Web Console, a user friendly interface to manage client and core REST APIs usable for CI/CD pipelines and Maven plugin.</li> <li>It includes configurable rules to control the validity and compatibility.</li> <li>It can be integrated with various applications and services but is not tied to a specific ecosystem.</li> <li>It is suitable for microservices architectures and cloud-native applications.</li> <li>Client applications can dynamically push or pull the latest schema updates to or from Apicurio Registry at runtime.</li> </ul> <p>Apicurio is compatible with existing Confluent schema registry client applications.</p> <ul> <li>It includes client serializers/deserializers (Serdes) to validate Kafka and other message types at runtime.</li> <li>Operator-based installation of Apicurio Registry on Kubernetes and OpenShift</li> <li>Use the concept of artifact group to collect schema and APIs logically related.</li> <li>Support search for artifacts by label, name, group, and description</li> </ul> <p>When using Kafka as persistence, special Kafka topic <code>&lt;kafkastore.topic&gt;</code> (default <code>_schemas</code>), with a single partition, is used as a highly available write ahead log. </p> <p>All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. </p> <p>A Schema Registry instance both produces and consumes messages under the <code>_schemas</code> topic.  It produces messages to the log when, for instance, new schemas are registered under a subject, or when updates to  compatibility settings are made. The Schema Registry consumes from the <code>_schemas</code> log in a background thread, updating its local  caches with each new message to reflect the added schema or compatibility setting.</p> <p>This approach to updating local state from the Kafka log ensures durability, ordering, and easy recoverability.</p> <p>Apicur.io handles schema association to topics by schema name. For example, if we have a topic called orders, the schemas that apply to it would be avros-key (when using a composite key) and orders-value (which is likely based on CloudEvents and includes a custom payload).</p>"},{"location":"techno/avro-schemas/#aws-glue-schema-registry","title":"AWS Glue Schema Registry","text":"<p>AWS Glue Schema Registry is used in AWS service to manage and enforce schemas on your data streaming applications using convenient integrations with Apache Kafka, Amazon Managed Streaming for Apache Kafka, Amazon Kinesis Data Streams, Amazon Managed Service for Apache Flink, and AWS Lambda. It supports Avro, JSON Schema, and Protobuf formats, offers versioning and validation of schemas, and integrates with AWS Identity and Access Management (IAM) for security.</p> <ul> <li>For organizations using AWS for data lakes, ETL, and real-time data processing.</li> <li>Useful for managing data schemas in serverless architecture</li> <li>Need to assess performance and total cost of ownership</li> <li>Pricing is based on the number of registered schemas and requests made.</li> <li>Costs can vary depending on the AWS services used in conjunction.</li> </ul> <p></p> <p>If you are predominantly using AWS services, Glue Schema Registry may be the better fit. Conversely, if your infrastructure is heavily based on Kafka, Confluent Schema Registry would likely be more advantageous.</p>"},{"location":"techno/avro-schemas/#data-schemas","title":"Data Schemas","text":"<p>Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format.</p>"},{"location":"techno/avro-schemas/#how-does-a-data-schema-look-like","title":"How does a data schema look like?","text":"<p>Let's see how a data schema to define a person's profile:</p> <pre><code>{\n  \"namespace\": \"banking.schemas.demo\",\n  \"name\": \"profile\",\n  \"type\": \"record\",\n  \"doc\": \"Data schema to represent a person profile for a banking domain\",\n  \"fields \": [\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"surname\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"age\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"account\",\n      \"type\": \"banking.schemas.demo.account\"\n    },\n    {\n      \"name\": \"gender\",\n      \"type\": {\n        \"type\": \"enum\",\n        \"name\": \"genderEnum\",\n        \"symbols\": [\n          \"male\",\n          \"female\",\n          \"Other\"\n        ]\n      }\n    }\n  ]\n}\n</code></pre> Notice: <ol> <li>There are primitive data types like <code>string</code> and <code>int</code> but also complex types like <code>record</code> or <code>enum</code>.</li> <li>Complex type <code>record</code> requires a <code>name</code> attribute but it also can go along with a <code>namespace</code> attribute which is a JSON string that qualifies the name.</li> <li>Data schemas can be nested as you can see for the <code>account</code> data attribute. See below.</li> </ol> <pre><code>{\n  \"namespace\": \"banking.schemas.demo\",\n  \"name\": \"account\",\n  \"type\": \"record\",\n  \"doc\": \"Data schema to represent a customer account with the credit cards associated to it\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"savings\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"cards\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"int\"\n      }\n    }\n  ]\n}\n</code></pre> <p>In the picture below we see two messages, one complies with the above Apache Avro data schema and the other does not:</p> <p></p> <p>You may begin to recognize the advantages of validating data flowing into your Apache Kafka event backbone against a schema. This validation not only ensures data quality but also enhances consistency and interoperability across different systems.</p> <p>For more information on the Apache Avro Data Schema specification see https://avro.apache.org/docs/current/spec.html</p>"},{"location":"techno/avro-schemas/#benefits-of-using-data-schemas","title":"Benefits of using Data Schemas","text":"<ul> <li>Clarity and Semantics: They document the usage of the event and the meaning of each field in the \"doc\" fields.</li> <li>Robustness: They protect downstream data consumers from malformed  data, as only valid data will be permitted in the topic. They let the producers or consumers of data streams know the right fields are need in an event and what type each field is (contract for microservices).</li> <li>Compatibility: model and handle change in data format.</li> </ul>"},{"location":"techno/avro-schemas/#more-reading","title":"More reading","text":""},{"location":"techno/avro-schemas/#articles-and-product-documentation","title":"Articles and product documentation","text":"<ul> <li>Apicur.io schema registry documentation</li> <li>Confluent schema registry overview</li> <li>Producer code with reactive messaging and apicurio schema registry</li> <li>Consumer code with reactive messaging and apicurio schema registry</li> </ul>"},{"location":"techno/confluent/","title":"Confluent as Kafka provider","text":""},{"location":"techno/confluent/#confluent-cloud","title":"Confluent Cloud","text":"Updates <ul> <li>Created 07/2024</li> <li>Update 10/2024</li> </ul> <p>Managed service offering for the Confluent platform on one of the AWS, Azure or GCP cloud. It runs on its own private network that can be integrated with client virtual private network and on-premises network. </p> <p></p> <p>Cluster can be shared or dedicated.</p> <p>The services supported are:</p> <ul> <li>Schema governance</li> <li>OAuth support</li> <li>ksqlDB</li> <li>Managed Connectors</li> <li>Cluster linking</li> <li>Schema linking</li> <li>Logs and metrics</li> </ul>"},{"location":"techno/confluent/#core-concepts","title":"Core Concepts","text":"<p>When creating a cluster, developers are connected to the Confluent control plane and can select a cloud provider, then one of the possible regions (not all cloud provider regions are available).</p> <p></p> <p>The main three concepts are:</p> <ul> <li>Organization: Billing, sso, user management, service accounts, has 1 to many environments</li> <li>Environment: delineates logical separation of env and region, includes schema registry, and one to many cluster</li> <li>Cluster: manages topics, consumer groups, API keys, ACLS, ksqlDB queries</li> </ul> <p>Below is a set of important information, facts, links on the Confluent offering.</p>"},{"location":"techno/confluent/#control-plane","title":"Control Plane","text":"<p>The control plane runs on AWS in k8s. Clusters are running on k8s too via Confluent Kubernetes Operator. A dedicated monitoring cluster runs and Kafka is used to for async messaging between those components. The satellite clusters may run independently even is there is a problem on the control plane.</p> <p></p> <p>Isolation is done via private network, and namespaces.</p>"},{"location":"techno/confluent/#kafka-cluster","title":"Kafka cluster","text":"<ul> <li>One environment contains clusters, kafka connect cluster, Flink apps, ksqlDB, Schema registry.</li> <li> <p>Confluent Cloud clusters are available in 3 types: Basic, Standard, and Dedicated.</p> <ul> <li>Standard can be single or multi zones 99.99% SLA, and offers RBAC and OAuth</li> <li>Dedicated: 1GB/s Ingress and Egress, with private networking, and self cloud managed keys. Define you CIDR according to your VPC connection (Peering, transit gateway)</li> </ul> </li> <li> <p>Kafka API keys are required to interact with Kafka clusters in Confluent Cloud. Each Kafka API key is valid for a specific Kafka cluster. To access one API key/secret pair for each Kafka cluster. Key can be setup with global access to produce and consume from the cluster.</p> </li> <li> <p>The potential deployment topology pattern for hybrid cloud:</p> <ul> <li> <p>One time migration from on-premises Kafka cluster to a Confluent Cloud cluster - In fact in can also being steady state for data propagation to the cloud and may be data lake.</p> <p></p> </li> <li> <p>The second topology, is when both clusters are exchanging data over private link:</p> <p></p> </li> <li> <p>The 3nd topology addresses the need to deploy at a global scale, with local market with data sovereignty constraint. Cloud clusters are used to aggregate the data for global use, or for read access.</p> <p></p> </li> <li> <p>The last topology pattern is a pure cloud play, with different cloud providers, or different regions of the cloud provider, for disaster recovery.</p> <p></p> </li> </ul> </li> </ul>"},{"location":"techno/confluent/#cluster-types","title":"Cluster Types","text":"<p>Dimensions to consider for selecting cluster type are: 1/ the scaling needs, 2/ Networking 3/ sizing for ingress, egress, 4/ security</p> <ul> <li>Basic: small, get started - no sizing needed</li> <li>Standard: production ready for most apps</li> <li>Enterprise: Enhanced security with private networking </li> <li>Freight: Bring your own cluster in VPC, Write to S3, so higher latency.</li> <li>Dedicated: programming scaling, priced by CKU</li> </ul>"},{"location":"techno/confluent/#cku","title":"CKU","text":"<p>Measure the capacity and limits of Confluent Kafka cluster. A 1 CKU is a 4 brokers clusters. 2 CKUs are needed or HA at 6 brokers.</p>"},{"location":"techno/confluent/#networking","title":"Networking","text":"<ul> <li>Shared clusters have only public endpoints. Dedicated may use VPC, and VPC peering or AWS transit gateway, or private links </li> <li>Public endpoints are protected via API keys, but connections are TLS v1.2. The connection are going to a NLB for TCP ingress.</li> <li>Public endpoints are protected with a proxy layer, NLB and other security guards to avoid  DoS, DDoS, syn flooding, and other network-level attacks.</li> <li>Kafka uses TCP protocol, and get cluster metadata during the bootstrap connection. The broker endpoints is used to send or get data from the brokers. TLS. mTLS, SASL-SSL can be used to secure the connecting and authentication. Confluent Cloud uses SASL-SSL.</li> <li>The traffic between client and broker could not go over a load balancer. The load balancing is done via the list of DNS names and ports of the brokers. To get this list, client connect to the advertise listener. </li> <li> <p>Client apps can connect with TLS v1.2 over public endpoints.</p> </li> <li> <p>Client apps running on-premises may connect with Transit Gateway or Private Link. Private Link has no constraint on CIDR ranges, and offers a one-way connectivity from customer's VPC to Confluent Cloud. A private link can support access to multiple Confluent Kafka clusters.</p> </li> <li>When using public endpoint, Clients need access to internet without proxy. When the cluster has only private endpoints, then we need to setup a proxy (HAproxy, nginx..) to be able to at least see topics. The proxy runs in a Jump host within our own VPC. The alternate solution is to use dynamic proxy.</li> <li>When using Kafka Connector source and sink connectors, running on Confluent Cloud, those connectors need to be able to accessible the sources over the internet from Confluent Cloud.</li> </ul>"},{"location":"techno/confluent/#getting-started","title":"Getting Started","text":"<p>There are different ways to access Confluent Cloud: via users or service accounts. SC accesses Confluent Cloud services only, but can get role assignment. Each types have API keys.</p> <p>There are different quick start tutorials as:</p> <ul> <li>Confluent Fundamentals</li> <li>Hands-on on Confluent Cloud (CCloud).  URL fo CCloud- https://confluent.cloud/</li> <li>Here are import CLI commands:</li> </ul> <pre><code>confluent login --save\nconfluent environment list\nconfluent environment use env-xxxxxxx\nconfluent kafka cluster list\nconfluent kafka cluster use lkc....\nconfluent kafka topic list\nconfluent kafka topic create test-topic\nconfluent kafka topic describe test-topic\nconfluent kafka topic produce test-topic --parse-key\nconfluent kafka topic consume --from-beginning test-topic --group &lt;grp-name&gt;\n# use a key\nconfluent api-key list\nconfluent api-key use &lt;API_Key&gt;\n# Generate a client config\nconfluent kafka client-config create &lt;LANGUAGE&gt; --api-key &lt;API_KEY&gt; --api-secret &lt;API_SECRET&gt;\n</code></pre> <p>CLI configuration is saved in ~/.confluent/config.json</p>"},{"location":"techno/confluent/#api-keys","title":"API Keys","text":"<p>We need:</p> <ul> <li>Confluent Cloud login user and password</li> <li>Cluster API Key: <code>confluent api-key create --resource &lt;cluster_id&gt;</code></li> <li>Special Key for Flink: select a service account key for long running jobs while for interactive sessions, use user account key. For the REST API use base64 to encode the key. Keys are scoped per environment.</li> </ul>"},{"location":"techno/confluent/#demos-and-quick-starts","title":"Demos and quick starts","text":"<ul> <li>Quickstart with Console</li> <li>Java Table API Quick Start</li> <li>Different demos to use local kafka or Confluent Cloud cluster and Flink studies</li> </ul>"},{"location":"techno/confluent/#schema-registry","title":"Schema Registry","text":"<ul> <li>One registry per environment, created in the same region as the first kafka cluster is created, but can manage future clusters in other regions. A schema registry is local to one kafka cluster as it persists state in topics.</li> <li>Two \"governance packages\": Essentials (not in all region) and Advanced. Schema registries are accessible via public and private endpoints, and uses separate Access Keys from the Kafka cluster ones.</li> <li>The schema registry is stateless and keep state in Kafka. Multiple instances of the Schema registry can run in parallel, they will be a node supporting the write operation, and any node can serve a READ. All nodes know how to forward requests to the primary for WRITEs.</li> <li>If one Schema Registry node goes down, another node is elected leader and the cluster auto-recovers.</li> <li>In multi-tenant deployments, one physical Schema Registry cluster per cloud and geographic region, hosts many logical schema registries.</li> <li>At the environment level, developers may view and search schemas, monitor usage, and set a compatibility mode for schemas. </li> </ul> <ul> <li> <p>Possible to deploy into two regions with replicator and so one data center is the primary, with manual promotion for a secondary to become a primary. </p> </li> <li> <p>Spanning multiple datacenters (DCs) with your Confluent Schema Registry synchronizes data across sites, </p> </li> </ul> <p></p> <ul> <li>The Schema Registry nodes in both datacenters link to the primary Kafka cluster in DC A, and the secondary datacenter (DC B) forwards Schema Registry writes to the primary (DC A). Schema Registry instances in DC B have <code>leader.eligibility</code> set to false, meaning that none can be elected leader during steady state operation with both datacenters online.</li> <li>Replication replicates also topic for the schema persistence.</li> <li>Producers write data to just the active cluster. </li> <li>Consumers can read from both regions.</li> <li> <p>Access Control List needs to be replicated too.</p> </li> <li> <p>Confluent Cloud uses API keys that are resource scoped for Schema Registry clusters to store schemas and route requests to the appropriate logical clusters.</p> </li> </ul> <p></p> <ul> <li>This is a different API key than the one to access the kafka cluster.</li> <li>When apps are in VPC, the VPC needs to be able to communicate with  Confluent Cloud public end point on port 443.</li> </ul>"},{"location":"techno/confluent/#schema-file","title":"Schema file","text":"<p>The development steps can be summarized as:</p> <ol> <li>Define the record schemas in the selected target serialization, avro, json or protobuf and define files within <code>src/main/avro</code>, <code>src/main/json</code> or <code>src/main/proto</code>.</li> <li>Use maven or graddle plugins: to create java classes from the schema definitions</li> </ol>"},{"location":"techno/confluent/#protobuf","title":"Protobuf","text":"<p>A schema needs to define the version of the syntax of the protobuf spec. <code>package</code> is mapped to a java package name and define a namespace. Protobuf define a schema for a record as a message</p> <pre><code>package org.acme.payment.events;\noption java_outer_classname = \"PaymentEvent\"\n\nmessage PaymentEvent {\n    string payment_id = 1;\n    double amount = 2;\n    string customer_id = 3; \n}\n</code></pre>"},{"location":"techno/confluent/#avro","title":"Avro","text":"<p>Use avro syntax to define schema. </p>"},{"location":"techno/confluent/#hands-on","title":"Hands-on","text":"<ul> <li>See Hands-on schema registry 101.</li> <li></li> </ul>"},{"location":"techno/confluent/#schema-management","title":"Schema management","text":"<ul> <li>Still compatibility mode can be overridden at the topic/schema level. The default is backward compatibility: consumer can consume older messages, as default values are added to new attributes.</li> <li> <p>Forward compatibility means that data produced with a new schema can be read by consumers using the last schema, or the schema before. See details in compatibility note. With Forward compatibility mode, consumers aren\u2019t guaranteed to be able to read old messages.</p> </li> <li> <p>Stream governance feature helps addressing data governance</p> </li> <li>Schema can be created via CLI, REST API, Console or Maven plugin to be used during CI/CD</li> <li>Schema is associated to topic, with mechanism to support backward compatibility</li> <li>Avro was developed with schema evolution in mind, and its specification clearly states the rules for backward compatibility, not the case for Json or Protobuf.</li> <li>Transitive compatibility means checking a new schema against all previously registered schemas.</li> </ul>"},{"location":"techno/confluent/#kafka-connector","title":"Kafka Connector","text":"<p>Managed connectors supports only, as of now, a subset of the connectors available on Connector hub. Still it is possible to upload our own plugins.</p> <p>Connectors can access public or private (via VPC peering or transit gateways) sources and sinks.</p> <p>Confluent Cloud support hosted Connect.</p>"},{"location":"techno/confluent/#security","title":"Security","text":"<ul> <li>There are different API keys to access cluster, cloud, resources, schema registry and logs and metrics.</li> </ul>"},{"location":"techno/confluent/#infrastructure-as-code","title":"Infrastructure As Code","text":"<p>Terraform Confluent Provider to configure any CCloud resources.</p>"},{"location":"techno/confluent/#governance","title":"Governance","text":"<p>Two packages: essentials and advanced for enterprise uses cases that groups client-side field level encryption, and doing data quality rules.</p>"},{"location":"techno/confluent/#confluent-platform","title":"confluent Platform","text":"<p>This the on-premises deployment, which for development can run with docker compose or using the Confluent For Kubernetes (CFK).</p>"},{"location":"techno/confluent/#docker-local","title":"Docker Local","text":"<p>See this excellent cp-all-in-one repository with different patterns and configurations to run Confluent platform locally or hybrid with Confluent Cloud and local components such as Schema Registry, Flink, ksqlServer...</p>"},{"location":"techno/confluent/#cfk","title":"CFK","text":"<p>See confluent-kubernetes-examples git repo. which can be summarized as</p> <pre><code># define helm repo for confluent helm releases\nhelm repo add confluentinc https://packages.confluent.io/helm\nk create ns confluent\nhelm upgrade --install operator confluentinc/confluent-for-kubernetes -n confluent --set kRaftEnabled=true\n</code></pre>"},{"location":"techno/ibm-mq/","title":"IBM MQ in the context of EDA","text":"<p>IBM MQ is the enterprise solution to exchange message over queues.  As it supports loosely coupling communication between applications, via asynchronous protocol, and message exchange, it has to be part of any modern digital, responsive solutions, and so it makes sense to consider it in the context of EDA.</p> <p>This blog is a summary, for architects, about the technology and as it fits into EDA with pointers to other documentations, articles, and code repositories.</p> <p>We already addressed the difference between eventing and messaging systems, and we can affirm that real production plaform needs to include both.</p> <p>This site includes a lot of content around Kafka as the backbone to support EDA, but EDA is not just Kafka. I will prefer to mention that EDA is about modern asynchronous microservice based solution, that needs to exchange messages. Messages can be sent to Queues or Topics or both. IBM MQ delivers different features than Kafka and, as an architect, it is important to assess the fit for purpose.</p>"},{"location":"techno/ibm-mq/#key-concepts","title":"Key concepts","text":"<p>IBM MQ queue managers are the main component to define queues and where applications connect to.  They can be organized in network to deliver messages between applications and locations.  Queue Managers can be organized in cluster to increase high availability and scaling.</p> <p></p> <p>We encourage to read the article from Richard Coppen's: 'IBM MQ fundamentals'. Here are the main concepts:</p> <ul> <li>Queues are addressable locations to deliver messages to and store them reliably until they need to be consumed. We can have many queues and topics on one queue manager.</li> <li>Queue managers are the MQ servers that host the queues. They can be interconnected via MQ network.</li> <li>Channels are the way queue managers communicate with each other and with the applications.</li> <li>MQ networks are loose collections of interconnected queue managers, all working together to deliver messages between applications and locations.</li> <li>MQ clusters are tight couplings of queue managers, enabling higher levels of scaling and availability</li> <li>Point to point for a single consumer. Senders produce messages to a queue, and receivers asynchronously consume messages from that queue. With multiple receivers, each message is only consumed by one receiver, distributing the workload across them all.</li> <li>Publish/subscribe is supported via topic and subscription, and MQ sends copies of the message to those subscribing applications</li> <li>MQI is the message queue interface to control MQ objects. It supports commands like create queue, start, stop QM, alter objects' attributes, create channels...</li> <li>MQSC commands are commands to intereact with MQ objects. Adminstrator can use those commands with runmqsc, and they can be in a script to configure the broker at start up time.</li> </ul> Custom configuration with a MQ docker image <p>Create a config.mqsc file with all the MQSC commands, and then get the Dockerfile copy the file to <code>/etc/mqm</code> folder. See an example here..</p>"},{"location":"techno/ibm-mq/#major-mq-benefits-in-eda","title":"Major MQ benefits in EDA","text":"<ul> <li>MQ provides assured delivery of data: No data loss and no duplication, strong support of exactly once.</li> <li>MQ is horizontally scalable: As the workload for a single queue manager increases, it is easy to add more queue managers to share tasks and to distribute the messages across them.</li> <li>Highly available (See section below) with scalable architecture with different topologies.</li> <li>Integrate well with Mainframe to propagate transaction to the eventual consistent world of cloud native distributed applications. Writing to database and MQ queue is part of the same transaction, which simplifies the injection into event backbone like Kafka, via Kafka MQ connector.</li> <li>Containerized to run on modern kubernetes platform or other container orchestrator.</li> </ul>"},{"location":"techno/ibm-mq/#decentralized-architecture","title":"Decentralized architecture","text":"<p>The figure below illustrates the different ways to organize the MQ brokers according to the applications' needs.</p> <p></p> <ul> <li>On the top row, applications have decoupled queue managers, with independent availability / scalability reuquirements. The ownership is decentralized, as each application also owns the broker configurations and deployments. Such cloud native applications may adopt the Command Query Responsability Seggregation pattern and use queues to propagage information between the microservices. The deployment of both brokers and microservices follows the same CI/CD pipeline, with a <code>kustomize</code> for Kubernetes, for example, to describe the broker configuration.</li> <li>A central MQ broker can still be part of the architecture to support legacy applications integrations and federated queues. </li> <li>MQ Brokers are connected together to build a mesh.</li> </ul> <p>This type of deployment supports heterogenous operational procedures across technologies.</p>"},{"location":"techno/ibm-mq/#high-availability","title":"High availability","text":"<p>Important to always revisit the requirements, in term of availability measurement for any application modernization projects.</p> <p>For a messaging system, it is important to consider three characteristics:</p> <ul> <li>Redundancy so applications can reconnect in case of broker failure. Applications locally bound to a queue manager will limit availability. Recommended to use remote MQ client connection, and considering <code>automatic client reconnection</code>.</li> <li>Message routing: always delivers messages even with failures.</li> <li>Message availability: not loosing messages and always readable.</li> </ul> <p>With IBM MQ on multiplatforms, a message is stored on exactly one queue manager. To achieve high message availability, we need to be able to recover a queue manager as quickly as possible. We can achieve service availability by having multiple instances of queue manager for client applications to use, for example by using an IBM MQ uniform cluster.</p> <p>A set of MQ topologies can be defined to support HA:</p> <p></p> <ol> <li>Single resilient queue manager: MQ broker runs in a VM or a single container, and if it stops, the VM or pod scheduler will restart it. This is using the platform resynch capability combined with HA storage. IP Address is kept between the instances. The queue content is saved to a storage supporting HA. In the case of container, new restarted pod will connect to existing storage, and the IP gateway routes traffic to the active instance via service and app selector.</li> <li>Multi-instance queue manager: active - standby topology - Failover is triggered on failure of the active instance. IP Address is also kept. When using k8s, the stand-by broker is on a separate node, ready to be activated. The pods use persistence volumes with ReadWriteMany settings.</li> <li>Replicated data queue manager: this is an extension of the previous pattern where data saved locally is replicated to other sites.</li> </ol> <p>The deployed MQ broker is defined in k8s as a <code>StatefulSet</code> which may not restart automatically in case of node failure. So there is a time to fail over. A service will provide consistent network identity to the MQ broker.</p> <p>On Kubernetes, MQ relies on the availability of the data on the persistent volumes. The availability of the storage providing the persistent volumes, defines IBM MQ availability.</p> <p>For multi-instance deployment, the shared file system must support write through to disk on flush operation, to keep transaction integrity (ensure writes have been safely committed before acknowledging the transaction), must support exclusive access to files so the queue managers write access is synchronized. Also it needs to support releasing locks in case of failure.</p> <p>See product documentation for testing message integrity on file systems.</p>"},{"location":"techno/ibm-mq/#active-active-with-uniform-cluster","title":"Active-active with uniform cluster","text":"<p>With Uniform Cluster and  Client Channel Definition Tables, it will be possible to achieve high availability with at least three brokers and multiple application instances accessing brokers group via the CCDT. The queue managers are configured almost identically, and application interacts with the group.</p> <p></p> <p>You can have as many application instances as there are queue managers in the cluster. </p> <p>Here are the main benefits of Uniform Cluster:</p> <ul> <li>A directory of all clustering resources, discoverable by any member in a cluster</li> <li>Automatic channel creation and connectivity</li> <li>Horizontal scaling across multiple matching queues, using message workload balancing</li> <li>Dynamic message routing, based on availability</li> </ul> <p>The brokers are communicating their states between each others, and connection rebalancing can be done behind the scene without application knowledge. In case of a Queue manager failure, the connections are rebalanced to the active ones. Those applications do not need strong ordering. </p> <p>With the same approach, we can add new Queue manager See this video from David Ware abour active - active with Uniform cluster to see how this rebalancing works between queue managers as part of a Uniform queue manager.</p> <p></p> <p>See the 'MQ Uniform Cluster' related repository.</p>"},{"location":"techno/ibm-mq/#native-ha","title":"Native HA","text":"<p>Native HA queue managers involves an active and two replica brokers, each with their own set of Kubernetes Persistent Volumes.</p> <p>Native HA provides built in replication of messages and state across multiple sets of storage, removing the  dependency of replication and locking from the file system.</p> <p>Each replica writes to its own recovery log, acknowledges the data, and then updates its own queue data from the replicated recovery log.</p> <p></p> <p>When deployed on Kubernetes, a Service is used to route TCP/IP client connections to the current active instance.</p> <p>Set the availability in the queueManager configuration in the CRD:</p> <pre><code>  queueManager:\n    availability:\n      type: NativeHA\n</code></pre>"},{"location":"techno/ibm-mq/#hands-on","title":"Hands-on","text":""},{"location":"techno/ibm-mq/#installation-on-aws-ec2","title":"Installation on AWS EC2","text":""},{"location":"techno/ibm-mq/#installation-on-kubernetes","title":"Installation on Kubernetes","text":"<p>Starting with release 2020.2, MQ can be installed via Kubernetes Operator on Openshift platform. From the operator catalog search for MQ. See the product documentation installation guide for up to date details.</p> <p>You can verify your installation with the following CLI, and get the IBM catalogs accessible:</p> <pre><code>oc project openshift-marketplace\noc get CatalogSource\nNAME                   DISPLAY                TYPE      PUBLISHER     AGE\ncertified-operators    Certified Operators    grpc      Red Hat       42d\ncommunity-operators    Community Operators    grpc      Red Hat       42d\nibm-operator-catalog   ibm-operator-catalog   grpc      IBM Content   39d\nopencloud-operators    IBMCS Operators        grpc      IBM           39d\nredhat-marketplace     Red Hat Marketplace    grpc      Red Hat       42d\nredhat-operators       Red Hat Operators      grpc      Red Hat       42d\n</code></pre> <p>Once everything is set up, create an operator. The IBM MQ operator can be installed scoped to a single namespace or to monitor <code>All namespaces</code>.  </p> <p></p> <p>Verify your environment fits the deployment. Prepare your Red Hat OpenShift Container Platform for MQ Then once the operator is installed (it could take up to a minute), go to the operator page and create a MQ Manager instance. For example be sure to have defined an ibm-entitlement-key in the project you are planning to use to deploy MQ manager.</p> <p></p> <p>Then update the Yaml file for name, license and persistence.</p> <p></p> <p>As an alternate, define a QueueManager manifest yaml file as:</p> <p>```yaml apiVersion: mq.ibm.com/v1beta1 kind: QueueManager metadata:   name: eda-mq-lab spec:   version: 9.2.5.0-r3   license:     accept: true     license:      use: NonProduction   web:     enabled: true   queueManager:     name: \"EDAQMGR1\"     storage:       queueManager:         type: ephemeral   template:     pod:       containers:        - name: qmgr          env:          - name: MQSNOAUT            value: \"yes\"  <pre><code>Then create the QueueManager resource with `oc or kubectl cli`: \n\n```shell\noc apply -f mq-manager.yaml \noc get queuemanager\n# Get the UI route \noc describe queuemanager eda-mq-lab\n</code></pre> <p>You should get the console from this URL: https://eda-mq-lab-ibm-mq-web-....containers.appdomain.cloud/ibmmq/console/#/</p> <p></p> <p>To access to the <code>mqsc</code> CLI and run configuration remote connect via <code>oc exec -it &lt;podname&gt; bash</code>.</p> <p>We need route to access MQ from outside of OpenShift. The connection uses TLS1.2. </p> <p>you can customize the environment using config map:</p> <pre><code>  queueManager:\n    name: QM1\n    mqsc:\n      - configMap:\n          name: mq-mqsc-config\n          items:\n            -  example.mqsc\n# ...\n  template:\n    pod:\n      containers:\n        - name: qmgr\n          env:\n            - name: MQSNOAUT\n              value: 'yes' \n          envFrom:\n          - configMapRef:\n              name: mq-config\n</code></pre> <p>And</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mq-config\ndata:\n  LICENSE: accept\n  MQ_APP_PASSWORD: passw0rd\n  MQ_ENABLE_METRICS: \"true\"\n  MQ_QMGR_NAME: QM1\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mq-mqsc-config\ndata:\n  example.mqsc: |\n    DEFINE QLOCAL('ITEMS') REPLACE\n    DEFINE CHANNEL('DEV.ADMIN.SVRCONN') CHLTYPE(SVRCONN) REPLACE\n    DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE\n    ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE')\n    DEFINE CHANNEL(DEV.APP.SVRCONN) CHLTYPE(SVRCONN) \n    ALTER QMGR CHLAUTH (DISABLED)\n    REFRESH SECURITY TYPE(CONNAUTH)\n</code></pre>"},{"location":"techno/ibm-mq/#running-mq-in-docker","title":"Running MQ in docker","text":"<p>The following recent article from Richard J. Coppen presents such deployment, and can be summarized as:</p> <pre><code># Use Docker to create a volume:\ndocker volume create qm1data\n# Start queue manager: QM1\ndocker run --env LICENSE=accept --env MQ_QMGR_NAME=QM1 --volume qm1data:/mnt/mqm --name mq --rm --publish 1414:1414 --publish 9443:9443 --detach --env MQ_APP_PASSWORD=passw0rd ibmcom/mq:latest\n# The queue manager\u2019s listener listens on port 1414 for incoming connections and port 9443 is used by MQ console\n</code></pre> <p>One queue is created DEV.QUEUE.1 and a channel: DEV.APP.SRVCONN. </p> <p>Then <code>docker exec</code> on the docker container and use the <code>mqsc</code> CLI.</p> <p>The ibm-messaging/mq-container github repository describes properties and different configurations.</p> <p>You can also run it via docker compose. We have different flavor in the real time inventory gitops repository under <code>local-demo</code> folder. Here is an example of such compose file:</p> <pre><code>version: '3.7'\nservices:\n  ibmmq:\n    image: ibmcom/mq\n    ports:\n        - '1414:1414'\n        - '9443:9443'\n        - '9157:9157'\n    volumes:\n        - qm1data:/mnt/mqm\n    stdin_open: true\n    tty: true\n    restart: always\n    environment:\n        LICENSE: accept\n        MQ_QMGR_NAME: QM1\n        MQ_APP_PASSWORD: passw0rd\n        MQ_ENABLE_METRICS: \"true\"\n</code></pre>"},{"location":"techno/ibm-mq/#getting-access-to-the-mq-console","title":"Getting access to the MQ Console","text":"<p>The MQ Console is a web browser based interface for interacting with MQ objects. It comes pre-configured inside the  developer version of MQ in a container. On localhost deployment the URL is  https://localhost:9443/ibmmq/console/ (user admin) while on OpenShift it depends of the Route created.</p> <p>See this article for a very good overview for using the console.</p> <p>From the console we can define access and configuration:</p> <ul> <li>A new channel called MQ.QUICKSTART.SVRCONN. This new channel will be configured with NO MCA user. An MCA user is the identity that is used for all communication on that channel. As we are setting no MCA user this means that the identity used to connect to the queue manager will be used for MQ authorization.</li> <li>A channel authority record set to block no-one. We will do this so that any authority records you add in the following security step will be automatically configured to allow access the queue manager and resources below.</li> </ul> <p>When an application connects to a queue manager it will present an identity. That identity needs two permissions; one to connect to the queue manager and one to put/get messages from the queue.</p>"},{"location":"techno/ibm-mq/#some-useful-cli","title":"Some useful CLI","text":"<p>Those commands can be run inside the docker container: <code>oc exec -ti mq1-cp4i-ibm-mq-0 -n cp4i-mq1 bash</code> <pre><code># Display MQ version\ndspmqver\n# Display your running queue managers \ndspmq\n</code></pre></p> <p>To access to log errors</p> <pre><code>oc rsh &lt;to-mq-broker-pod&gt;\n# use you QM manager name instead of BIGGSQMGR\ncd /var/mqm/qmgrs/BIGGSQMGR/errors\ncat AMQERR01.LOG\n</code></pre>"},{"location":"techno/ibm-mq/#connecting-your-application","title":"Connecting your application","text":"<p>JMS should be your first choice to integrate a Java application to MQ. The mq-dev-patterns includes JMS code samples for inspiration.  See also this IBM developer tutorial  and our code example from the store simulator used in different MQ to Kafka labs.</p> <p>The article seems to have issue in links and syntax, below is the updated steps:</p> <pre><code># get the file\nmkdir -p com/ibm/mq/samples/jms &amp;&amp; cd com/ibm/mq/samples/jms\ncurl -o JmsPuGet.java https://raw.githubusercontent.com/ibm-messaging/mq-dev-samples/master/gettingStarted/jms/com/ibm/mq/samples/jms/JmsPutGet.java\ncd ../../../../..\n# Modify the connection setting in the code\n# Compile\njavac -cp com.ibm.mq.allclient-9.2.1.0.jar:javax.jms-api-2.0.1.jar com/ibm/mq/samples/jms/JmsPutGet.java\n# Run it\njava -cp com.ibm.mq.allclient-9.2.1.0.jar:javax.jms-api-2.0.1.jar:. com.ibm.mq.samples.jms.JmsPutGet\n</code></pre> <p>To connect to MQ server, you need to get the hostname for the Queue manager, the port number, the channel and the queue name to access.</p> <p>For Quarkus and Maven use the following dependencies:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;javax.jms&lt;/groupId&gt;\n  &lt;artifactId&gt;javax.jms-api&lt;/artifactId&gt;\n  &lt;version&gt;2.0.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;com.ibm.mq&lt;/groupId&gt;\n  &lt;artifactId&gt;com.ibm.mq.allclient&lt;/artifactId&gt;\n  &lt;version&gt;9.2.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>For automatic client reconnection, use the JMS connection factory configuration.</p>"},{"location":"techno/ibm-mq/#client-channel-definition-tables","title":"Client Channel  Definition Tables","text":"<p>CCDT provides encapsulation and abstraction of connection information for applications, hiding the MQ architecture and configuration from the application. CCDT defines which real queue managers the application will connect to. Which could be a single queue manager or a group of queue managers.</p> <pre><code>  \"channel\": [\n    {\n      \"name\": \"STORE.CHANNEL\",\n      \"clientConnection\": {\n        \"connection\": [\n          {\n            \"host\": \"mq1-cp4i-ibm-mq-qm-cp4i-mq1........com\",\n            \"port\": \"443\"\n          }\n        ],\n        \"queueManager\": \"BIGGSQMGR\"\n      },\n      \"type\": \"clientConnection\"\n    }\n  ]\n}\n</code></pre> Important readings <ul> <li>MQ family page</li> <li>MQ 9.3 product documentation</li> <li>Article for developer from Richard Coppen's: 'IBM MQ fundamentals'</li> <li>MQ on Container</li> <li>Learning path: IBM MQ Developer</li> <li>Developer cheat sheet</li> </ul>"},{"location":"techno/ibm-mq/#code-repositories","title":"Code repositories","text":"<ul> <li>Store simulator - JMS producer to MQ</li> <li>AMQP and reactive messaging</li> </ul>"},{"location":"techno/kafka/","title":"Apache Kafka need to know","text":"<p>Update</p> <p>Created 07/01/2023 - Updated 07/01/2024</p> <p>This content is a summary of the Apache Kafka open-source project, one of the most important event backbone to support EDA. This content does not replace the excellent introduction every developer using Kafka should read, but provides support for analysis, design and implementation discussions.</p>"},{"location":"techno/kafka/#introduction","title":"Introduction","text":"<p>Kafka is a distributed real-time event streaming platform with the following key capabilities:</p> <ul> <li>Publish and subscribe streams of records. Data are stored on disks with the kafka replication protocol. Consumer applications can pull the information when they need, and keep track of what they have seen so far.</li> <li>It can handle hundreds of read and write operations per second from many producers and consumers.</li> <li>Supports Atomic broadcast, sends a record once, and every subscriber gets it once.</li> <li>Replicate stream of data within the distributed cluster for fault-tolerance. Persist data during a given time period before deleting records.</li> <li>Elastic horizontal scaling and transparently for the client applications with no downtime.</li> <li>Until version 3, it is built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available. After version 3 it uses the Kraft  protocol, which integrates metadata management into Kafka itself.</li> </ul> <p>Here is the standard architecture view:</p> <p></p> <ul> <li>Kafka runs as a cluster of broker servers that can, in theory, span multiple availability zones. Each broker manages data replication, topic/partition management, and offset management. To cover multiple availability zones within the same cluster, the network latency needs to be very low, at the 15ms or less, as there is a lot of communication between kafka brokers and between kafka brokers and zookeeper servers. With Kraft the latency still needs to be very low.</li> <li>The Kafka cluster stores streams of records in topics. Topic is referenced by producer application to send data to, and subscribed by consumers to get data from. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be a network based (SAN).</li> </ul> <p>In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorize planned failure and un-planned failure, and when doing replicas, use a replica factor at least equals to three.</p>"},{"location":"techno/kafka/#zookeeper","title":"Zookeeper","text":"<p>Zookeeper is used to persist the component and the platform states. It runs in cluster of at least three nodes to ensure high availability. One zookeeper server is the leader and other are used in backup.</p> <ul> <li>Kafka does not keep state regarding consumers and producers.</li> <li>Depending of the kafka version, offsets are maintained in Zookeeper or in Kafka: newer versions use an internal Kafka topic called <code>__consumer_offsets</code>. In any case, consumers can read next messages (or from a specific offset) correctly even during broker server outrages.</li> <li>Access Controls are saved in Zookeeper.</li> </ul> <p>As of Kafka 2.8+ Zookeeper is becoming optional and the Kraft protocol is used to exchange cluster management metadata.</p>"},{"location":"techno/kafka/#topics","title":"Topics","text":"<p>Topics represent end-points to publish and consume records.</p> <ul> <li>Each record consists of a key, a value (the data payload as byte array), a timestamp and some metadata.</li> <li>Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server.</li> <li>Kafka stores log data in its <code>log.dir</code> and topics map to subdirectories in this log directory.</li> <li>Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups.</li> <li>Consumer performs asynchronous pull to the connected brokers via the subscription to a topic.</li> </ul> <p>The figure below illustrates one topic having multiple partitions, replicated within the broker cluster:</p> <p></p>"},{"location":"techno/kafka/#partitions","title":"Partitions","text":"<p>Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions.</p> <p></p> <ul> <li>Each broker may have zero or more partitions per topic. When creating topic, users specify the number of partition to use.</li> <li>Kafka tolerates up to N-1 server failures without losing any messages. N is the replication factor for a given partition.</li> <li>Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. Topic is a labelled log.</li> <li>Consumers see messages in the order they are stored in the log.</li> <li>Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker.</li> <li>Each partitioned message has a unique sequence id called offset (\"a,b,c,d,e, ..\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are immutable.</li> <li>When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from.</li> <li>Brokers keep offset information in an hidden topic.</li> <li>Partitions guarantee that data with the same keys will be sent to the same consumer and in order.</li> <li>The older records are deleted after a given time period or if the size of log goes over a limit. It is possible to compact the log. The log compaction means, the last known value for each message key is kept. Compacted Topics are used in Streams processing for stateful operator to keep aggregate or grouping by key. You can read more about log compaction from the kafka doc.</li> </ul>"},{"location":"techno/kafka/#replication","title":"Replication","text":"<p>Each partition can be replicated across a number of servers. The replication factor is captured by the number of brokers to be used for replication. To ensure high availability it should be set to at least a value of three. Partitions have one leader and zero or more followers.</p> <p></p> <p>The leader manages all the read and write requests for the partition. The followers replicate the leader content. </p> <p>It is not recommended to get the same number of replicas as the number of brokers. </p> <p>There is a consumer capability, that can be enabled, to consume from a replica, to optimize read latency when consumers are closer to the broker server. Without this configuration, the consumer reads from the partition leader.</p>"},{"location":"techno/kafka/#consumer-app","title":"Consumer App","text":"<p>See dedicated chapter.</p>"},{"location":"techno/kafka/#consumer-group","title":"Consumer group","text":"<p>This is the way to group consumers so the processing of event is parallelized.  The number of consumers in a group is the same as the number of partition defined in a topic.  We are detailing consumer group implementation in this note.</p>"},{"location":"techno/kafka/#kafka-streams","title":"Kafka Streams","text":"<p>An api to develop streaming processing application. See deeper dive, demos and labs in the kafka-studies repository</p>"},{"location":"techno/kafka/#cost-dimensions","title":"Cost dimensions","text":"<p>Sizing a Kafka cluster can be complex, but it\u2019s not always necessary to determine the exact specifications upfront. Adding nodes to the cluster is straightforward, and a single Kafka cluster can scale to hundreds of nodes. However, it\u2019s important to conduct a budget assessment to evaluate the potential costs of transitioning to an event backbone like Kafka. Abaccus used in sizing may be tuned and adapted by your own performance tests.</p> <p>The ingestion rate and data size are the primary factors to consider, but they are not the only ones. Let\u2019s begin by defining the key dimensions. Using the diagram below, architects should evaluate the number of potential event sources that will be ingested through source connectors. For each major inbound flow, we need to assess the number of messages per second and their average size. It may also be helpful to analyze average, 90<sup>th</sup> percentile (p90), and peak values. </p> <p></p> <p>Each message in Kafka is replicated, which means that the disk size on each broker must account for the ingestion rate, the replication factor, and the retention time. With three nodes and a replication factor of three, each node needs sufficient disk space to maintain a steady state for the ingestion rate. All traffic that arrives in each broker from producers and replication traffic from other brokers is eventually persisted to storage volumes attached to the broker.</p> <p>The most common resource bottlenecks for clusters, from an infrastructure perspective, are network throughput, storage throughput, and the network throughput between brokers and the storage backend, particularly for brokers utilizing network-attached storage. Consider increasing the volume throughput if possible or adding more volumes to the cluster.</p> <p>Next, the estimations should consider the number of streaming functions that will consume events from topics and generate new messages for other topics. These new messages will contribute to the inbound metrics. On average, the number of messages processed by these functions may approach the input message count. For example, if there are 1,000 messages per second on the left side of the diagram, each function may read and produce approximately 1,000 messages. Thus, multiplying the input rate by the number of functions provides a good estimate.</p> <p>However, it's important to note that some functions will compute aggregates over time windows, resulting in fewer output messages compared to the input messages. Additionally, modern event-driven microservices may also generate their own events that adds to the ingestion global throughput.</p> <p>Finally some topics will be consumed to send messages to sink, so leaving the universe of Kafka, with one important dimension being the number of consumers to get those messages to.</p> <p>The most common resource bottlenecks for clusters, from an infrastructure perspective, are network throughput, storage throughput, and the network throughput between brokers and the storage backend, particularly for brokers utilizing network-attached storage. Having more brokers is helping to support more throughput. The max throughput of a cluster is represented by the following function:</p> \\[ \\max(T_{cluster}) = \\min( \\max(T_{storage}) * brokers / replicas,                           \\max(T_{SAN network}) * brokers / replicas,                            \\max(T_{network}) * brokers / (consumergroups + replicas - 1)) \\] <p>The more consumer groups there are, the greater the outbound data traffic on the broker's network. When the number of consumers is larger than 1, the broker network traffic is always higher than the traffic into the broker. We can therefore ignore data traffic into brokers as a potential bottleneck. Increasing the number of consumer groups reading from a cluster decreases its sustained throughput limit</p> <p>The number of brokers should account for maintenance, scaling, potential broker loss, or even the failure of an entire availability zone. When consumers fall behind or need to reprocess historical data, the requested data may no longer reside in memory, necessitating that brokers fetch it from the storage volume. This results in non-sequential I/O reads. When using Storage Area Network volumes, it also generates additional network traffic to the volume. Utilizing larger brokers with more memory or enabling compression can help mitigate this effect.</p> <p>For sizing, it is assumed that producers load balance messages evently among brokers, there are enough partitions to support ingress throughput and consumers are reading from the last topic.</p> <p>For disk attachment to broker, there are two choices: AN or local disk. With SAN the throughput is linked to the storage network throughput. While for local disk is it based on IOPS. It is important to note that with attached disk, that changing the broker means loosing disk, and the broker needs to reload from replicas from other brokers. With mounted disk, the access to the stored append log is quicker. With SAN, the throughput characteristics of the different volume types range between 128 MB/sec and 4000 MB/sec. Some cloud providers are also controlling the network bandwidth by VM type.</p> <p>For production keeps the workload below 80% of the max sustained throughput. So when the SAN network is limited to 250 MB/s the throughput limit is: $$ 80\\% * 250 = 200 MB/sec$$</p> <p>When a consumer falls behind or needs to recover from failure it reprocesses older messages. In that case, the pages holding the data may no longer reside in the page cache, and brokers need to fetch the data from the SAN volume. That causes additional network traffic to the volume and non-sequential I/O reads. This can substantially impact the throughput of the SAN volume.</p> <p>Amazon MSK recently has provision storage capacity to increase the throughput up to 1000 MB/sec. With self managed cluster on EC2, solution architects need to consider gp3. io2 or st1 volume types. But in this case, the VM network may become the bottleneck.</p> <p>Finally encryption is CPU intensive, the brokers need to encrypt and decrypt individual messages.</p>"},{"location":"techno/kafka/#cloud-provider-specifics","title":"Cloud provider specifics","text":"<p>Cloud deployment brings its own challenges and costs: as seen in previous section, type of machine impacts IOPS and network throughput, and the number of brokers helps supporting higher throughput limit. Amazon MSK enforces that brokers are evenly balanced across all configured Availability Zones. So adding broker is always by a factor of # of AZs. Having too many small brokers cost more during maintenance operations.</p> <p>Also having too big machine increases the blast radius in case of failure. </p> <p>MSK clusters have a sustained throughput limit and can burst to a higher throughput for short periods. The cluster can then instantaneously absorb much higher throughput for some time.</p> <p>In the cloud, application placement will have an impact on throughput and also on the cost of the overall solution. In the figure below, the deployment uses a typical hybrid architecture, where applications are still running on-premises, while new cloud native apps are deployed on the cloud, container orchetration platform. </p> <p></p> <p>Most of cloud providers do not charge inbound traffic, but charge outbound and cross- availability zones traffic. These are the red arrows in the figure. </p> <p>So the number of brokers, the number of replicas, the number of consumers and local producers will impact the overall cost. Look at the cost per GB/sec for VPC to VPC, outbound to on-premises servers.</p>"},{"location":"techno/kafka/#additional-readings","title":"Additional readings","text":""},{"location":"techno/kafka/#microservices-and-event-driven-patterns","title":"Microservices and event-driven patterns","text":"<ul> <li>API for declaring messaging handlers using Reactive Streams</li> <li>Microservice patterns - Chris Richardson</li> <li>Develop Stream Application using Kafka</li> </ul>"},{"location":"techno/kafka/#kafka","title":"Kafka","text":"<ul> <li>Start by reading Kafka introduction - a must read!</li> <li>Another introduction from Confluent, one of the main contributors of the open source.</li> <li>Using Kafka Connect to connect to enterprise MQ systems - Andrew Schofield</li> <li>Does Apache Kafka do ACID transactions? - Andrew Schofield</li> <li>Spark and Kafka with direct stream, and persistence considerations and best practices</li> <li>Example in scala for processing Tweets with Kafka Streams</li> </ul>"},{"location":"techno/kafka/#conferences-talks-and-sessions","title":"Conferences, Talks, and Sessions","text":"<ul> <li>Kafka Summit 2016 - San Francisco</li> <li>Kafka Summit 2017 - New York</li> <li>Kafka Summit 2017 - San Francisco</li> <li>Kafka Summit 2018 - San Francisco</li> <li>Kafka Summit 2019 - San Francisco</li> <li>Kafka Summit 2019 - London</li> <li>Kafka Summit 2020 - Virtual</li> <li>Kafka Summit 2022 - Recap</li> </ul>"},{"location":"techno/kafka/consumer/","title":"Understanding Kafka Consumers","text":"<p>This chapter includes Kafka consumer technology summary and best practices. It may be useful for kafka developer beginner or for seasoned developers who  want a refresh after some time far away from Kafka...</p>"},{"location":"techno/kafka/consumer/#consumer-group","title":"Consumer group","text":"<p>Consumers belong to consumer groups. </p> <p></p> <p>We specify the group name as part of the consumer connection parameters using the <code>group.id</code> configuration:</p> <pre><code>  properties.put(ConsumerConfig.GROUP_ID_CONFIG,  groupid);\n</code></pre> <p>Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. </p> <p>The figure below represents 2 consumer groups. Group 1 has more consumers than partitions, each consumer will get from one partition, and one consumer will get no record. In second group the Consumer B is getting data from 2 partitions, while consumer 1 is getting from one partition.</p> <p></p> <p>When a consumer is unique in a group, it will get data from all partitions. There is always at least one consumer per partition.</p> <p>One broker is responsible to be the consumer group coordinator which is responsible for assigning partitions to the consumers in the group. </p> <p>The first consumer to join the group will be the group coordinator. It will get the list of consumers and it is responsible for assigning a subset of partitions to each consumer.</p> <p>Membership in a consumer group is maintained dynamically. Consumers send heartbeats to the group coordinator broker (see configuration parameters heartbeat.interval.ms) and <code>session.timeout.ms</code>. </p> <p>Partition assignement is done by different strategies from range, round robin, sticky and cooperative sticky (See partition assignement strategy documentation). </p> <p>When a consumer fails, the partitions assigned to it, will be reassigned to an other consumer in the same group. When a new consumer joins the group, partitions will be moved from existing consumers to the new one. Group rebalancing is also used when new partitions are added to one  of the subscribed topics. The group will automatically detect the new partitions through periodic metadata refreshes and assign them to members  of the group. During a rebalance, depending of the strategy, consumers may not consume messages (Need Kafka 2.4+ to get cooperative balancing feature). </p> <p>Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. </p> <p>The consumer can take time to process records, so to avoid the consumer group controler removing consumer taking too much time, it is possible to tune the max.poll.interval.ms consumer property. If <code>poll()</code> is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member. </p> <p>The second mechanism is the heartbeat the consumers send to the group cordinator to show they are alive. The session.timeout.ms specifies the max value to consider before removing a non responding consumer. </p> <p>Implementing a Topic consumer is using the kafka KafkaConsumer class  which the API documentation is a must read. It is interesting to note that:</p> <ul> <li>To support the same semantic of a queue processing like other integration messaging systems, we need to have all the consumers assigned to a single consumer group,  so that each record delivery would be balanced over the group like with a queue.</li> <li>To support pub/sub like other messaging systems, each consumer would have its own consumer group, and subscribes to all the records published to the topic/partition.</li> <li>With <code>client.rack</code> parameter, a consumer may consume from a local replica, which will improve  latency when using a stretched cluster over multiple availability zones.</li> </ul> <p>For a single thread consumer, the implementation code follow the following pattern:</p> <ul> <li>prepare the consumer properties</li> <li>create an instance of KafkaConsumer to subscribe to at least one topic</li> <li>loop on polling events: the consumer ensures its liveness with the broker via the poll API. It will get <code>n records</code> per poll.</li> <li>process the ConsumerRecords and commit the offset by code or by using the autocommit attribute of the consumer.</li> </ul> <p>As long as the consumer continues to call <code>poll()</code>, it will stay in the group and continues to receive messages from the partitions it was assigned to.  When the consumer does not send heartbeats for a duration of <code>session.timeout.ms</code>, then it is considered unresponsive and its partitions will be reassigned.</p> <p>Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder.</p> <p>We are proposing a deeper dive study for the manual offset commit in this consumer code.</p> <p>But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer.</p>"},{"location":"techno/kafka/consumer/#assess-number-of-consumers-needed","title":"Assess number of consumers needed","text":"<p>The KafkaConsumer is not thread safe, therefore it is recommended to run it in a unique thread. With a multi-threads solution, each thread will open a TCP connection to the Kafka brokers, be sure to close the connection to avoid memory leak.  The alternate is to start n processes (JVM process) with a mono thread.</p> <p>The consumer-per-partition pattern maximizes throughput. When consumers run in parallel with a multiple threads per consumer implementation, it is very important to consider the total number of threads across all consumer instances, to do not exceed the total number of partitions in the topic.</p> <p>Also, a consumer can subscribe to multiple topics. </p>"},{"location":"techno/kafka/consumer/#offset-management","title":"Offset management","text":"<p>Recall that offset is just a numeric identifier of a record within a partition. It is used to keep the last record read by a consumer within a partition. Consumers periodically need to  commit the offsets they have received, to present a recovery point in case of failure. To commit offset (via API or automatically) the consumer  sends a message to kafka broker to the special topic named <code>__consumer_offsets</code> to keep the committed offset for each partition. (When there is a committed offset, the <code>auto.offset.reset</code> property is not used)</p> <p>Consumers do a read commit for the last processed record: </p> <p></p> <p>When a consumer starts, it receives a partition to consume, and it starts at its group's committed offset or the latest or earliest offset  as specified in the auto.offset.reset property.</p> <p>If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. </p> <p></p> <p>This means that the message will be processed again by the next consumer, in that group, which received the assigned partition.</p> <p>In the case where consumers are set to auto commit, it means the offset is committed at the <code>poll()</code> function and if the application crashed while processing this record: </p> <p></p> <p>then the record (partition 0 - offset 4) will never be processed. BThe record is still in the partition, on the broker file system, and so it is not lost.</p> <p>As shown in the figure below, in case of consumer failure, it is possible to get duplicates. When the last message processed by the consumer,  before crashing, is younger than the last committed offset, the consumer will get this record again. This case may happen when using a time based commit strategy.</p> <p></p> <p>Source: Kafka definitive guide book from Todd Palino, Gwen Shapira</p> <p>In the opposite, if the last committed offset is after the last processed messages and there were multiple messages returned in the poll(),  then those messages may be never processed by those consumers. This will happen with autocommit set up at the time  of the read operation, and the last offset of the poll is the committed offset. See the enable.auto.commit property.</p> <p></p> <p>Limiting to poll one message at the time, will help to avoid this problem, but will impact throughput.</p> <p>It is possible to commit by calling API, so developer can control when to commit the read. For manual commit, we can use one of the two approaches:</p> <ul> <li>offsets\u2014synchronous commit: send the offset number for the records read using <code>consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1)))</code> method </li> <li>asynchronous commit.</li> </ul> <p>As soon as you are coding manual commit, it is strongly recommended to implement the ConsumerRebalanceListener interface to be able to do state  modifications when the topic is rebalanced. The consumer is keeping state of its offset, so it becomes stateful.</p> <p>Assess if it is acceptable to loose messages from topic.  If so, when a consumer restarts, it will start consuming the topic from the latest committed offset within the partition allocated to itself.</p> <p>As storing a message to an external system and storing the offsets are two separate operations, and in case of failure between them,  it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. </p> <p>In this case, consumer's idempotency is needed. Different techniques can be used to support idempotentcy, like unique identifier, timestamp and time window, ....</p> <p>As presented in the producer coding practice, using transaction to support \"exactly-once\", also means the consumers should read committed data only.  This can be achieved by setting the <code>isolation.level=read_committed</code> in the consumer's configuration. The last offset will be the first message  in the partition belonging to an open not yet committed transaction. This offset is known as the 'Last Stable Offset'(LSO).</p>"},{"location":"techno/kafka/consumer/#producer-transaction","title":"Producer transaction","text":"<p>When consuming from a Kafka topic and producing to another topic, like in Kafka Streams programming approach, we can use the producer's transaction  feature to send the committed offset message and the new records in the second topic in the same transaction.  This can be seen as a  <code>consume-transform-produce</code> loop pattern so that every input event is processed exactly once. </p> <p>An example of such pattern in done in the order management microservice - command part.</p>"},{"location":"techno/kafka/consumer/#consumer-lag","title":"Consumer lag","text":"<p>The consumer lag, within a partition, is the difference between the offset of the most recently published message and the consumer's committed offset.</p> <p>If the lag starts to grow, it means the consumer is not able to keep up with the producer's pace.</p> <p>The risk, is that slow consumer may fall behind, and when partition management may remove old log segments, leading the consumer to jump forward to continue on the next log segment. Consumer may have lost messages.</p> <p></p> <p>You can use the kafka-consumer-groups tool to see and manage the consumer lag.</p>"},{"location":"techno/kafka/consumer/#reset-a-group","title":"Reset a group","text":"<p>Sometime, it is needed to reprocess the messages. The easiest way is to change the groupid of the consumers to get an implicit offsets reset, but it is also possible to reset for some topic to the earliest offset:</p> <pre><code>kafka-consumer-groups \\\n                    --bootstrap-server kafkahost:9092 \\\n                    --group ordercmd-command-consumer-grp \\\n                    --reset-offsets \\\n                    --all-topics \\\n                    --to-earliest \\\n                    --execute\n</code></pre>"},{"location":"techno/kafka/consumer/#kafka-useful-consumer-apis","title":"Kafka useful Consumer APIs","text":"<p>KafkaConsumer a topic consumer which support:</p> <ul> <li>transparently handles brokers failure</li> <li>transparently adapt to partition migration within the cluster</li> <li>support grouping for load balancing among consumers</li> <li>maintains TCP connections to the necessary brokers to fetch data</li> <li>subscribe to multiple topics and being part of consumer groups</li> <li>each partition is assigned to exactly one consumer in the group</li> <li>if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group</li> </ul> <p>ConsumerRecords holds the list ConsumerRecord per partition for a particular topic.</p> <p>ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp</p>"},{"location":"techno/kafka/consumer/#repositories-with-consumer-code","title":"Repositories with consumer code","text":"<ul> <li>Within the Reefer ontainer shipment solution we have a order events consumer: order event agent</li> <li>Quarkus app with Kafka streams</li> <li>A lot of python consumer codes in the integration tests, with or without Avro schema</li> </ul>"},{"location":"techno/kafka/consumer/#references","title":"References","text":"<ul> <li>KafkaConsumer class</li> </ul>"},{"location":"techno/kafka/faq/","title":"Frequently asked questions","text":""},{"location":"techno/kafka/faq/#basic-questions","title":"Basic questions","text":"What is Kafka? <ul> <li>Pub/sub middleware to share data between applications</li> <li>Open source, started in 2011 by Linkedin</li> <li>Based on append log to persist immutable records ordered by arrival.</li> <li>Support data partitioning, distributed brokers, horizontal scaling, low-latency and high throughput.</li> <li>Producer has no knowledge of consumers</li> <li>Records stay even after being consumed</li> <li>Durability with replication to avoid loosing data for high availability</li> </ul> What are the Kafka major components? <ul> <li>Topic, consumer, producer, brokers, cluster see this note for deep dive</li> <li>Rich API to control the producer semantic, and consumer</li> <li>Consumer groups. See this note for detail</li> <li>Kafka streams API to support data streaming with stateful operations and stream processing topology</li> <li>Kafka connect for source and sink connection to external systems</li> <li>Topic replication with Mirror Maker 2</li> </ul> What are major use cases? <ul> <li>Modern data pipeline with buffering to data lake</li> <li>Data hub, to continuously expose business entities to event-driven applications and microservices</li> <li>Real time analytics with aggregate computation, and complex event processing</li> <li>The communication layer for Event-driven, reactive microservice.</li> </ul> Why does Kafka use zookeeper? <p>Kafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains Kafka cluster integrity, select broker leader... </p> <p>Zookeeper is also used to manage offset commit, and to the leader selection process.</p> <p>Version 2.8 introduces another algorithm to define partition leadership and cluster health via one broker becoming the cluster controller. See this note on KIP 500</p> What is a replica? <p>Nodes responsible to participate into the data replication process for a given partition. It is a critical feature to ensure durability, be able to continue to consume records, or to ensure a certain level of data loss safety is guaranteed when producing records.</p> What are a leader and follower in Kafka? <p>Topic has 1 to many partition, which are append logs. Every partition in Kafka has a server that plays the role of leader . Leader receives read and write operation, and ensure acknowledge management. When replication is set in a topic, follower brokers will pull data from the leader to ensure replication, up to the specified replication factor. If the leader fails, one of the followers needs to take over as the leader\u2019s role. The leader election process involves zookeeper and assesses which follower was the most in-synch with the leader. </p> <p>To get the list of In-synch Replication for a given topic the following tool can be used:</p> <pre><code>kafka-topics.sh --bootstrap-server :9092 --describe --topic &lt;topicname&gt;\n</code></pre> What is Offset? <p>A unique identifier of records inside a partition. It is automatically created by the broker, and producer can get it from the broker response. Consumer uses it to commit its read. It means, in case of consumer restarts, it will read from the last committed offset.</p> What is a consumer group? <p>It groups consumers of one to many topics. Each partition is consumed by exactly one consumer within each subscribing consumer group.</p> <p>Consumer group is specified via the <code>group.id</code> consumer's property, and when consumers subscribe to topic(s).</p> <p>There is a protocol to manage consumers within a group so that partition can be reallocated when a consumer lefts the group. The group leader is responsible to do the partition assignment.</p> <p>When using the group.instance.id properties, consumer is treated as a static member, which means there will be no partition re-balance when consumer lefts a group for a short time period. When not set, the group coordinator (a broker) will allocate <code>ids</code> to group members, and reallocation will occur. For Kafka Streams application, it is recommended to use static membership.</p> <p>Brokers keep offsets until a retention period within which consumer group can lose all its consumers. After that period, offsets are discarded. The consumer group can be deleted manually, or automatically when the last committed offset for that group expires.</p> <p>When the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __consumer_offsets. Ack from the broker is done once all replicas on this hidden topics are successful.</p> <p>The tool <code>kafka-consumer-group.sh</code> helps getting details of consumer group:</p> <pre><code># Inside a Kafka broker container\nbin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose\n</code></pre> How to support multi-tenancy? <p>Multi-tenant means multiple different groups of application can produce and consume messages isolated from each other. So by constructs, topics and brokers are multi-tenant. Now the control will be at the access control level policy, the use of service account, and naming convention on the topic name. Consumer and producer authenticate themselves using dedicated service account users, with SCRAM user or Mutual TLS user. Each topic can have security policy to control read, write, and creation operations.</p> How client access Kafka cluster metadata? <p>Provide a list of Kafka brokers, minimum two, so the client API will get the metadata once connected to one of the broker.</p> How to get at most once delivery? <p>Set producer acknowledge level (acks) property to 0 or 1.</p> How to support exactly once delivery? <p>The goal is to address that, if a producer sends a message twice the system will keep only one message to the consumer, and once the consumer commits the read offset, it will not receive the message again even if it restarts.</p> <p>See the section in the producer implementation considerations note.</p> <p>The consumer needs to always read from its last committed offset.</p> <p>Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: <code>processing.guarantee=exactly_once</code>. Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once.</p> <p>Exactly-once delivery for other destination systems generally requires cooperation with such systems which may be possible by using the offset processing.</p> Retention time for topic what does it mean? <p>The message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: <code>retention.ms</code> and <code>retention.bytes</code>. Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to <code>cleanup.policy</code> topic's parameter. See the Kafka documentation on topic configuration parameters.</p> <p>Here is a command to create a topic with specific retention properties:</p> <pre><code>bin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config  retention.ms=55000 --add-config  retention.byte=100000\n</code></pre> <p>But there is also the <code>offsets.retention.minutes</code> property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers do not run continuously. So consumers need to commit their offset. If the consumer settings define: <code>auto.offset.reset=earliest</code>, the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to <code>latest</code>). When using <code>latest</code>, if the consumers are offline for more than the offsets retention time window, they will lose events.</p> What are the topic characteristics I need to define during requirements? <p>This is a requirement gathering related question to understand what need to be done for topic configuration but also consumer and producer configurations, as well as retention strategy.</p> <ul> <li>Number of brokers in the cluster.</li> <li>Retention time and size</li> <li>Need for HA, set replicas to number of broker or at least the value of 3, with in-synch replica to 2</li> <li>Type of data to transport to assess message size</li> <li>Plan to use schema management to control change to the payload definition</li> <li>Volume per day with peak and average</li> <li>Need to do geo replication to other Kafka cluster</li> <li>Network filesystem used on the target Kubernetes cluster and current storage class</li> </ul> What are the impacts of having not enough resource for Kafka? <p>When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things to consider when diagnosing kafka broker issues.</p>"},{"location":"techno/kafka/faq/#security-questions","title":"Security questions","text":"<p>For deeper dive on security administration see Confluent article and Apache Kafka product documentation.</p> What Security support in Kafka? <ul> <li>Encrypt data in transit between producer and Kafka brokers</li> <li>Client authentication</li> <li>Client authorization</li> </ul> Explain security protocol <p>security.protocol defines the protocol used to communicate with brokers. Verify how the listeners are configured in the Kafka cluster. The valid values are:</p> <pre><code>PLAINTEXT (using PLAINTEXT transport layer &amp; no authentication - default value).\nSSL (using SSL transport layer &amp; certificate-based authentication)\nSASL_PLAINTEXT (using PLAINTEXT transport layer &amp; SASL-based authentication)\nSASL_SSL (using SSL transport layer &amp; SASL-based authentication)\n</code></pre> <p>In Strimzi the following yaml defines the different listeners type and port: <code>tls</code> boolean is for the traffic encryption, while <code>authentication.type</code> defines the matching security protocol.</p> <pre><code>listeners:\n    - name: plain\n        port: 9092\n        type: internal\n        tls: false\n    - name: tls\n        port: 9093\n        type: internal\n        tls: true\n        authentication:\n        type: tls\n    - name: external\n        type: route\n        port: 9094\n        tls: true \n        authentication:\n        type: scram-sha-512\n</code></pre> What are <code>ssl.truststore.location</code> and <code>ssl.truststore.password</code>? <p>When doing TLS encryption, we need to provide the Kafka clients with the location of a trusted Certificate Authority-based certificate. This file is often provided by the Kafka administrator and is generally unique to the specific Kafka cluster deployment. The certificate is in JKS format for JVM languages and PEM/ P12 for nodejs or Python.</p> <p>To extract a PEM-based certificate from a JKS-based truststore, we can use the following command: </p> <pre><code>keytool -exportcert -keypass {truststore-password} -keystore {provided-kafka-truststore.jks} -rfc -file {desired-kafka-cert-output.pem}\n</code></pre> What are the security configuration to consider? <p>On Kubernetes, Kafka can be configured with external and internal URLs. With Strimzi internal URLs are using TLS or Plain authentication, then TLS for encryption. If no authentication property is specified then the listener does not authenticate clients which connect through that listener. The listener will accept all connections without authentication.</p> <ul> <li>Mutual TLS authentication for internal communication looks like:</li> </ul> <pre><code>- name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n    type: tls\n</code></pre> <p>To connect any app (producer, consumer) we need a TLS user like:</p> <pre><code>piVersion: kafka.strimzi.io/v1beta2\nkind: KafkaUser\nmetadata:\nname: tls-user\nlabels:\n    strimzi.io/cluster: vaccine-kafka\nspec:\nauthentication:\n    type: tls\n</code></pre> <p>Then the following configurations need to be done for each app. For example in Quarkus app, we need to specify where to find the client certificate (for each Kafka TLS user a secret is created with the certificate (ca.crt) and a user password)</p> <pre><code>oc describe secret tls-user\nData\n====\nca.crt:         1164 bytes\nuser.crt:       1009 bytes\nuser.key:       1704 bytes\nuser.p12:       2374 bytes\nuser.password:  12 bytes\n</code></pre> <p>For Java client we need the following security settings, to specify from which secret to get the keystore password and certificate. The certificate will be mounted to <code>/deployments/certs/user</code>. </p> <pre><code>%prod.kafka.security.protocol=SSL\n%prod.kafka.ssl.keystore.location=/deployments/certs/user/user.p12\n%prod.kafka.ssl.keystore.type=PKCS12\n</code></pre> <p>For the server side certificate, it will be in a truststore, which is mounted to  <code>/deployments/certs/server</code> and from a secret (this secret is created at the cluster level).</p> <p>Also because we also use TLS for encryption we need:</p> <pre><code>%prod.kafka.ssl.protocol=TLSv1.2\n</code></pre> <p>Mutual TLS authentication is always used for the communication between Kafka brokers and ZooKeeper pods. For mutual, or two-way, authentication, both the server and the client present certificates.</p> <ul> <li>The sasl.mechanism property is for defining the authentication protocol used. Possible values are:</li> </ul> <pre><code>PLAIN (cleartext passwords, although they will be encrypted across the wire per security.protocol settings above)\nSCRAM-SHA-512 (modern Salted Challenge Response Authentication Mechanism)\nGSSAPI (Kerberos-supported authentication and the default if not specified otherwise)\n</code></pre> <ul> <li> <p>SCRAM: (Salted Challenge Response Authentication Mechanism) is an authentication protocol that can establish mutual authentication using passwords. Strimzi can configure Kafka to use SASL (Simple Authentication and Security Layer) SCRAM-SHA-512 to provide authentication on both unencrypted and encrypted client connections.</p> <ul> <li>The listener declaration:</li> </ul> <pre><code>- name: external\nport: 9094\ntype: route\ntls: true\nauthentication:\ntype: scram-sha-512\n</code></pre> <ul> <li>Need a scram-user:</li> </ul> <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaUser\nmetadata:\nname: scram-user\nlabels:\n    strimzi.io/cluster: vaccine-kafka\nspec:\nauthentication:\n    type: scram-sha-512\n</code></pre> </li> </ul> <p>Then the app properties need to have:</p> <pre><code>security.protocol=SASL_SSL\n%prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n%prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\n%prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.from-secret=${KAFKA_USER:scram-user}\n%prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.with-key=password\n%prod.quarkus.openshift.mounts.kafka-cert.path=/deployments/certs/server\n%prod.quarkus.openshift.secret-volumes.kafka-cert.secret-name=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n</code></pre> What are the setting for <code>sasl.jaas.config</code>? <pre><code>sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\nsasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\n</code></pre> How internal and external broker listener work? <p>See this article to understand the listener configuration. Here is a config to be used in docker container:</p> <pre><code>KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092\nKAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT\nKAFKA_LISTENERS: EXTERNAL://0.0.0.0:9092,INTERNAL://kafka:29092\nKAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\nKAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\n</code></pre> <p>For external connection to Strimzi cluster use the following, where USERNAME is a scram-user</p> <pre><code>bootstrap.servers={kafka-cluster-name}-kafka-bootstrap-{namespace}.{kubernetes-cluster-fully-qualified-domain-name}:443\nsecurity.protocol=SASL_SSL\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\nssl.truststore.location={/provided/to/you/by/the/kafka/administrator}\nssl.truststore.password={__provided_to_you_by_the_kafka_administrator__}\n</code></pre> <p>To get the user password get the user secret (oc or kubectl CLIs):</p> <pre><code>oc get secret scram-user -o jsonpath='{.data.admin_password}' | base64 --decode &amp;&amp; echo \"\"\n</code></pre> <p>To get the Bootstrap URL use: </p> <pre><code>expost K_CLUSTER_NAME=mycluster\nexport BOOTSTRAP=\"$(oc get route ${K_CLUSTER_NAME}-kafka-bootstrap -o jsonpath='{.spec.host}'):443\"\n</code></pre> <p>The <code>sasl.jaas.config</code> can come from an environment variable inside of a secret, but in fact it is already predefined in the scram user in Strimzi:</p> <pre><code>oc get secret my-user -o json | jq -r '.data[\"sasl.jaas.config\"]' | base64 -d -\n</code></pre> <ul> <li>For internal communication, with PLAIN the setting is:</li> </ul> <pre><code>bootstrap.servers={kafka-cluster-name}-kafka-bootstrap.{namespace}.svc.cluster.local:9093\nsecurity.protocol = SASL_PLAINTEXT (these clients do not require SSL-based encryption as they are local to the cluster)\nsasl.mechanism = PLAIN\nsasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\n</code></pre> <ul> <li>For internal authentication with mutual TLS the settings:</li> </ul> <pre><code>security.protocol=SSL \n</code></pre> App running not in same namespace as Kafka <p>Remember that if the application does not run in the same namespace as the kafka cluster then copy the secrets to the namespace with something like:</p> <pre><code>if [[ -z $(oc get secret ${TLS_USER} 2&gt; /dev/null) ]]\nthen\n# As the project is personal to the user, we can keep a generic name for the secret\noc get secret ${TLS_USER} -n ${KAFKA_NS} -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n\nif [[ -z $(oc get secret ${SCRAM_USER} 2&gt; /dev/null) ]]\nthen\n    # As the project is personal to the user, we can keep a generic name for the secret\n    oc get secret ${SCRAM_USER} -n ${KAFKA_NS} -o json |  jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n</code></pre> How to protect data at rest? <ul> <li>Use encrypted file system for each brokers</li> <li>Encrypt data at the producer level, using some API, and then decode at the consumer level. The data in the append log will be encrypted.</li> </ul>"},{"location":"techno/kafka/faq/#more-advanced-concepts","title":"More advanced concepts","text":"What is range partition assignment strategy? <p>There are multiple partition assignment strategies for a consumer, part of a consumer group, to get its partition to fetch data from. Members of the consumer group subscribe to the topics they are interested in and forward their subscriptions to a Kafka broker serving as the group coordinator. The coordinator selects one member to perform the group assignment and propagates the subscriptions of all members to it. Then assign(Cluster, GroupSubscription) is called to perform the assignment and the results are forwarded back to each respective members.</p> <p>Range assignor works on a per-topic basis: it lays out the available partitions in numeric order and the consumers in lexicographic order, and assign partition to each consumer so partition with the same id will be in the same consumer: topic-1-part-0 and topic-2-part-0 will be processed by consumer-0</p> What is sticky assignor? <p>The CooperativeStickyAssignor helps supporting incremental cooperative re-balancing to the clients' group protocol, which allows consumers to keep all of their assigned partitions during a re-balance, and at the end, revoke only those which must be migrated to another consumer for overall cluster balance.</p> <p>The goal is to reduce unnecessary downtime due to unnecessary partition migration, by leveraging the sticky assignor which link consumer to partition id. See KIP 429 for details. </p> How to get an homogeneous distribution of message to partitions? <p>Design the message key and hash coding for even distributed. Or implement a customer 'partitioner' by implementing the Partitioner interface. </p> How to ensure efficient join between two topics? <p>Need to use co-partitioning, which means having the same key in both topic, the same number of partitions and the same producer partitioner, which most likely should be the default one that uses the following formula: partition = hash(key) % numPartitions.</p> What is transaction in Kafka? <p>Producer can use transaction begin, commit and rollback APIs while publishing events to a multi-partition topic. This is done by setting a unique transactionId as part of its configuration (with idempotence and min inflight record set to 1).  Either all messages are successfully written or none of them are. There are some producer exception to consider to abort the transaction: any KafkaException for sure, but also OutOfSequenceTx which may happen when the PID is greater than the last one seen by the producer.</p> <p>See explanations here.</p> <p>And the KIP 98</p> What is the high watermark? <p>The high watermark offset is the offset of the last message that was successfully copied to all of the log\u2019s replicas. A consumer can only read up to the high watermark offset to prevent reading un-replicated messages.</p> What should we do for full queue exception or timeout exception on the producer side? <p>The brokers are running behind, so we need to add more brokers and redistribute partitions.</p> How to send large messages? <p>We can set some properties at the broker, topic, consumer and producer level:</p> <ul> <li>Broker: consider the message.max.bytes and replica.fetch.max.bytes</li> <li>Consumer: max.partition.fetch.bytes. Records are fetched in batches by the consumer, so this properties gives the max amount of data per partition the server will return. Default 1 Megabyte</li> </ul> <p>Always considering the claim-check pattern, by writing the big file in a cloud object storage and then keep the unique URI in the event payload.</p> How to maximize throughput? <p>For producer if you want to maximize throughput over low latency, set batch.size and linger.ms to higher value. Linger delay producer, it will wait for up to the given delay to allow other records to be sent so that the sends can be batched together.</p> Why Kafka Stream applications may impact cluster performance? <ul> <li>They may use internal hidden topics to persist their states for Ktable and GlobalKTable.</li> <li>Process input and output topics</li> </ul> How message schema version is propagated? <p>The record includes a byte with the version number from the schema registry.</p> Consumers do not see message in topic, what happens? <p>The brokers may have an issue on this partition. If a broker, part of the ISR list fails, then new leader election may delay the broker commit operation initiated by a producer. </p> <p>The consumer has a communication issue, or fails, so the consumer group re-balance is underway.</p> How, the compression schema used, is known by the consumer? <p>The record header includes such metadata. So it is possible to have different compression schema per record.</p> What does out-of-synch partition mean and when it occurs? <p>With partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write operation to be considered successful. The record is considered \u201ccommitted\u201d when all ISRs for a partition wrote to their log. Only committed records are readable from consumer.</p> <p>Therefore <code>out-of-synch</code> will happen if the followers are not able to send their acknowledge to the replica leader as quickly as expected.</p>"},{"location":"techno/kafka/faq/#kafka-connect","title":"Kafka Connect","text":"How to remove personal identifying information? <p>From the source connector in Kafka Connect, it is possible to add processing class to process the records before publishing them to Kafka topic, so that any Kafka Streams apps will not see PII.</p> How to handle variable workload with Kafka Connector source connector? <p>Increase and decrease the number of Kafka connect workers based upon current application load.</p>"},{"location":"techno/kafka/faq/#derived-products-related-questions","title":"Derived products related questions","text":"What are the competitors to Kafka? <ul> <li>NATS</li> <li>Redpanda a Modern streaming platform for mission critical workloads, and is compatible with Kafka API. It is a cluster of brokers without any zookeepers. It also leverage the SSD technology to improve I/O operations.</li> <li> <p>AWS Kinesis</p> <ul> <li>Cloud service, managed by AWS staff, paid as you go, proportional to the shard (like partition) used.</li> <li>24h to 7 days persistence</li> <li>Number of shards are adaptable with throughput.</li> <li>Uses the concept of Kinesis data streams, which uses shards: data records are composed of a sequence number, a partition key and a data blob.</li> <li>restrictions on message size (1 MB) and consumption rate of messages (5 reads /s, &lt; 2MB per shard, 1000 write /s)</li> <li>Server side encryption using master key managed by AWS KMS</li> </ul> </li> <li> <p>GCP Pub/sub</p> </li> <li>Solace</li> <li> <p>Active MQ:</p> <ul> <li>Java based messaging server to be the JMS reference implementation, so it supports transactional messaging. </li> <li>various messaging protocols including AMQP, STOMP, and MQTT</li> <li>It maintains the delivery state of every message resulting in lower throughput.</li> <li>Can apply JMS message selector to consumer specific message</li> <li>Point to point or pub/sub, but servers push messages to consumer/subscribers</li> <li>Performance of both queue and topic degrades as the number of consumers rises</li> </ul> </li> <li> <p>Rabbit MQ:</p> <ul> <li>Support queues, with messages removed once consumed</li> <li>Add the concept of Exchange to route message to queues</li> <li>Limited throughput, but can send large message</li> <li>Support JMS, AMQP protocols, and participation to transaction</li> <li>Smart broker / dumb consumer model that focuses on consistently delivering messages to consumers.</li> </ul> </li> </ul> Differences between AMQ Streams and Confluent <p>AMQ Streams and Confluent are based on the open source Kafka, but Confluent as the main contributor to Kafka, is adding proprietary features to make the product more market-able,  so we will not do a pure features comparison but a generic features comparison:</p> Feature Confluent AMQ Streams Kafka open source Aligned within a month to the Kafka release Within 2 months after Kafka release Kafka API Same Same k8s / OpenShift deployment Helm \"operator\" Real Kubernetes Operator based on open source Strimzi Kafka Connectors Connectors hub to reference any connectors on the market, with some certified for Confluent. Open source connectors supported. Apache Camel offers a set of connectors not directly supported by Red Hat but useful in a BYO connectors. Fuse and Debezium can be used. Schema registry Proprietary API and schema Solution may leverage open source Apicur.io schema registry which is compatible with Confluent API. Cruise control for auto cluster balancing Adds on Available via Operator Mirroring between clusters Replicator tool Mirror Maker 2 deployable and managed by Strimzi operator Multi region cluster Supported Supported Role Based access control Supported Supported with explicit user manifest, integrated with Red Hat SSO and OPA. ksql Open sourced licensed by Confluent Customer can use open source version of kSQL but meed to verify licensing for cloud deployment. SQL processing on Kafka Records may also being done with Apache Flink Kafka Streams Supported from Kafka Open Source Supported from Kafka Open Source. Also moving CEP and Streams processing to an external tool makes a lot more sense. Apache Flink should be considered. Not directly supported by Red Hat Storage NFS and tiered storage Block storage with replication to s3 buckets for long persisence using Kafka connector. S3 Connector is not supported by Red Hat. As a managed service Proprietary solution Same with: IBM Event Streams and Red Hat AMQ streams as a service Integration with IBM mainframe Not strategic - Weak Strong with IBM connector and deployment on Z and P Admin User Interface Control center Operator in OpenShift and 3nd party open-source user interface like Kafdrop, Kowl, work with AMQ Streams but without direct support from Red Hat <p>As Kafka adoption is a strategic investment, it is important to grow the competency and skill set to manage kafka clusters.  Zookeeper was an important element to complexify the cluster management, as 2.8 it is removed, so it should be simpler to manage cluster.</p> <p>With customers can influence the product roadmap, but it will kill the open source approach if only Confluent committers prioritize the feature requets.  It is important to keep competitions and multi committers.</p> Differences between Akka and Kafka? <p>Akka is an open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism for I/O operations: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM.  Kafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture.</p> <p>vert.x is another open source implementation of such internal messaging mechanism but supporting more languages like, Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.</p> Is is possible to stream video to kafka? <p>Yes it is possible, but developers need to do that with care and real justification, and verify if video processing can be done from files at rest. If the goal is to classify streamed images in real-time then it may be possible.</p> <p>The following article, from Neeraj Krishna illustrates a python implementation to send video frame every 3 images, do image classification using <code>ResNet50</code> model trained on ImageNet, embedded in python consumer. The results are saved in mongodb with the metadata needed to query after processing.</p>"},{"location":"techno/kafka/faq/#other-faqs","title":"Other FAQs","text":"<ul> <li>IBM Event streams on Cloud FAQ </li> <li>FAQ from Confluent</li> </ul>"},{"location":"techno/kafka/playground/","title":"Kafka development sandbox","text":""},{"location":"techno/kafka/playground/#quarkus-dev","title":"Quarkus dev","text":"<p>If any Kafka-related extension is present (e.g. quarkus-smallrye-reactive-messaging-kafka), Dev Services for Kafka automatically starts a Kafka broker in dev mode and when running tests.</p> <ul> <li>Using the REST Client</li> <li>Apache Kafka Reference Guide with dev service</li> <li>Testing Your Application</li> </ul>"},{"location":"techno/kafka/playground/#kafka-local","title":"Kafka local","text":"<p>When using docker compose in the eda-quickstart repository, the compose starts one zookeeper and one Kafka broker locally using last Strimzi packaging, one Apicurio for schema registry and Kafdrop for UI.</p> <p>In the docker compose the Kafka defines two listeners, for internal communication using the DNS name <code>kafka</code> on port 29092 and one listener for external communication on port 9092.</p> <p></p> <p>A container in the same network needs to access kafka via its hostname. </p> <p>To start kafkacat and kafkacat doc to access sample consumer - producer</p> <pre><code>docker run -it --network=host edenhill/kafkacat -b kafka:9092 -L\n</code></pre>"},{"location":"techno/kafka/producer/","title":"Understanding Kafka Producers","text":"<p>A Kafka producer is a thread-safe client API that publishes records to the Kafka cluster. It utilizes buffers, thread pools, and serializers to efficiently send data.</p> <p>Producers are stateless entities; the responsibility of managing message offsets lies with the consumers. </p> <p>When a producer establishes connection, it retrieves metadata about the target topic, including information about partitions and the leader broker to connect to.</p> <p>The assignment of messages to partitions follows different algorithms, depending on the configuration. If no key is specified, a round-robin approach is used. Alternatively, the hash code of the message key can be employed to determine the target partition. Additionally, custom partition assignment strategies can be defined.</p> <p>We recommend reading the IBM Event streams producer guidelines to understand  how producers work with its configuration parameters.</p>"},{"location":"techno/kafka/producer/#design-considerations","title":"Design considerations","text":"<p>When developing a record producer developers need to assess the followings:</p> <ul> <li>What is the event payload to send? Is it a root aggregate, as defined in domain driven design, with value objects?  Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic/partition. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site. Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to arrive: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section).</li> <li>Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro schema.</li> <li>What is the expected throughput to send events?: Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with <code>buffer.memory</code> property. (See producer configuration API)</li> <li>Can the producer batches events together to send them over one send operation? By design kafka producers batch events.</li> <li>Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection.</li> <li> <p>When deploying kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic.</p> </li> <li> <p>What is the exactly once delivery requirement: Look at idempotent producer: retries will not introduce duplicate records (see section below).</p> </li> <li>Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition.</li> <li>Where the event timestamp comes from? Should the producer <code>send()</code> operation set it or is it loaded from external data? Remember that <code>LogAppendTime</code> is considered to be processing time, and <code>CreateTime</code> is considered to be event time.</li> </ul>"},{"location":"techno/kafka/producer/#typical-producer-code-structure","title":"Typical producer code structure","text":"<p>The producer code, using java or python API, does the following steps:</p> <ul> <li>Define producer properties.</li> <li>Create a producer instance.</li> <li>Connect to the bootstrap URL, get a broker leader</li> <li>Send event records and get resulting metadata.</li> </ul> <p>Producers are thread safe. The <code>send()</code> operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements.</p> <p>Here is an example of producer code from the quick start.</p>"},{"location":"techno/kafka/producer/#kafka-useful-producer-apis","title":"Kafka useful Producer APIs","text":"<p>Here is a list of common APIs to use in any producer and consumer code.</p> <ul> <li>KafkaProducer A Kafka client that publishes records to the Kafka cluster.  The send method is asynchronous. A producer is thread safe so we can have per topic to interface.</li> <li>ProducerRecord to be published to a topic</li> <li>RecordMetadata metadata for a record that has been acknowledged by the server.</li> </ul>"},{"location":"techno/kafka/producer/#properties-to-consider","title":"Properties to consider","text":"<p>The following properties are helpful to tune at each topic and producer and will vary depending on the requirements:  </p> Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to <code>All</code>. TRANSACTION_ID A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session."},{"location":"techno/kafka/producer/#advanced-producer-guidances","title":"Advanced producer guidances","text":""},{"location":"techno/kafka/producer/#how-to-support-exactly-once-delivery","title":"How to support exactly once delivery","text":"<p>Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events.</p> <p>Producer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported:</p> <ul> <li>At least once: means the producer set  <code>ACKS=1</code> and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3).  If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message.</li> <li>At most semantic: means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost.</li> <li>Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event.</li> </ul> <p>With <code>acks = 1</code> it is possible to lose messages, as illustrated in the following diagram, where new messages were not replicated yet, and ack was already sent back to the producer. Losing messages will depend if a replica is taking the leader position or not, or if the failed broker goes back online before replicas election but has no fsynch to the disk before the crash. </p> <p></p> <p>To avoid that we need to have <code>ack=-1</code>, three replicas and in-sync-replica=2. Replicated messages are acknowledged, when broker fails, a new leader is selected with the replicated messagesm it becomes the partition leader and others start to replicate from him. Producer reconnect to the partition leader.</p> <p></p> <p>Here is an example of cluster replication configuration set for all topics:</p> <pre><code>spec:\n    strimziOverrides:\n        kafka:\n            config:\n                default.replication.factor: 3\n                min.insync.replicas: 2\n</code></pre> <p>But it can be specified at the topic level:</p> <pre><code>kind: KafkaTopic\nmetadata:\n  name: rt-store.inventory\n  namespace: rt-inventory-dev\n  labels:\n    eventstreams.ibm.com/cluster: dev\nspec:\n  partitions: 1\n  replicas: 3\n  config:\n    min.insync.replicas: 1\n</code></pre> <p>At the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end.</p> <p></p> <p>Sometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate records...</p> <p></p> <p>To avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost.</p> <p></p> <p>The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system.</p> <p>The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance.</p> <p>To add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transacitional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method:</p> <pre><code>producerProps.put(\"enable.idempotence\", \"true\");\nproducerProps.put(\"transactional.id\", \"prod-1\");\nkafkaProducer.initTransactions()\n</code></pre> <p><code>initTransactions()</code> registers the producer with the broker as one that can use transaction, identifying it by its <code>transactional.id</code> and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work.</p> <p>Kafka streams with consume-process-produce loop requires transaction and exactly once. Even commiting its read offset is part of the transaction. So Producer API has a sendOffsetsToTransaction method.</p> <p>See the KIP 98 for details.</p> <p>In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction.</p> <p>To support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. </p> <p>See the code in order command microservice.</p> <p>The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration</p> <pre><code>consumerProps.put(\"enable.auto.commit\", \"false\");\nconsumerProps.put(\"isolation.level\", \"read_committed\");\n</code></pre> <p>With <code>read_committed</code>, no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written.</p> <p>In consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read.</p> <pre><code>offsetsToCommit.put(partition, new OffsetAndMetadata(offset + 1))\nproducer.sendOffsetsToTransaction(offsetsToCommit, \"consumer-group-id\");\n</code></pre> <p>The producer then commits the transaction.</p> <pre><code>try {\n    kafkaProducer.beginTransaction();\n    ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(ApplicationConfig.ORDER_COMMAND_TOPIC, key, value);\n    Future&lt;RecordMetadata&gt; send = kafkaProducer.send(record, callBackFunction);\n\n    kafkaProducer.commitTransaction();\n} catch (KafkaException e){\n    kafkaProducer.abortTransaction();\n}\n</code></pre> <p>There is an interesting article from the Baeldung team about exactly once processing in kafka with code example which we have re-used to implement the order processing in our Reefer Container Shipment reference application and explained here</p>"},{"location":"techno/kafka/producer/#more-readings","title":"More readings","text":"<ul> <li>Creating advanced kafka producer in java - Cloudurable</li> <li>Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it</li> <li>Order management with CQRS in Java</li> <li>EDA quickstart Quarkus Producer API</li> <li>Event driven microservice template</li> </ul>"},{"location":"techno/kafka/security/","title":"Kafka Security Overview","text":"Versioning <p>Created 04/13/2022 - Updated 09/2024</p> <p>Review this video for a refresh on SSL and TLS certificates and keep in mind what the speaker quotes:</p> <ul> <li>Any message encrypted with Bob's public key can only be decrypted with Bob's private key</li> <li>Anyone with access to Alice's public key can verify that a message could only have been created by someone with access to Alice's private key.</li> </ul> <p></p> <p>For a deeper dive into security administration see this Confluent article  and Kafka's product documentation.</p> <p>We also strongly recommend reading Rick Osowski's Medium blogs Part 1 and Part 2 on Kafka security configuration.</p>"},{"location":"techno/kafka/security/#understand-the-kafka-cluster-listeners","title":"Understand the Kafka cluster listeners","text":"<p>You can secure your Kafka resources by managing the access each user and application has to each resource.</p> <p>A Kafka cluster may be configured to expose up to 2 internal and 1 external Kafka listeners. These listeners provide the mechanism for Kafka client applications to communicate with the Kafka brokers and these can be configured as secured listeners (which is the default for the <code>tls</code> and <code>external</code> Kafka listener).</p> <p>Each listener providing a connection to Kafka brokers can also be configured to authenticate connections with either Mutual TLS or SCRAM SHA 512 authentication mechanisms.  Additionally, the Kafka cluster can be configured to authorize operations sent via an authenticated listener using access control list defined at the user level.</p> <p>The following figure presents a decision tree and the actions to consider for configuring cluster and applications.</p> <p></p> <p>In Kafka, the following yaml snippet from an Strimzi instance definition defines the following Kafka listeners\"</p> <ul> <li>One internal non secured kafka listener on port <code>9092</code> called <code>plain</code></li> <li>One internal secured (TLS encrypted) Kafka listener on port <code>9093</code> called <code>tls</code>, which also enforces authentication throughout TLS,</li> <li>One external secured (TLS encrypted) Kafka listener on port <code>9094</code> called <code>external</code>, which also enforces authentication throughout SCRAM credentials, that is exposed through a route.</li> </ul> Cluster definition in Strimzi<pre><code>listeners:\n  - name: plain\n    port: 9092\n    type: internal\n    tls: false\n  - name: tls  # (1)\n    port: 9093\n    type: internal  # (2)\n    tls: true\n    authentication:\n        type: tls\n  - name: external\n    type: route\n    port: 9094\n    tls: true \n    authentication:\n      type: scram-sha-512\n</code></pre> <ol> <li><code>tls: true</code> enforces traffic encryption. Default is true for Kafka listeners on ports <code>9093</code> and <code>9094</code></li> <li><code>type: internal</code> specifies that a Kafka listener is internal. Kafka listenes on ports <code>9092</code> and <code>9093</code> default to internal.</li> </ol>"},{"location":"techno/kafka/security/#connect-kafka-api","title":"Connect Kafka API","text":"<p>The most important and essential property to connect to Kafka brokers, is the <code>bootstrap.servers</code> property. This property tells Kafka clients what URL to use to talk to Kafka cluster. Based on that Kafka listener, developers may need to provide the application with extra configuration.</p> <p>At the very minimum, they will need to set the security.protocol property that will tell whether the application isconnecting to a secured Kafka listener or not. As a result, the values for <code>security.protocol</code> are:</p> <ul> <li><code>PLAINTEXT</code> - using PLAINTEXT transport layer &amp; no authentication - default value.</li> <li><code>SSL</code> - using SSL transport layer &amp; certificate-based authentication or no authentication.</li> <li><code>SASL_PLAINTEXT</code> - using PLAINTEXT transport layer &amp; SASL-based authentication.</li> <li><code>SASL_SSL</code> - using SSL transport layer &amp; SASL-based authentication.</li> </ul> <p>Based on the above, the security protocol developers will use to connect to the different Kafka listeners that Kafka deploys are:</p> <ul> <li><code>PLAINTEXT</code> when connecting to the non secured internal <code>plain</code> Kafka listener on port <code>9092</code></li> <li><code>SSL</code> when connecting to the secured (TLS encrypted) internal <code>tls</code> Kafka listener on port <code>9093</code> that also enforces authentication through TLS certificates</li> <li><code>SASL_SSL</code> when connecting to the secured (TLS encrypted) external <code>external</code> Kafka listener on port <code>9094</code> that also enforces authentication through SCRAM credentials.</li> </ul>"},{"location":"techno/kafka/security/#non-secured-listener","title":"Non-secured listener","text":"<p>You would only need to specify that there is no security in place for your application to connect to a non-secured kafka listener:</p> Configuration properties<pre><code>security.protocol=PLAINTEXT\n</code></pre> <p>Do not set <code>sasl.mechanisms</code> when security.protocol=PLAINTEXT</p>"},{"location":"techno/kafka/security/#secured-listener","title":"Secured listener","text":"<p>In order for the application to be able to connect to Kafka through the internal secured (TLS encrypted) Kafka listener, you need to set the appropriate value for <code>security.protocol</code>, as seen above, plus provide the Certificate Authority of the Kafka cluster (its public key).</p> <p>Depending on the technology of the application, you will need to provide the Certificate Authority of the Kafka cluster for the TLS encryption either as a <code>PKCS12</code> certificate for a Java client or as a <code>PEM</code> certificate for anything else. <code>PKCS12</code> certificates (or truststores) come in the form of a <code>.p12</code> file and are secured with a password. You can inspect a <code>PKCS12</code> certificate with:</p> <pre><code>openssl pkcs12 -info -nodes -in truststore.p12\n</code></pre> <p>and providing the truststore password.</p> <p>An example of the output would be:</p> <pre><code>MAC Iteration 100000\nMAC verified OK\nPKCS7 Encrypted data: Certificate bag\nBag Attributes\n    friendlyName: ca.crt\n    2.16.840.1.113894.746875.1.1: &lt;Unsupported tag 6&gt;\nsubject=/O=io.strimzi/CN=cluster-ca v0\nissuer=/O=io.strimzi/CN=cluster-ca v0\n-----BEGIN-----\nMIIDOzCCAiOgAwIBAgI\n....\n\nDl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf\n924CQFYsZS2kdhl5GgqQ\n-----END CERTIFICATE-----\n</code></pre> <p>On the other hand, <code>PEM</code> certificates come in the form of a <code>.pem</code> file and are not password protected.</p> <p>You can inspect them using the <code>cat</code> command. </p> <p>The output should be the same certificate as the one provided within the <code>PKCS12</code> certificate:</p> <pre><code>cat es-cert.pem \n-----BEGIN CERTIFICATE-----\nMIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL\n...\nDl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf\n924CQFYsZS2kdhl5GgqQ\n-----END CERTIFICATE-----\n</code></pre> <p>Once you have the Certificate Authority of your Kafka cluster, you will provide its location and password in your properties file through the <code>ssl.truststore.location</code> and <code>ssl.truststore.password</code> properties.</p> Secured configuration<pre><code>security.protocol=SSL or SASL_SSL\nssl.protocol=TLSv1.2\n\nssl.truststore.password=&lt;truststore.p12-password&gt;\nssl.truststore.location=truststore.p12\nssl.truststore.type=PKCS12\n</code></pre> <p>where <code>security.protocol</code> will vary between <code>SSL</code> or <code>SASL_SSL</code> based on the authentication as you will see in the next section.</p>"},{"location":"techno/kafka/security/#authentication","title":"Authentication","text":"<p>You have seen above that your Kafka listeners can require authentication to any application or client wanting to connect to the Kafka cluster through them. It was also said that authentication could be either of type SASL-based, through SCRAM (modern Salted Challenge Response Authentication Mechanism) credentials, or certificate-based (TLS). Either way, Kafka will handle authentication through <code>KafkaUser</code> objects. </p> <p>These objects that represent Kafka users of your Kafka instance will have their authentication (and authorization through ACLs) credentials or TLS certificates associated to them stored in a secret. In order to find out how to create these <code>KafkaUsers</code>, which will vary depending on the authentication method.</p>"},{"location":"techno/kafka/security/#scram","title":"Scram","text":"<p>If you have created a <code>KafkaUser</code> to be used with a Kafka listener that requires SCRAM authentication, you will be able to retrieve its SCRAM credentials either from the Kafka UI at creation time or later on from the secret these are stored to:</p> <pre><code>oc extract secret/&lt;KAFKA_USER&gt; -n &lt;NAMESPACE&gt; --keys=sasl.jaas.config --to=-\n</code></pre> <p>where</p> <ul> <li><code>&lt;KAFKA_USER&gt;</code> is the name of the <code>KafkaUser</code> object you created.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace where Kafka is deployed on.</li> </ul> <p>Example:</p> <pre><code>oc extract secret/test-app -n tools --keys=sasl.jaas.config --to=-\n# sasl.jaas.config\norg.apache.kafka.common.security.scram.ScramLoginModule required username=\"test-app\" password=\"VgWpkjAkvxH0\";\n</code></pre> <p>You can see above your SCRAM username and password.</p>"},{"location":"techno/kafka/security/#tls","title":"TLS","text":"<p>If you have created a <code>KafkaUser</code> to be used with a Kafka listener that requires TLS authentication,  you will be able to retrieve its TLS certificates either from the Kafka UI at creation time in a zip folder or  later on from the secret these are stored to.</p> <p>First, describe the secret to see what certificates are stored in it:</p> <pre><code>$ oc describe secret test-app-tls -n tools\nName:         test-app-tls\nNamespace:    tools\nLabels:       app.kubernetes.io/instance=test-app-tls\n              app.kubernetes.io/managed-by=strimzi-user-operator\n              app.kubernetes.io/name=strimzi-user-operator\n              app.kubernetes.io/part-of=eventstreams-test-app-tls\n              eventstreams.ibm.com/cluster=es-inst\n              eventstreams.ibm.com/kind=KafkaUser\nAnnotations:  &lt;none&gt;\n\nType:  Opaque\n\nData\n====\nuser.key:       1704 bytes\nuser.p12:       2384 bytes\nuser.password:  12 bytes\nca.crt:         1180 bytes\nuser.crt:       1025 bytes\n</code></pre> <p>You can see that the secret will store the following:</p> <ul> <li><code>user.key</code> and <code>user.crt</code> - the client certificate key-pair.</li> <li><code>user.p12</code> - trustore that contains the <code>user.key</code> and <code>user.crt</code>.</li> <li><code>user.password</code> - contains the <code>user.p12</code> truststore password.</li> <li><code>ca.crt</code> - CA used to sign the client certificate key-pair.</li> </ul> <p>Then, you can extract the appropriate certificate based on whether your application or Kafka client is Java based or not. In the case of a Java based application or Kafka client, extract the <code>user.p12</code> and <code>user.password</code>:</p> <pre><code>oc extract secret/&lt;KAFKA_USER&gt; -n &lt;NAMESPACE&gt; --keys=user.p12\noc extract secret/&lt;KAFKA_USER&gt; -n &lt;NAMESPACE&gt; --keys=user.password\n</code></pre> <p>where</p> <ul> <li><code>&lt;KAFKA_USER&gt;</code> is the name of the <code>KafkaUser</code> object you created.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace where Kafka is deployed on.</li> </ul>"},{"location":"techno/kafka/security/#properties-config","title":"Properties config","text":"<p>Now that you know how to get the authentication credentials or certificates for a proper authentication of your application or Kafka client you need to configure the appropriate properties for that:</p> <ul> <li>If your Kafka listener authentication method is SCRAM:</li> </ul> Scram authentication with SSL encryption<pre><code>security.protocol=SASL_SSL\n\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"&lt;USERNAME&gt;\" password\\=\"&lt;PASSWORD&gt;\";\n</code></pre> <ul> <li>If your Kafka listener authentication method is TLS:</li> </ul> TLS authentication and SSL encryption<pre><code>security.protocol=SSL\n\nssl.keystore.location=&lt;location_to_your_user.p12&gt;\nssl.keystore.password=&lt;user.p12-password&gt;\nssl.keystore.type=PKCS12\n</code></pre>"},{"location":"techno/kafka/security/#recapitulation","title":"Recapitulation","text":"<p>Let's have a full look at how the Kafka communication properties, for a Java application or client, would look like for Kafka  on RedHat OpenShift with the defaults. </p> <p>This translates to Strimzi (the open source project Kafka is based on) in:</p> <pre><code>listeners:\n  - name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n        type: tls\n  - name: external\n    type: route\n    port: 9094\n    tls: true \n    authentication:\n      type: scram-sha-512\n</code></pre> <p>Let's also add the <code>plain</code> non-secure Kafka listener to the picture so that all cases are covered in this recap section.</p> <pre><code>listeners:\n  plain: {}\n  external:\n    type: route\n    authentication:\n      type: scram-sha-512\n  tls:\n    authentication:\n      type: tls\n</code></pre> <p>As a result, the Kafka instance deployed will count with:</p> <ul> <li>One internal non secured kafka listener on port <code>9092</code> called <code>plain</code></li> <li>One internal secured (TLS encrypted) Kafka listener on port <code>9093</code> called <code>tls</code>, which also enforces authentication throughout TLS, and </li> <li>One external secured (TLS encrypted) Kafka listener on port <code>9094</code> called <code>external</code>, which also enforces authentication throughout SCRAM credentials, that is exposed through a route.</li> </ul>"},{"location":"techno/kafka/security/#plain","title":"Plain","text":"<p>The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the non secured kafka listener on port <code>9092</code> called <code>plain</code> will be as follows:</p> <pre><code># Internal plain listener\n# =======================\nsecurity.protocol=PLAINTEXT\nbootstrap.servers=&lt;ES_NAME&gt;-kafka-bootstrap.&lt;NAMESPACE&gt;.svc\\:9092\n</code></pre> <p>where</p> <ul> <li><code>&lt;ES_NAME&gt;</code> is the name of the Kafka instance deployed you are trying to connect to.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace the Kafka instance you are trying to connect to is deployed in.</li> </ul>"},{"location":"techno/kafka/security/#internal-tls","title":"Internal tls","text":"<p>The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the internal s ecured (TLS encrypted) Kafka listener on port <code>9093</code> called <code>tls</code>, which also enforces authentication throughout mTLS will be as follows:</p> <pre><code># Internal tls listener\n# =====================\nbootstrap.servers=&lt;&lt;ES_NAME&gt;-kafka-bootstrap.&lt;NAMESPACE&gt;.svc\\:9093\n\nsecurity.protocol=SSL\nssl.protocol=TLSv1.2\n\n## mTLS Authentication for the client.\nssl.keystore.location=&lt;user.p12-location&gt;\nssl.keystore.password=&lt;user.p12-password&gt;\nssl.keystore.type=PKCS12\n\n## Certificate Authority of your Kafka cluster\nssl.truststore.password=&lt;trustore.p12-password&gt;\nssl.truststore.location=&lt;truststore.p12-location&gt;\nssl.truststore.type=PKCS12\n</code></pre> <p>where</p> <ul> <li><code>&lt;ES_NAME&gt;</code> is the name of the Kafka instance deployed you are trying to connect to.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace the Kafka instance you are trying to connect to is deployed in.</li> <li><code>&lt;user.p12-location&gt;</code> is the location of the <code>user.p12</code> truststore containing the <code>user.key</code> and <code>user.crt</code> client certificate key-pair for the application or client mTLS authentication as explained above.</li> <li><code>&lt;user.p12-password&gt;</code> is the password of the <code>&lt;user.p12&gt;</code> truststore.</li> <li><code>&lt;truststore.p12-location&gt;</code> is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your Kafka instance and your application or Kafka client.</li> <li><code>&lt;trustore.p12-password&gt;</code> is the password for the <code>truststore.p12</code> truststore.</li> </ul> <p>When the application is deployed on OpenShift, certificates will be mounted to the application pod. Below is an example of a Quarkus app deployment  descriptor, with environment variables:</p> <pre><code>env:\n  - name: KAFKA_SSL_TRUSTSTORE_FILE_LOCATION\n  value: /deployments/certs/server/ca.p12\n- name: KAFKA_SSL_TRUSTSTORE_TYPE\n  value: PKCS12\n- name: KAFKA_SSL_KEYSTORE_FILE_LOCATION\n  value: /deployments/certs/user/user.p12\n- name: KAFKA_SSL_KEYSTORE_TYPE\n  value: PKCS12\n- name: KAFKA_SECURITY_PROTOCOL\n  value: SSL\n- name: KAFKA_USER\n  value: tls-user\n- name: KAFKA_CERT_PWD\n  valueFrom:\n    secretKeyRef:\n      key: ca.password\n      name: kafka-cluster-ca-cert\n- name: USER_CERT_PWD\n  valueFrom:\n    secretKeyRef:\n      key: user.password\n      name: tls-user\n# ...\n        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: kafka-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n      volumes:\n      - name: kafka-cert\n        secret:\n          optional: true\n          secretName: kafka-cluster-ca-cert\n      - name: user-cert\n        secret:\n          optional: true\n          secretName: tls-user\n</code></pre>"},{"location":"techno/kafka/security/#external-tls","title":"External tls","text":"<p>The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the external secured (TLS encrypted) Kafka listener on port <code>9094</code> called <code>external</code>, which also enforces authentication throughout SCRAM credentials, and that is exposed through a route will be as follows:</p> <pre><code># External listener SCRAM\n# =======================\nbootstrap.servers=&lt;ES_NAME&gt;-kafka-bootstrap-&lt;NAMESPACE&gt;.&lt;OPENSHIFT_APPS_DNS&gt;\\:443\n\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\n\n## Certificate Authority of your Kafka cluster\nssl.truststore.password=&lt;trustore.p12-password&gt;\nssl.truststore.location=&lt;truststore.p12-location&gt;\nssl.truststore.type=PKCS12\n\n## Scram credentials\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"&lt;SCRAM_USERNAME&gt;\" password\\=\"&lt;SCRAM_PASSWORD&gt;\";\n</code></pre> <p>where</p> <ul> <li><code>&lt;ES_NAME&gt;</code> is the name of the Kafka instance deployed you are trying to connect to.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace the Kafka instance you are trying to connect to is deployed in.</li> <li><code>&lt;OPENSHIFT_APPS_DNS&gt;</code> is your RedHat OpenShift DNS domain for application routes.</li> <li><code>&lt;truststore.p12-location&gt;</code> is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your Kafka instance and your application or Kafka client.</li> <li><code>&lt;trustore.p12-password&gt;</code> is the password for the <code>truststore.p12</code> truststore.</li> <li><code>&lt;SCRAM_USERNAME&gt;</code> and <code>&lt;SCRAM_PASSWORD&gt;</code> are your SCRAM credentials.</li> </ul>"},{"location":"techno/kafka/security/#tips","title":"Tips","text":"<p>Remember that if the application does not run in the same namespace as the kafka cluster then you need to copy the secrets so that  the application developers can access the required credentials and certificates from their own namespaces with something like</p> <pre><code>if [[ -z $(oc get secret ${TLS_USER} 2&gt; /dev/null) ]]\nthen\n   # As the project is personal to the user, we can keep a generic name for the secret\n   oc get secret ${TLS_USER} -n ${KAFKA_NS} -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n\nif [[ -z $(oc get secret ${SCRAM_USER} 2&gt; /dev/null) ]]\nthen\n    # As the project is personal to the user, we can keep a generic name for the secret\n    oc get secret ${SCRAM_USER} -n ${KAFKA_NS} -o json |  jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n</code></pre>"},{"location":"techno/kafka/security/#kafka-connect","title":"Kafka Connect","text":"<p>For Kafka connector, you need to define authentication used to connect to the Kafka Cluster:</p> <pre><code>  authentication: \n    type: tls\n    certificateAndKey:\n      secretName: tls-user\n      certificate: user.crt\n      key: user.key\n</code></pre> <ul> <li>Get TLS public cluster certificate:</li> </ul> <pre><code>  tls: \n    trustedCertificates:\n      - secretName: dev-cluster-ca-cert\n        certificate: ca.crt\n</code></pre>"},{"location":"techno/kafka/security/#working-with-certificates","title":"Working with certificates","text":"<p>To extract a PEM-based certificate from a JKS-based truststore, you can use the following command:</p> <pre><code>keytool -exportcert -keypass {truststore-password} -keystore {provided-kafka-truststore.jks} -rfc -file {desired-kafka-cert-output.pem}\n</code></pre> <p>To build a PKCS12 from a pem do</p> <pre><code>openssl pkcs12 -export -in cert.pem -out cert.p12\n# if you want jks\nkeytool -importkeystore -srckeystore cert.p12 -srcstoretype pkcs12 -destkeystore cert.jks\n</code></pre>"},{"location":"techno/kafka-connect/","title":"Kafka Connect","text":"Version Update <p>Created 05/2019 - Updated 09/24/2024</p> <p>Kafka connect is an open-source framework designed to facilitate the integration of external systems with Kafka. It is compatible with all Kafka products and employs the concepts of source and sink connectors to ingest data into or deliver data from Kafka topics.</p>"},{"location":"techno/kafka-connect/#concepts-and-architecture","title":"Concepts and Architecture","text":"<p>A source connector collects data from a system. A sink connector delivers data from Kafka topics into other systems.</p> <p>Here is the basic concepts of Kafka Connect integration with Kafka Cluster and external systems</p> <p></p> <p>The general concepts are presented in Robin Moffatt's video, which we can summarize as:</p> <ul> <li>Connector represents a logical job to move data from / to Kafka  to / from external systems. Apache Camel offers open source Kafka connectors, or developers can implement their own connector.</li> <li>Workers are JVMs running the connectors. For production deployment workers run in cluster or \"distributed mode\", and leverage the Kafka consumer group management protocol to scale tasks horizontally.</li> <li>Tasks: each worker coordinates a set of tasks to move the data. In distributed mode, task states are saved in Kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.</li> <li>REST API to configure the connectors and monitors the tasks.</li> </ul> <p>It supports both distributed and standalone deployment modes. Fully integrated with the Kafka cluster to maintain its state, it automatically manages offsets and handles the complex process of offset commitment.</p> <p>The following figure illustrates a typical 'distributed' deployment of a Kafka Connect cluster:</p> <p></p> <p>Workers are the running processes (JVMs) responsible for executing connectors and tasks. They utilize the existing Kafka group management protocol to scale easily. Each connector defines and manages a set of tasks that handle the actual data movement. Tasks run as threads within a JVM.</p> <p>When a connector is initially submitted to the cluster, the workers rebalance the entire set of connectors and their tasks to ensure that each worker has approximately the same workload.</p> <p>Connectors and tasks are not guaranteed to run on the same instance in the cluster, especially when multiple tasks and instances are present in the Kafka Connect cluster.</p> <p>The connector can be configured to include Converters (which translate data between Connect and the system sending or receiving the data) and Transforms (which apply simple logic to alter each message produced by or sent to a connector).</p>"},{"location":"techno/kafka-connect/#key-value-propositions","title":"Key value propositions","text":"<ol> <li>Simplified Integration: Kafka Connect provides a straightforward, configuration based, way to connect external systems to Kafka, enabling easy data ingestion and delivery without needing custom code. There are a lot of existing connectors, and each main source or sink technology vendors have their own connectors. Confluent connectors.</li> <li>Scalability: Kafka Connect supports both distributed and standalone deployment modes, allowing it to scale efficiently with the needs of the application.</li> <li>Fault Tolerance: It integrates seamlessly with Kafka\u2019s fault-tolerant architecture, ensuring that data is reliably processed even in the event of failures.</li> <li>Data Transformation: With support for Converters and Transforms, Kafka Connect allows for data transformation and formatting on-the-fly, ensuring that data is in the correct format for both producers and consumers.</li> <li>Community and Ecosystem: Free your teams from writing generic integration code and managing connectors to focus on building real-time apps. Being part of the Kafka ecosystem, it benefits from a large community and a wide range of existing connectors for various systems, such as databases, cloud services, and other messaging platforms.</li> <li>Ease of Use: It simplifies the management of data flows, allowing users to focus on business logic rather than the intricacies of data integration.</li> <li>Real-Time Processing: Kafka Connect enables real-time data streaming, making it suitable for applications that require immediate insights and actions based on incoming data.</li> </ol>"},{"location":"techno/kafka-connect/#characteristics","title":"Characteristics","text":"<ul> <li>Kafka Connect connectors can efficiently copy large volumes of data from a source to Kafka by operating at the datasource level using native protocols. For instance, when the source is a database, it utilizes the JDBC API.</li> <li>It supports both streaming and batching of data.</li> <li>It can scale from a standalone, single connector setup to running tasks in parallel on a distributed cluster.</li> <li>Kafka Connect defines three models: the data model, worker model, and connector model. The worker model enables Kafka Connect to scale the application effectively.</li> <li>A Kafka Connect cluster can support multiple applications and can be organized as a service.</li> <li>A REST API is available for submitting and managing connectors.</li> </ul>"},{"location":"techno/kafka-connect/#fault-tolerance","title":"Fault tolerance","text":"<p>For fault tolerance and offset management, Kafka Connect utilizes three Kafka topics to persist its state, which are created when the connectors start:</p> <ul> <li>connect-configs: This topic stores the configurations for connectors and tasks.</li> <li>connect-offsets: This topic retains offsets for Kafka Connect.</li> <li>connect-status: This topic contains status updates for connectors and tasks.</li> </ul> <p>When a worker fails: </p> <p></p> <p>Tasks allocated in the failed worker are reallocated to existing workers, and the task's states (read offsets, source record mapping to offset) are reloaded from the different state topics.</p> <p></p>"},{"location":"techno/kafka-connect/#connector-cluster-configuration","title":"Connector cluster configuration","text":"<p>Connectors are defined using properties files, configuration and Java code, package as jars and dropped into plugins folder.</p> <p>The following configurations are important to review:</p> <ul> <li><code>group.id</code>: one per connect cluster. It is used by source connectors only.</li> <li><code>heartbeat.interval.ms</code>: The expected time between heartbeats to the group coordinator when using Kafka\u2019s group management facilities.</li> <li><code>config.storage.topic</code>: Topic where connector configurations are stored</li> <li><code>offset.storage.topic</code>: Topic where source connector offsets are stored</li> <li><code>status.storage.topic</code>: Topic where connector and task status are stored</li> <li><code>key.converster</code> and <code>value.converter</code>: converter classes (e.g. Avro, Json) used to convert between Kafka Connect format and the serialized form that is written to Kafka.</li> <li><code>exactly.once.source.support</code>: For source connectors to use transactions to write source records and their source offsets, and by proactively fencing out old task generations before bringing up new ones.</li> </ul>"},{"location":"techno/kafka-connect/#playground","title":"Playground","text":""},{"location":"techno/kafka-connect/#installation","title":"Installation","text":"<p>The  Kafka connect framework fits well into a Kubernetes deployment model. Currently, there are different options for that deployment: the Strimzi Kafka connect operator, directly install binary on VM, or use a managed service from Confluent or one of the cloud providers.</p>"},{"location":"techno/kafka-connect/#local-demonstrations","title":"Local demonstrations","text":""},{"location":"techno/kafka-connect/#file-connectors","title":"File Connectors","text":"<p>The Kakfa Connect labs in [kafka-studies]((https://github.com/jbcodeforce/eda-quickstarts/tree/main/kafka-connect) present the simplest way to demontrate Kafka Connect using File Source and File Sink on a local computer. The approach is based on docker compose with 1 Kafka broker, 1 topic, and a container running the source and sink file connectors in standalone mode. File lines are copied to target file after simple message transformation.</p> <p>The distributed mode with this sample.</p>"},{"location":"techno/kafka-connect/#strimzi","title":"Strimzi","text":"<p>KafkaConnector resources allow developers to create and manage connector instances for Kafka Connect in a Kubernetes-native way. To manage connectors, developers can use the Kafka Connect REST API, or use KafkaConnector custom Kubernetes resources. Using the GitOps methodology, devlopers will define connector cluster and connector instance as yamls. Connector configuration is passed to Kafka Connect as part of an HTTP request and stored within Kafka itself.</p>"},{"location":"techno/kafka-connect/#ibm-mq-connector","title":"IBM MQ connector","text":"<p>The source code is in this repo and uses JMS as protocol to integrate with IBM MQ. When the connector encounters a message that it cannot process, it stops rather than throwing the message away.  The MQ source connector does not currently make much use of message keys. It is possible to use CorrelationID as a key by defining MQ source <code>mq.record.builder.key.header</code> property:</p> <pre><code>    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.converters.ByteArrayConverter\n    mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\n    mq.connection.mode: client\n    mq.message.body.jms: true\n    mq.record.builder.key.header: JMSCorrelationID\n</code></pre> <p>The record builder helps to transform the input message to a Kafka record, using or not a schema.</p> <p>Always keep the coherence between body.jms, record builder and data converter. </p> <p>The MQ source task starts a unique JMS Reader that will read n messages from the queue. The `poll() function  returns the list of MQ source records, and will commit to JMS if the number of message read match the batch size or if there is no more records. Once the Kafka Producer gets acknowledge that records are received by Brokers then use callback on the source task to commit MQ transaction for example. </p> <p>Any producer configuration can be modified in the source connector configuration:</p> <pre><code>producer.override.acks: 1\n</code></pre>"},{"location":"techno/kafka-connect/#example-of-kafka-connect-deployments","title":"Example of Kafka Connect Deployments","text":"<p>TBD </p> <ul> <li>File sink and sources in distributed mode sample</li> <li>S3 Sink connector</li> <li>Kinesis data stream with source connector.</li> </ul>"},{"location":"techno/kafka-connect/#further-readings","title":"Further Readings","text":"<ul> <li>Apache Kafka connect documentation</li> </ul>"},{"location":"techno/kstreams/","title":"Kafka Streams","text":"<p>Kafka Streams is client API to build microservices with input and output data are in Kafka. It is based on programming a graph of processing nodes to support the business logic developer wants to apply on the event streams. </p> <p>We recommend reading this excellent introduction Kafka stream made simple from Jay Kreps from Confluent to get a good understanding of why Kafka stream was created.</p>"},{"location":"techno/kstreams/#concepts","title":"Concepts","text":"<p>The business logic is implemented via topology which represents a graph of processing nodes.  Each node within the graph, processes events from the parent node. </p> <p>To summarize, Kafka Streams has the following capabilities:</p> <ul> <li>Kafka Streams applications are built on top of the producer and consumer Kafka APIs and are leveraging Kafka capabilities to do data parallelism processing, support distributed coordination of partition to task assignment, and being fault tolerant.</li> <li>Kafka supports exactly-once processing semantics to guarantee that each record is processed once and only once even when there is a failure.</li> <li>Streams processing is helpful for handling out-of-order data, reprocessing input as code changes, and performing stateful computations, like real time analytics. It uses stateful storage and consumer groups. It treats both past and future data the same way.</li> <li>Kafka Streams is an embedded library to integrate in any Java application. No need for separate processing cluster. As deployable container, it can scale horizontally easily within Kubernetes platform. It does not run in Kafka cluster.</li> <li>Topology consumes continuous real time flows of records and publishes new flows to one or more topics.</li> <li>A stream (represented by the KStream API) is a durable, partitioned sequence of immutable events. When a new event is added a stream, it's appended to the partition that its key belongs to.</li> <li>It can scale vertically, by increasing the number of threads for each Kafka Streams application on a single machine, and horizontally by adding additional machines or pods in kubernetes.  Each deployed instance use the same value for the <code>application.id</code> kafka stream property.</li> </ul> <p></p> <p>The assignment of stream partitions to stream tasks never changes, so task is the unit of parallelism. Task executes the topology, and is buffering records coming from the attached partitions. </p> <ul> <li>KTable is a durable, partitioned collection that models change over time. It's the mutable counterpart of KStreams. It represents what is true at the current moment. Each data record is considered a contextual update. Tables are saved in state store backed up with kafka topic and may be queried. Any operation on the table such as querying, inserting, or updating a row is carried out behind the scenes by a corresponding operation on the table\u2019s state store.</li> </ul> <p>These state stores are being\u00a0materialized on local disk\u00a0inside the application instances</p> <p></p> <ul> <li>Stream APIs transform, aggregate and enrich data, per record with millisecond latency, from one topic to another one.</li> <li>Supports stateful and windowing operations by processing one record at a time.</li> <li>An application's processor topology is scaled by breaking it into multiple tasks.</li> <li>Tasks can then instantiate their own processor topology based on the assigned partitions.</li> </ul> <p></p>"},{"location":"techno/kstreams/#fault-tolerance","title":"Fault tolerance","text":"<p>As KTables are persisted on state store, they are materialized on local to broker disk, as change log streams:</p> <p></p> <p>In the case of a stream processing task fails, it can rebuild its internal, in memory state store, from the kafka topic / change log (1). Once done it can consume messages again (2). The system is fault tolerant.</p>"},{"location":"techno/kstreams/#scaling","title":"Scaling","text":"<p>When topics have multiple partitions, each kafka streams task consumes a unique partition. </p> <p></p> <p>If for any reasons, we need to scale by adding new instances of the application, so in term of kubernetes, adding more pods, then the system will rebalance the stream tasks allocation to new instances created.</p> <p></p> <p>We can start as many threads of the application as there are input Kafka topic partitions.</p> <p>Another good example to illustrate threading, task and machine scaling is documented in this on Confluent article.</p>"},{"location":"techno/kstreams/#code-structure","title":"Code structure","text":"<p>In general the code for processing event does the following:</p> <ul> <li>Set a properties object to specify which brokers to connect to and what kind of key and value des/serialization mechanisms to use.</li> <li>Define a stream client: if you want to get the stream of records use KStream. If you want a changelog with the last value of a given key, use KTable (For example, using KTable to keep a user profile data by userid key).</li> <li>Create a topology of input source and sink target and the set of actions to perform in between.</li> <li>Start the stream client to consume records.</li> </ul> <p>Programming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining. Code size stays on the low size.</p> <p>A stateful operator uses the streaming Domain Specific Language, with constructs for aggregation, join and time window operations. Stateful transformations require a state store associated with the stream processor.</p> <p>We recommend at this stage to do our exercise 1 from the Tech Academy tutorial to develop a first simple topology and test it without any Kafka cluster using the <code>Topology test driver</code>.</p> <p>The following code extract, is part of the Apache Kafka Word count example and is used to illustrate the programming model used: </p> <pre><code>// Streams processing are created from a builder.\nfinal StreamsBuilder builder = new StreamsBuilder();\n// pattern to extract word\nfinal Pattern pattern = Pattern.compile(\"\\\\W+\");\n// source is a kafka topic, and materialized as a KStream\nKStream&lt;String, String&gt; textLines = builder.stream(source);\n// implement the logic to count words\nKTable&lt;String, Long&gt; wordCounts = textLines\n    .flatMapValues(textLine -&gt; Arrays.asList(pattern.split(textLine.toLowerCase())))\n    .print(Printed.toSysOut())\n    .groupBy((key, word) -&gt; word)\n    .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(\"counts-store\"));\n// sink is another kafka topic. Produce for each word the number of occurrence in the given doc\nwordCounts.toStream().to(sink, Produced.with(Serdes.String(), Serdes.Long()));\n\nKafkaStreams streams = new KafkaStreams(builder.build(), props);\nstreams.start();\n</code></pre> <ul> <li>KStream represents KeyValue records coming as event stream from the input topic.</li> <li><code>flatMapValues()</code> transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream (in memory). So here the text line is split into words. The parameter is a ValueMapper which applies transformation on values but keeps the key. Another important transformation is the KeyValueMapper.</li> <li><code>groupBy()</code> Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key.</li> <li><code>count()</code> counts the number of records in this stream by the grouped key. <code>Materialized</code> is an class to define a \"store\" to persist state and data. So here the state store is \"counts-store\". As store is a in-memory table, but it could also be persisted in external database. Could be the Facebook's RocksDB key value persistence or a log-compacted topic in Kafka.</li> <li>Produced defines how to provide the optional parameter types when producing to new topics.</li> <li>KTable is an abstraction of a changelog stream from a primary-keyed table.</li> </ul> <p>Important: map, flatMapValues and mapValues ... functions don\u2019t modify the object or value presented as a parameter.</p>"},{"location":"techno/kstreams/#available-tutorials","title":"Available tutorials","text":"<p>We found the following tutorial helpful to grow your competency on Kafka Streams:</p> <ul> <li>Word count Kafka Stream example from product documentation</li> <li>Use Quarkus and Kafka Streams to use groupBy, join with another Stream</li> <li>Quarkus and Kafka Streams guides</li> <li>Build an inventory aggregator with Quarkus, with kstreams, ktable and interactive queries, Mutiny, all deployable on OpenShift with quarkus kubernetes plugin.</li> </ul>"},{"location":"techno/kstreams/#interactive-queries","title":"Interactive queries","text":"<p>State store can be queried, and this is supported by the interactive queries. Result can be from the local store, if the key is in the local store, or a remote one. The metadata of the key to task allocation is maintained and shared between tasks. </p> <p>As a Kafka stream app runs on multiple instances, the entire state of the app is distributed among the instances. </p> <p>The Stream topology will transform the stream to table with one of the groupBy or aggregate operation:</p> <pre><code>items\n.groupByKey(ItemStream.buildGroupDefinition())\n.aggregate(\n    () -&gt;  new Inventory(),\n    (k , newItem, existingInventory) \n        -&gt; existingInventory.updateStockQuantity(k,newItem), \n        InventoryAggregate.materializeAsInventoryStore()); \n</code></pre> <p>which then it can be materialized as queryable key - value store.</p> <pre><code>// class InventoryAggregate\n/**\n * Create a key value store named INVENTORY_STORE_NAME to persist store inventory\n */\npublic static Materialized&lt;String, Inventory, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materializeAsInventoryStore() {\n    return Materialized.&lt;String, Inventory, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(INVENTORY_STORE_NAME)\n            .withKeySerde(Serdes.String()).withValueSerde(inventorySerde);\n}\n</code></pre> <p>Each store is local to the instance it was created in:</p> <p></p> <p>The storeID is the key used in the KTable. Once the Kafka Stream is started and store created (loop to get it ready) then it is easy to access it:</p> <pre><code>@Inject\nKafkaStreams streams;\n\nprivate ReadOnlyKeyValueStore&lt;String, Inventory&gt; getInventoryStockStore() {\n        while (true) {\n            try {\n\n                StoreQueryParameters&lt;ReadOnlyKeyValueStore&lt;String,Inventory&gt;&gt; parameters = StoreQueryParameters.fromNameAndType(InventoryAggregate.INVENTORY_STORE_NAME,QueryableStoreTypes.keyValueStore());\n                return streams.store(parameters);\n        ...\n\n// access one element of the store\nInventory result = getInventoryStockStore().get(storeID);\n</code></pre> <p>To get access to remote store, we need to expose each store via an API. The easiest one is a REST api, but it could be any RPC protocol. Each instance is uniquely identified via the <code>application.server</code> property. When deploying on kubernetes it could be the pod IP address accessible via the $POD_IP environment variable.  </p> <p>Below is a Quarkus declaration:</p> <pre><code>hostname=${POD_IP:localhost}\nquarkus.kafka-streams.application-server=${hostname}:8080\n</code></pre> <p>Now the design decision is to return the URL of the remote instance to the client doing the query call or do the call internally to the instance reached to always returning a result.</p> <p>The knowledge of other application instance is done by sharing metadata. The following code example illustrates the access to metadata for store via the kafka stream context and then build a pipeline metadata to share information about host, port and partition allocation.</p> <pre><code>streams.allMetadataForStore(ItemStream.ITEMS_STORE_NAME)\n                .stream()\n                .map(m -&gt; new PipelineMetadata(\n                        m.hostInfo().host() + \":\" + m.hostInfo().port(),\n                        m.topicPartitions()\n                                .stream()\n                                .map(TopicPartition::toString)\n                                .collect(Collectors.toSet())))\n                .collect(Collectors.toList());\n</code></pre>"},{"location":"techno/kstreams/#design-considerations","title":"Design considerations","text":"<ul> <li>Partitions are assigned to a StreamTask, and each StreamTask has its own state store. So it is important to use key and kafka will assign records with same key to same partition so lookup inside state store will work.</li> <li>Avoid external database lookup as part of the stream: As kafka can handle million of records per second, so a lookup to an external  database to do a join between a primary key that is in the event and a table in the database to do a data enrichment, for example,  is a bad practice. The approach will be to use Ktable, with state store and perform a join in memory.</li> <li>Reference data can be loaded inside a Ktable for event stream enrichment.</li> <li> <p>Table and streams joins: we recommend reading this deep dive article on joining between streams and joining stream with table. The important points from this article:</p> <ul> <li>kstream - kstream joins are windowed to control the size of data to keep in memory to search for the matching records.</li> </ul> </li> </ul>"},{"location":"techno/kstreams/#faust-a-python-library-to-do-kafka-streaming","title":"Faust: a python library to do kafka streaming","text":"<p>Faust is a python library to support stream processing. It does not have its own DSL as Kafka streams in Java has, but just python functions.</p> <p>It uses rocksdb to support tables.</p> <p>For the installation, in your python environment do a <code>pipenv run pip install faust</code>, or <code>pip install faust</code>. Then use faust as a CLI. So to start an agent as worker use:</p> <pre><code>faust -A nameofthepythoncode -l info\n</code></pre> <p>Multiple instances of a Faust worker can be started independently to distribute stream processing across machines and CPU cores.</p>"},{"location":"techno/kstreams/#further-readings","title":"Further readings","text":"<ul> <li>The API and product documentation.</li> <li>Kafka Streams  concepts from Confluent</li> <li>Deep dive explanation for the differences between KStream and KTable from Michael Noll</li> <li>Our set of samples to getting started in coding kafka streams </li> <li>Distributed, Real-time Joins and Aggregations using Kafka Stream, from Michael Noll at Confluent</li> <li>Confluent Kafka Streams documentation</li> <li>Kafka Streams architecture article from Confluent.</li> <li>Andy Bryant's article on kafka stream work allocation and sub-topologies</li> </ul>"},{"location":"techno/mirrormaker/","title":"Kafka Mirror Maker 2","text":"<p>Under construction</p> <p>This section introduces Mirror Maker 2.0, a replication feature of Kafka 2.4, detailing its usage and best practices for data replication between two Kafka clusters.  Mirror Maker 2.0 was defined as part of the Kafka Improvement Process - KIP 382 - and can be used for disaster recovery (active / passive) or for more complex topologies involving multiple Kafka Clusters to support an \"always-on\" architecture.</p>"},{"location":"techno/mirrormaker/#overview","title":"Overview","text":"<ul> <li>Mirror Maker is built on top of Kafka Connect and and is designed to replicate topics and consumer group metadata while preserving partitioning.</li> <li>At least two Kafka clusters are established: one designated as the source in the active region and another as the target for disaster recovery or passive use.</li> <li>The source cluster hosts producer and consumer applications, which can be deployed either within the same Kubernetes cluster or on VMs, accessing Kafka brokers.</li> <li>Producer, consumer, or streaming applications deployed in Kubernetes connect to the Kafka broker using the <code>bootstrap URL</code>, defined through an internal service. For example, it may look like <code>es-prod-kafka-bootstrap.ibm-eventstreams.svc.</code> This configuration ensures that the settings remain consistent across the target cluster.</li> <li>Kubernetes / OpenShift clusters are defined across three data centers, with a well-structured distribution. For a clearer understanding of the golden topology for OpenShift, refer to the following diagram, which illustrates the deployment of master and worker nodes.</li> </ul> <ul> <li>The target cluster hosts a MirrorMaker cluster, which operates based on the Kafka Connect framework.</li> </ul> <p>The following diagram illustrates those principles:</p> <p></p> <p>When zooming in on the replication details, we can observe the source topics from the blue cluster being replicated to the target topics on the green cluster. This configuration is designed for disaster recovery, utilizing an active-passive model. In this setup, only the left side has active applications that are producing and consuming records from the Kafka topics.</p> <p></p> <p>Since the mirroring occurs over longer internet distances, some latency in data replication is to be expected.</p> <p>We can enhance this deployment by utilizing MirrorMaker 2 to replicate data across multiple clusters in a more active-active configuration. The following diagram illustrates the concepts for an \"always-on\" deployment:</p> <p></p> <p>This model can also being used between cloud providers.</p> <p>In active-active mode, data is injected into the local cluster while replicated data is received from the remote cluster. The topic names are prefixed with the original cluster name. In the figure below, the cluster on the right features green local producers and consumers, with topics being replicated to the left into the blue cluster. Similarly, topics from the blue cluster on the left are replicated to the right</p> <p></p> <p>Consumers on both sides receive data from the <code>orders</code> topics (both local and replicated) to gain a comprehensive view of all orders created across both clusters.</p> <p>The following diagram provides a closer look at a classic web-based solution design, where mobile or web applications interact with a web tier to serve single-page applications, static content, and APIs.</p> <p></p> <p>A set of microservices implements the business logic, with some services being event-driven, producing and consuming events from topics. With active-active replication in place, the same topology is deployed in another data center, allowing data from the same topic (representing a business entity) to arrive at the replicated topic. Each service can then store the record in its own database and cache. (The service tier does not detail the expected replicas, and the application load balancer does not display routes to the other data center.)</p> <p>In the event of a failure on one side of the data replication, the data remains transparently accessible. A read model query will yield consistent results on both sides.</p> <p>In replication, the data within the topics, as well as topic states and metadata, are replicated.</p>"},{"location":"techno/mirrormaker/#mirror-maker-2-components","title":"Mirror Maker 2 components","text":"<p>Mirror maker 2.0 is the solution to replicate data in topics from one Kafka cluster to another. It uses the Kafka Connect framework to simplify configuration, parallel execution and horizontal scaling.</p> <p>The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.</p> <p></p> <p>MirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. It runs in standalone mode, which can be used for development and test purpose, or in distributed mode (cluster) for production deployment. With distribution mode, MirrorMaker 2.0 creates the following topics on the cluster it is connected to (See later the property <code>connectCluster</code>):</p> <ul> <li>...-configs.source.internal: This topic is used to store the connector and task configuration.</li> <li>...-offsets.source.internal: This topic is used to store offsets for Kafka Connect.</li> <li>...-status.source.internal: This topic is used to store status updates of connectors and tasks.</li> <li>source.heartbeats: to check that the remote cluster is available and the clusters are connected</li> <li>*source.checkpoints.internal: MirrorCheckpointConnector tracks and maps offsets for specified consumer groups using an offset sync topic and checkpoint topic.</li> </ul> <p>A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition.  Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and  SASL authentication protocol.  The IBM Event Streams instance runs on the Cloud.</p> <pre><code>clusters=source, target\nsource.bootstrap.servers=${KAFKA_SOURCE_BROKERS}\ntarget.bootstrap.servers=${KAFKA_TARGET_BROKERS}\ntarget.security.protocol=SASL_SSL\ntarget.ssl.protocol=TLSv1.2\ntarget.ssl.endpoint.identification.algorithm=https\ntarget.sasl.mechanism=PLAIN\ntarget.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY};\n# enable and configure individual replication flows\nsource-&gt;target.enabled=true\nsource-&gt;target.topics=products\ntasks.max=10\n</code></pre> <ul> <li>Topics to be replicated are configured via a whitelist that may include regular expression for pattern matching on the topic name. So if you use naming convention for your topic, you could do fine grained selection of the replicated topic.  It is possible to specify topics you do not want to replicate via the blacklist property.</li> <li>White listed topics are set with the <code>source-&gt;target.topics</code> attribute of the replication flow and uses Java regular expression syntax.</li> <li>The default blacklisted topics are Kafka internal topic:</li> </ul> <pre><code>blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas]\n</code></pre> <p>We can also define the blacklist with the properties: <code>topics.blacklist</code>. Comma-separated lists and Java Regular Expressions are supported.</p> <p>Internally, <code>MirrorSourceConnector</code> and <code>MirrorCheckpointConnector</code> will create multiple Kafka tasks (up to the value of <code>tasks.max</code> property), and <code>MirrorHeartbeatConnector</code> creates an additional task. <code>MirrorSourceConnector</code> will have one task per topic-partition combination to replicate, while <code>MirrorCheckpointConnector</code> will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the <code>assign()</code> API, so there is no consumer group used while fetching data from source topics. There is no call to <code>commit()</code> either; rebalancing occurs only when there is a new topic created that matches the whitelist pattern.</p> <p>Mirror Maker 2 can run on VM, bare metal or within containers deployed on kubernetes cluster.</p>"},{"location":"techno/mirrormaker/#why-replicating","title":"Why replicating?","text":"<p>The classical needs for replication between clusters can be listed as:</p> <ul> <li>Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The following article goes over those principals.</li> <li>Active-active cluster mirroring for inter services communication: consumers and producers are on both sides and consume or produce to their local cluster.</li> <li>Moving data to a read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view.</li> <li>GDPR compliance to isolate data in country and geography</li> <li>Hybrid cloud operations to share data between on-premise cluster and managed service clusters.</li> </ul>"},{"location":"techno/mirrormaker/#deployment-examples","title":"Deployment examples","text":"<p>We encourage you to go over our Mirror maker 2 labs which addresses different replication scenarios. The <code>Connect</code> column defines where the Mirror Maker 2 runs.</p> Scenario Source Target Connect Lab Getting Started Lab 1 Event Streams on Cloud Local Kafka Local on localhost Kafka Mirror Maker 2 - Lab 1 Lab 2 Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud On OCP OCP Kafka Mirror Maker 2 - Lab 2"},{"location":"techno/mirrormaker/#replication-considerations","title":"Replication considerations","text":""},{"location":"techno/mirrormaker/#topic-metadata-replication","title":"Topic metadata replication","text":"<p>It is possible to disable the topic metadata replication. We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created by the stream topology semantic, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare. By synchronizing configuration properties, the need for rebalancing is reduced.</p> <p>When doing manual configuration, even if the initial topic configuration is duplicated, any dynamic changes to the topic properties are not going  to be automatically propagated and the administrator needs to change the target topic. If the throughput on  the source topic has increased and the number of partition was increased to support the load, then the target cluster will not have the same  downstream capability which may lead to overloading (disk space or memory capacity). With a GitOps approach, this risk is mitigated as topic definition in the GitOps repository could be propagated to the target and source cluster mostly at the same time. </p> <p>Also if the consumer of a partition is expecting to process the events in order within the partition, then changing the number of partitions  between source and target will make the ordering not valid any more.</p> <p>If the replication factor are set differently between the two clusters then the availability guaranty of the replicated data may be impacted  and bad settings with broker failure will lead to data lost.</p> <p>Finally, it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates  a backlog in the pipeline and increases the end to end latency observed by the downstream application.</p>"},{"location":"techno/mirrormaker/#naming-convention","title":"Naming convention","text":"<p>Mirror maker 2 sets the prefix for the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid  infinite loop when doing bi-directional mirroring. At the consumer side the <code>subscribe()</code> function supports regular expression for topic name. So a code like:</p> <pre><code>kafkaConsumer.subscribe(\"^.*accounts\")\n</code></pre> <p>will listen to all the topics in the cluster having cluster name prefixed topics and the local <code>accounts</code> topic.  This could be useful when we want to aggregate data from different data centers / clusters.</p>"},{"location":"techno/mirrormaker/#offset-management","title":"Offset management","text":"<p>Mirror maker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters  and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its records, it gets the offsets  in the partition the records were created.</p> <p>In the diagram below we have a source topic A/partition 1 with the last write offset done by a producer to be  5, and the last read committed offset by the consumer assigned to partition 1 being 3.  The last replicated offset 3 is mapped as downstream offset 12 in the target partition. Offset numbers do not match between replicated partitions. So if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment.  This information is saved in the <code>checkpoint</code> topic.</p> <p></p> <p>Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges.  For example, the normal behavior is to increase the offset by one 2,3,4,5,6,7, which is mapped to 12,13,14,15,16,... on target cluster.  If the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the <code>offset-synch</code> topic.</p> <p>The <code>checkpoint</code> and <code>offset_synch</code> topics enable replication to be fully restored from the correct offset position on failover.  On the following diagram, once the cluster source is down, the consumers on the target cluster are restarted, and they will start from the last committed  offset of the source, which was offset 3 that is in fact offset 12 on target replicated topic. No record skipped.</p> <p></p>"},{"location":"techno/mirrormaker/#record-duplication","title":"Record duplication","text":"<p>Exactly-once delivery is difficult to achieve in distributed system. In the case of Kafka, producer, brokers, and consumers are working together to ensure  only one message is processed end to end. With coding practice and configuration settings, within a unique cluster, Kafka can guarantee exactly once processing.  No duplicated records between producer and broker, and committed reads, on consumer side, are not reprocessed in case of consumer restarts.</p> <p>But for cross cluster replications, the semantic is based on at least once approach. Duplicates can happen when the mirror maker source task stops  before committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates.  The following diagram illustrate this case, record offset 26 on target topic is a duplicate of record 25.</p> <p></p> <p>Also Mirror Maker 2 is a generic topic consumer, it will not participate to the \"read-committed\" process, if the topic includes duplicate messages  it will propagate them to the target.</p> <p>In the future MM2 will be able to support exactly once by using the <code>checkpoint</code> topic on the target cluster to keep the state of the committed offset  from the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part  of the same transaction.</p>"},{"location":"techno/mirrormaker/#mm2-topology","title":"MM2 topology","text":"<p>In this section we want to address horizontal scalability and how to organize the MirrorMaker 2 topology for multi-tenancy. The simplest approach is  to use one Mirror Maker instance per family of topics: the classification of family of topic can be anything, from line of business, to team, to application.  Suppose an application is using 1000 topic - partitions, for data replication it may make sense to have one MM2 instance for this application.  The configuration will define the groupId to match the application name for example.</p> <p>The following diagram illustrates this kind of topology by using regular expression on the topic white list selection, there are three Mirror Maker 2  instances mirroring the different topics with name starting with <code>topic-name-A*, topic-name-B*, topic-name-C*,</code> respectively.</p> <p></p> <p>Each connect instance is a JVM workers that replicate the topic/partitions and has different group.id.</p> <p>For Bi-directional replication for the same topic name, Mirror Maker 2 will use the cluster name as prefix. With MM2, we do not need to have 2 MM2 clusters  but only one and bidirectional definitions. The following example is showing the configuration for a MM2 bidirectional settings, with <code>accounts</code> topic to be  replicated on both cluster:</p> <pre><code>apiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaMirrorMaker2\n...\n mirrors:\n  - sourceCluster: \"event-streams-wdc\"\n    targetCluster: \"kafka-on-premise\"\n    ...\n    topicsPattern: \"accounts,orders\"\n  - sourceCluster: \"kafka-on-premise\"\n    targetCluster: \"event-streams-wdc\"\n    ...\n    topicsPattern: \"accounts,customers\"\n</code></pre>"},{"location":"techno/mirrormaker/#consumer-coding","title":"Consumer coding","text":"<p>We recommend to review the producer implementation best practices and the consumer considerations.</p>"},{"location":"techno/mirrormaker/#capacity-planning","title":"Capacity planning","text":"<p>For platform sizing, the main metric to assess, is the number of partitions to replicate. The number of partitions and number of brokers are somehow connected as  getting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster  and each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size.</p> <p>To address capacity planning, we need to review some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running.  We can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on.  The parameters <code>max.tasks</code> specifies the max parallel processing we can have per JVM. So for each Topic we need to assess the number of partitions to be replicated.  Each task is using the consumer API and is part of the same consumer group, the partition within a group are balanced by an internal controller.  With Kafka connect any changes to the topic topology triggers a partition rebalancing. In MM2 each consumer / task is assigned a partition by the controller.  So the rebalancing is done internally. Still adding a broker node into the cluster will generate rebalancing.</p> <p>The task processing is stateless, consume - produce wait for acknowledge,  commit offset. In this case, the CPU and network performance are key.  For platform tuning activity, we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start  to scale horizontally by adding more Mirror Maker 2 instances. If the network at the server level is the bottleneck, then adding more servers will help.  Kafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughput as with small  message the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation.</p> <p>The parameters to consider for sizing are the following:</p> Parameter Description Impact Number of topic/ partition Each task processes one partition For pure parallel processing max.tasks is set around the number of CPU Record size Size of the message in each partition in average Memory usage and Throughput: the # of records/s decrease when size increase, while MB/s throughput increases in logarithmic Expected input throughput The producer writing to the source topic throughput Be sure the consumers inside MM2 absorb the demand Network latency This is where positioning MM2 close to the target cluster may help improve latency"},{"location":"techno/mirrormaker/#version-migration","title":"Version migration","text":"<p>Once the Mirror Maker cluster is up and running, it may be needed to update the underlying code when a new product version is released. Based on Kafka Connect distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves.  If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers. If a new worker starts work, a rebalance ensures it takes over some work from the existing workers.</p> <p>Using the REST API it is possible to stop and restart a connector. As of now the recommendation is to start a new MirrorMaker instance with the new version and the same groupId as the existing workers you want to migrate. Then stop the existing version. As each MirrorMaker workers are part of the same group, the internal worker controller will coordinate with the other workers the  'consumer' task to partition assignment.</p> <p>First we will stop the Node 1 instance, upgrade it to the latest version, then start it again.  Then we\u2019ll repeat the same procedure on Node 2.  We\u2019ll continue to watch the Consumer VM window to note that replication should not stop at any point.</p> <p></p> <p>We\u2019ve now upgraded to Kafka 2.5 including the latest MirrorMaker 2.  Meanwhile, replication was uninterrupted due to the second instance of MirrorMaker 2:</p> <p></p> <p>Now we\u2019ll restart the Node 1 instance of MirrorMaker 2, stop the Node 2 instance, we can still see replication occurring on the upgraded Node 1 instance of MirrorMaker 2.</p> <p></p> <p>We upgrade Node 2\u2019s instance of MirrorMaker 2 exactly as on Node 1, and start it again, and once again, replication is still going.</p> <p></p> <p>When using Strimzi, if the update applies to the MM2 Custom Resource Definition, just reapplying the CRD should be enough.</p> <p>Be sure to verify the product documentation as new version may enforce to have new topics. It was the case when Kafka connect added the config topic in a recent version.</p>"},{"location":"techno/mirrormaker/#resources","title":"Resources","text":"<ul> <li>IBM Event Streams product documentation</li> <li>IBM Event Streams managed service - Disaster recovery example scenario</li> <li>Strimzi configuration for Mirror Maker 2</li> <li>Getting started with Mirror Maker 2 - Tech Academy</li> </ul>"}]}